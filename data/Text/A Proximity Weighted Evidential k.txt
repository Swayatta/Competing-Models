A Proximity Weighted Evidential k

Nearest Neighbor Classiﬁer

for Imbalanced Data

Md. Eusha Kadir1(B), Pritom Saha Akash1, Sadia Sharmin2,

Amin Ahsan Ali3, and Mohammad Shoyaib1

1 Institute of Information Technology, University of Dhaka, Dhaka, Bangladesh

{bsse0708,bsse0604}@iit.du.ac.bd, shoyaib@du.ac.bd
2 Islamic University of Technology, Gazipur, Bangladesh

sharmin@iut-dhaka.edu

3 Independent University, Dhaka, Bangladesh

aminali@iub.edu.bd

Abstract. In k Nearest Neighbor (kNN) classiﬁer, a query instance
is classiﬁed based on the most frequent class of its nearest neighbors
among the training instances. In imbalanced datasets, kNN becomes
biased towards the majority instances of the training space. To solve
this problem, we propose a method called Proximity weighted Eviden-
tial kNN classiﬁer. In this method, each neighbor of a query instance is
considered as a piece of evidence from which we calculate the probabil-
ity of class label given feature values to provide more preference to the
minority instances. This is then discounted by the proximity of the neigh-
bor to prioritize the closer instances in the local neighborhood. These
evidences are then combined using Dempster-Shafer theory of evidence.
A rigorous experiment over 30 benchmark imbalanced datasets shows
that our method performs better compared to 12 popular methods. In
pairwise comparison of these 12 methods with our method, in the best
case, our method wins in 29 datasets, and in the worst case it wins in
least 19 datasets. More importantly, according to Friedman test the pro-
posed method ranks higher than all other methods in terms of AUC at
5% level of signiﬁcance.

Keywords: Classiﬁer · Imbalanced learning · kNN · Evidence theory

1 Introduction

Classiﬁcation is one of the most important tasks in machine learning. Numerous
classiﬁcation approaches, such as k Nearest Neighbor (kNN) [9], Decision Tree
(DT), Na¨ıve Bayes (NB), and Support Vector Machine, have been well developed
and applied in many applications. However, most of the classiﬁers face serious
trouble for imbalanced class distribution and thus learning from the imbalanced
dataset is one of the top ten challenging problems in data mining research [20].
c(cid:2) Springer Nature Switzerland AG 2020
H. W. Lauw et al. (Eds.): PAKDD 2020, LNAI 12085, pp. 71–83, 2020.
https://doi.org/10.1007/978-3-030-47436-2_6

72

Md. E. Kadir et al.

To solve class imbalance problem, various strategies have already been pro-
posed which can be grouped into two broad categories namely data oriented and
algorithm oriented approaches. Data oriented approaches use sampling tech-
niques. In order to make dataset balanced, the sampling techniques either over-
sample the minority instances or select instances (under-sample) from the major-
ity class. A sampling technique namely Synthetic Minority Over-sampling TEch-
nique (SMOTE) has been proposed that increases the number of minority class
instances by creating artiﬁcial and non-repeated samples [4].

In contrast, algorithm oriented approaches are the modiﬁcations of tradi-
tional algorithms such as DT and kNN. The modiﬁed DTs for imbalanced clas-
siﬁcation are Hellinger Distance DT (HDDT) [5], Class Conﬁdence Proportion
DT (CCPDT) [13] and Weighted Inter-node Hellinger Distance DT (iHDwDT)
[1]. These DTs use diﬀerent splitting criteria while selecting a feature in split
point.

kNN is one of the simplest classiﬁers. Despite its simplicity, kNN is considered
as one of the top most inﬂuential data mining algorithms [19]. Traditional kNN
ﬁnds the k closest instances from the training data to a query instance and treats
all neighbors equally. Dudani has proposed a distance based weighted kNN which
provides more weights to closer neighbors [8]. Another variant of kNN approach,
Generalized Mean Distance based kNN (GMDKNN) [10], has been presented by
introducing multi-generalized mean distance and the nested generalized mean
distance. All these variants of kNN are sensitive to the majority instances and
thus perform poorly for imbalanced datasets.

Considering this imbalance problem, several researchers extended kNN for
imbalanced datasets [7,11,12]. In Exemplar-based kNN (kENN) [11], Li and
Zhang expand the decision boundary for the minority class by identifying the
exemplar minority instances. A weighting algorithm namely Class Conﬁdence
Weighted kNN (CCWKNN) has been presented in [12] where the probability of
feature values given the class labels is considered as weight. Dubey and Pudi have
proposed a weighted kNN (WKNN) [7] which considers the class distribution in a
wider region around a query instance. The class weight for each training instance
is estimated by taking the local class distributions into account.

The purpose of these existing studies is to improve the overall performance for
imbalanced data. However, these methods overlook the problem of uncertainty
which is prevalent in almost all datasets [18]. The reason behind this uncertainty
is that the complete statistical knowledge associated with the conditional density
function of each class is hardly available [6]. To address this problem, kNN has
been extended using Dempster-Shafer Theory of evidence (DST) to better model
uncertain data named Evidential kNN (EKNN) [6]. In EKNN, each neighbor
assigns basic belief on classes based on a distance measure. Nevertheless, this
approach again does not take consideration of the class imbalance problem.

To address these aforementioned problems, we propose a Proximity weighted
Evidential kNN (PEkNN) classiﬁer and make the following contributions. Firstly,
we have proposed a conﬁdence (posterior) assignment procedure on each neigh-
bor of a query instance. Secondly, we have also proposed to use proximity of a

Proximity Weighted Evidential k NN

73

neighbor as a weight to discount the conﬁdence of a neighbor. It is shown that,
this weighted conﬁdence increases the likelihood of classifying a minority class.
Thirdly, DST framework is used to combine decisions from diﬀerent neighbors.

2 Dempster-Shafer Theory of Evidence

Dempster-Shafer theory of evidence is a generalized form of Bayesian theory.
It assigns degree of belief for all possible subsets of the hypothesis set. Let,
C = {C1, . . . , CM} be a ﬁnite hypothesis set of mutually exclusive and exhaustive
hypotheses. The belief in a hypothesis assigned based on a piece of evidence is
ranged numerically as [0, 1]. A Basic Belief Assignment (BBA) is a function
m : 2C → [0, 1] which satisﬁes the following properties:

m(∅) = 0 and

(cid:2)

A⊆C

m(A) = 1

(1)

where m(A) is a degree of belief (referred as mass) which reﬂects how strongly
A is supported by the piece of evidence. m(C) represents the degree of ignorance.
Several pieces of evidence characterized by their BBAs can be fused using
Dempster’s rule of combination [16]. For two BBAs m1(.) and m2(.) which are
not totally conﬂicting, the combination rule can be expressed using Eq. (2).

A (cid:4)= ∅

(2)

B∩C=A m1(B)m2(C)

(cid:3)
1 − (cid:3)
m(A) =
(cid:3)
B∩C=∅ m1(B)m2(C) < 1.

B∩C=∅ m1(B)m2(C)

where A, B, C ∈ 2C and
For decision making, Belief, Plausibility and betting Probability (Pbet) are
usually used. For a singleton class A, Pbet(A) is derived in Eq. (3) where |B|
represents the cardinality of the element B.

Pbet(A) =

(cid:2)

A⊆B

|A ∩ B|
|B| × m(B)

(3)

3 Proximity Weighted Evidential kNN (PEkNN)

kNN faces diﬃculty in imbalanced datasets as it treats all neighbors of the query
instance equally and most of the neighbors will be of the majority class. To deal
with this issue, the proposed algorithm attempts to provide more importance to
neighbors with a higher proximity weighted conﬁdence. Here, conﬁdence of an
instance indicates a conditional probability of that instance based on training
data. Algorithms such as NB also uses conditional probability while classifying
a query instance. However, the performance of NB degrades due to the poor
estimation of the conditional density of the query instance associated with each
class. In contrast, PEkNN computes conditional probability of neighborhood
instances rather than query instance. Furthermore, as uncertainty is prevalent in

74

Md. E. Kadir et al.

almost all datasets [18]. This is more signiﬁcant for imbalanced datasets where
little information is available for the minority class. To deal with this issue,
PEkNN uses DST to combine the evidences provided by each neighbor.

For a new query instance (xt), PEkNN ﬁrst ﬁnds k closest neighbors accord-
ing to some distance measurement (e.g. Euclidean distance). Let, S(xt, k) be the
set of k closest neighbors of xt and each member of S(xt, k) is considered as a
piece of evidence which assigns mass values for each subset of C known as BBA.
Now, consider xi as the i-th neighbor of xt belonging to class Cq. As xi is a
piece of evidence belonging to Cq, some part of its belief will be committed to
Cq. The rest of the belief can not be distributed to any other subset of C except
itself. The BBA provided by xi can be represented by Eq. (4), (5) and (6) where
0 < β0 < 1.

mi(C) = 1 − β

mi({Cq}) = β = β0 × Ψ(xi, xt)
mi(A) = 0 ∀A ∈ 2C\{C,{Cq}}

(4)
(5)
(6)
Now, we will discuss about two of our intuitions. First, a piece of evidence
belonging to Cq will assign a larger belief to Cq when the evidence is more reliable
which we call conﬁdence. An evidence having higher posterior probability should
get more conﬁdence than the one which is in lower posterior probability region.
The second intuition is that a neighbor will assign more belief to a speciﬁc class
when the neighbor and the query instance are more proximate. The function
deﬁned in Eq. (7), Ψ(.) satisﬁes the two aforementioned intuitions where pi is
the conﬁdence of xi represented by the probability of class label (yi) given xi
and prx(xi, xt) represents the proximity between xi and xt.

Ψ(xi, xt) = prx(xi, xt) × pi

(7)

The procedure how PEkNN algorithm classiﬁes a query instance is presented
in Algorithm 1. The conﬁdence assignment, proximity estimation and decision
making steps are presented in detail in Sects. 3.1, 3.2 and 3.3 respectively.

3.1 Estimation of Conﬁdence
The conﬁdence (pi) of an instance xi (xi ∈ R
following manner derived in Eq. (8).
pi = P (yi | xi) =

(cid:3)M
j=1

l) belonging to yi is assigned in the

P (yi) × P (xi | yi)

P (Cj) × P (xi | Cj)

(8)

where yi ∈ {C1, C2, . . . , CM}, P (Cj) represents the prior of Cj in training space
and P (xi|Cj) represents the likelihood in Bayes’ theorem. Here, two approaches
of estimating class-wise Probability Density Function (PDF) is presented. First
one is using Single Gaussian Model (SGM) and another one is using Gaussian
Mixture Model (GMM). When PEkNN uses conﬁdence derived from SGM, we
call it sPEkNN, and mPEkNN when it uses conﬁdence derived from GMM.

Proximity Weighted Evidential k NN

75

Algorithm 1: PEkNN Algorithm

Input : Training data (X), Training data labels (Y ), Neighborhood size (k),

Query instance (xt)

Output: Predicted class label (yt)
1 conf , dmax ← fitModel(X, Y )
2 s ← indices of k nearest neighbors of xt
3 Initialize a list bba of mass values
4 for i = 1 to k do
index ← s[i]
5
conf idence ← conf [index]
d ← distance(xt, X[index])
proximity ← calculate proximity using Eq. (11) from d, dmax
bba[i] ← assign mass value using Eq. (4), (5), (6) and (7) from conf idence,
proximity

6

7

8

9

10 end
11 m ← combine mass values from bba using Eq. (2)
12 Pbet ← calculate betting probabilities for all classes using Eq. (3) from m
13 yt ← calculate decision using Eq. (12) from Pbet
14 function fitModel(X, Y ):
15

conf [i] ← Calculate conﬁdence using Eq. (8) from X[i], Y [i]
for j = i+1 to |X| do

d ← distance(X[i], X[j])
dmax ← max(d, dmax)

Initialize Array, conf
dmax ← 0
for i = 1 to |X| do

end

end
return conf , dmax

16

17

18

19

20

21

22

23

24

Single Gaussian model assumes that all the features are independent and the
continuous values associated with each class follow a normal distribution. Under
these assumptions, the likelihood function can be represented as Eq. (9).

P (x) =

l(cid:4)

j=1

P (xj) =

l(cid:4)

j=1

f(xj; μj, σj

2) =

l(cid:4)

j=1

1√
2πσj

× exp(−(xj − μj)2

2σ2
j

)

(9)

where xj denotes the j-th feature of x and f(.) represents the normally dis-
tributed PDF parameterized by mean (μ) and variance (σ2).

On the other hand, GMM can also be used to estimate PDF from multivariate
data. The class-wise PDF using m-component mixture model is given in Eq. (10).

P (x) =

m(cid:2)

i=1

αiP (x | Zi)

(10)

The procedure of ﬁnding complete set of parameters (Z1, . . . , Zm, α1, . . . , αm)

specifying the mixture model is brieﬂy described in [14].

76

Md. E. Kadir et al.

3.2 Estimation of Proximity

To capture the proximity between two instances, some distance measurement can
be used. The proximity between two instances (xi and xj) from training samples
will be maximum when xi and xj are identical. One the other hand, it will be
lowest when they are the farthest two instances in the feature space. To measure
this proximity, a normalization is applied as Eq. (11) so that prx(xi, xj) ∈ [0, 1].
Here, dmax is the distance between two farthest training instances.

prx(xi, xj) = 1 − d(xi, xj)
dmax

(11)

3.3 Decision Making

According to Eq. (7), Ψ(.) will return a larger value when a neighbor is more
conﬁdent and more closer to the query instance. Now, for each of the k nearest
neighbors, the BBAs are deﬁned using Eq. (4), (5) and (6). In order to classify
xt, these BBAs are combined using DST. The betting probability (Pbet) for each
singleton class from this combined decision will be then calculated using Eq. (3).
Finally, the decision from this Pbet is taken using Eq. (12).

ˆy = arg max

c∈{C1,...,CM}

Pbet(c)

where c is a singleton class so that the cardinality of c is 1.

Properties of β: Value of β is bounded between 0 to 1.
Proof. From Eq. (4), (7) and (8), it can be derived that,
β = β0 × P (yi | xi) × prx(xi, xt)

(12)

(13)

Here, β0 is a user given constant satisfying 0 < β0 < 1. The second term,
P (yi | xi), represents the posterior probability. The last term, prx(xi, xt) is at
most equal to 1 and at least equal to zero. As can be seen from Eq. (13), β is
a product of three terms and all these terms are bounded between 0 to 1. It is
suﬃcient to claim that, the value of β must be bounded between 0 to 1.

3.4 An Illustrative Example

Figure 1 shows the instances of a two-class imbalance problem where (+)s and
(•)s represent the minority (Class-A) and majority class instances (Class-B)
instances respectively. The class boundaries are represented as dotted lines and
three query instances (t1, t2, t3) are marked with ((cid:7))s. Here, ﬁrst query instance
t1 is situated in a majority class region bounded by minority instances. Both
kNN and PEkNN can successfully classify t1. Traditional algorithms such as
C4.5 and NB face diﬃculties in this situation.

Proximity Weighted Evidential k NN

77

(a) Training space

(b) A1 region

Fig. 1. A synthetic imbalanced dataset

The two other query instances t2 and t3 associated with a region namely A1
(see Fig. 1b). Here, for both t2 and t3, the four neighbors are xa, xb, xc and xd.
Traditional kNN with k = 4, will classify both t2 and t3 as Class-B. PEkNN, on
the other hand, considers the conﬁdence of each neighbor. Here, xd will provide a
higher conﬁdence compared to majority class instances (xa, xb and xc). Assume,
the conﬁdence of xa, xb, xc, and xd are 0.30, 0.40, 0.30, and 0.75 respectively.
And the proximity with respect to t2 are 0.90, 0.95, 0.85 and 0.95 respectively.
Then BBAs assigned by PEkNN for these neighbors are ma({B}) = 0.2565,
ma({A, B}) = 0.7435, mb({B}) = 0.3610, mb({A, B}) = 0.6390, mc({B}) =
0.2423, mc({A, B}) = 0.7577 and md({A}) = 0.6769, md({A, B}) = 0.3231.
Here, β0 is set to 0.95. Now, combing these BBAs using DST, we get Pbet(A) =
0.5325 and Pbet(B) = 0.4675 which indicates that t2 will be correctly classiﬁed
as Class-A.

On the other hand, for the query instance t3, the proximity of xa, xb, xc and
xd are 0.85, 0.95, 0.95 and 0.85 respectively. We, therefore, get Pbet(A) = 0.4661
and Pbet(B) = 0.5339 indicating that t3 will be classiﬁed as Class-B. Therefore,
t3 is correctly classiﬁed as a majority class instance even though the neighbors
of t2 and t3 are same.

Instead of DST, let us reconsider simpler techniques to combine evidences
such as summing and taking the maximum of the proximity weighted conﬁ-
dences. If we simply sum class-wise proximity weighted conﬁdences, both t2 and
t3 get a higher value for Class-B as three of the four neighbors belong to that
class. To avoid this bias, a query can be simply classiﬁed in the class for which
it gets maximum proximity weighted conﬁdence among the neighbors. But this
method does not consider the local neighborhood priors. For which, it will clas-
sify both t2 and t3 as minority class which is not desired. PEkNN on the other
hand using the DST framework successfully classiﬁes both query instances.

78

Md. E. Kadir et al.

4 Experiments and Results

Dataset description, implementation details and the performance metrics fol-
lowed by the results obtained from the experiments with discussion are given in
the following subsections.

4.1 Dataset Description

The characteristics of the 30 benchmark datasets are shown in Table 1 which
are collected from UCI machine learning repository [3] and KEEL Imbalanced
Datasets [2]. Imbalance Ratio (IR) between the samples of majority class and
minority class of the datasets used in these experiment are at least 1.5 and values
of all the features are numeric. A dataset is highly imbalanced when the value
of IR is very high.

Table 1. Descriptions of Imbalanced Datasets. Idx, #Inst, #Cl and #Ftr represent
index of a dataset, number of instances, classes and features respectively.

Idx Name

#Ftr #Cl #Inst IR

Idx Name

#Ftr #Cl #Inst IR

01 Appendicitis

02 Ecoli1

03 Ecoli2

04 Ecoli3

05 Ecoli4

7

7

7

7

7

06 Glass-0-1-2-3 vs 4-5-6 9

07 Glass1

08 Glass4

09 Glass6

10 Haberman

11

Ionosphere

12 New-thyroid1

13 Page-blocks0

14 Pima

15 Segment0

9

9

9

3

34

5

10

8

19

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

106

336

336

336

336

214

214

214

214

306

351

215

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

1829

13.87

846

846

846

846

459

988

683

528

947

482

3.25

2.90

2.88

2.99

14.30

9.98

1.86

9.35

30.57

23.10

1484

2.46

1484

8.10

1484

32.73

1484

41.40

8

8

8

8

8

8

4.05

16 Shuttle-c0-vs-c4

3.36

17 Vehicle0

5.46

18 Vehicle1

8.60

19 Vehicle2

15.80 20 Vehicle3

3.20

24 Yeast-1 vs 7

1.82

21 Vowel0

15.46 22 Wisconsin

9

18

18

18

18

7

13

9

6.38

23 Yeast-0-5-6-7-9 vs 4 8

2.78

25 Yeast-1-2-8-9 vs 7

1.79

26 Yeast-2 vs 8

5.14

27 Yeast1

5472

8.79

28 Yeast3

768

1.87

29 Yeast5

2308

6.02

30 Yeast6

4.2

Implementation Details and Performance Metrics

PEkNN is benchmarked against other algorithms including traditional learn-
ing algorithms (kNN, C4.5, NB), oversampling strategy (SMOTE), recent algo-
rithms in the kNN family (EKNN, WKNN, CCWKNN, kENN, GMDKNN) and
few tree based recent algorithms for imbalanced classiﬁcation (CCPDT, HDDT,
iHDwDT). For PEkNN, we use β0 = 0.95 in this experiment. For kENN, the
conﬁdence level is set 0.1 and we set p = 1 for GMDKNN.

We have conducted 10-fold stratiﬁed cross validation to evaluate the per-
formance of the proposed method. The Receiver Operating Characteristic
(ROC) curve [17] is widely used to evaluate imbalanced classiﬁcation. We use
Area Under the ROC Curve (AUC) for evaluating the classiﬁer performance.

)
4
(
1
2
.
7
6

)
2
(
5
2
.
8
9

)
1
(
6
0
.
9
6

)
1
(
5
4
.
8
9

)
3
(
1
8
.
1
8

)
2
(
1
0
.
2
8

)
4
(
7
9
.
0
8

)
5
(
8
8
.
0
8

)
7
(
6
0
.
0
8

)
4
1
(
3
8
.
1
7

)
6
(
9
3
.
0
8

)
8
(
9
7
.
9
7

)
9
(
8
5
.
9
7

)
2
1
(
8
3
.
5
7

)
3
1
(
1
6
.
3
7

)
9
(
8
5
.
9
7

)
1
(
8
7
.
2
8

)
1
1
(
4
9
.
6
7

)
2
(
7
6
.
4
9

)
5
(
5
9
.
3
9

)
8
(
8
6
.
2
9

)
4
(
5
1
.
4
9

)
1
(
1
7
.
4
9

)
3
(
4
4
.
4
9

)
9
(
7
6
.
2
9

)
6
(
0
0
.
3
9

)
0
1
(
1
4
.
2
9

)
1
1
(
0
2
.
2
9

)
3
1
(
5
1
.
4
8

)
7
(
9
8
.
2
9

)
2
1
(
9
3
.
1
9

)
4
1
(
0
4
.
1
8

)
4
(
9
7
.
4
9

)
9
(
6
3
.
3
9

)
5
(
1
6
.
4
9

)
2
1
(
3
8
.
0
9

)
0
1
(
1
4
.
1
9

)
1
1
(
7
2
.
1
9

)
3
(
3
9
.
4
9

)
2
(
0
3
.
5
9

)
7
(
0
4
.
4
9

)
6
(
6
5
.
4
9

)
3
1
(
3
7
.
0
9

)
1
(
2
3
.
5
9

)
8
(
8
8
.
3
9

)
4
1
(
7
5
.
5
8

)
1
(
9
3
.
4
9

)
3
(
5
6
.
5
9

)
1
(
8
4
.
7
9

)
2
(
7
4
.
2
9

)
9
(
7
7
.
8
8

)
2
1
(
6
7
.
7
8

)
6
(
4
8
.
9
8

)
4
(
7
8
.
0
9

)
1
1
(
7
2
.
8
8

)
8
(
8
0
.
9
8

)
3
(
9
1
.
1
9

)
0
1
(
6
4
.
8
8

)
4
1
(
8
0
.
3
7

)
7
(
3
8
.
9
8

)
5
(
8
9
.
9
8

)
3
1
(
0
4
.
6
7

)
8
(
6
7
.
4
9

)
0
1
(
7
4
.
2
9

)
1
1
(
6
3
.
0
9

)
3
1
(
7
5
.
6
8

)
3
(
5
6
.
5
9

)
5
(
2
9
.
4
9

)
2
(
9
8
.
5
9

)
7
(
4
8
.
4
9

)
2
1
(
9
6
.
9
8

)
5
(
2
9
.
4
9

)
9
(
9
6
.
4
9

)
4
1
(
9
5
.
3
8

)
2
(
8
8
.
8
9

)
3
(
6
2
.
8
9

)
6
(
2
5
.
6
9

)
7
(
6
4
.
6
9

)
1
1
(
7
7
.
5
9

)
5
(
9
5
.
6
9

)
1
(
2
1
.
9
9

)
2
1
(
1
4
.
5
9

)
4
(
8
1
.
7
9

)
8
(
3
4
.
6
9

)
4
1
(
9
7
.
0
9

)
0
1
(
2
8
.
5
9

)
9
(
8
3
.
6
9

)
3
1
(
9
3
.
1
9

)
7
(
1
4
.
2
8

)
3
(
0
7
.
3
8

)
8
(
1
4
.
1
8

)
2
1
(
9
3
.
6
7

)
9
(
8
8
.
0
8

)
4
(
1
5
.
3
8

)
2
(
2
9
.
4
8

)
6
(
0
7
.
2
8

)
1
(
4
8
.
5
8

)
0
1
(
5
7
.
0
8

)
1
1
(
8
7
.
8
7

)
5
(
8
8
.
2
8

)
4
1
(
8
7
.
0
7

)
3
1
(
8
6
.
5
7

)
2
(
9
7
.
7
9

)
8
(
1
0
.
4
9

)
1
(
7
7
.
8
9

)
6
(
6
7
.
4
9

)
6
(
6
7
.
4
9

)
4
(
1
5
.
5
9

)
3
(
1
5
.
7
9

)
9
(
2
5
.
1
9

)
5
(
9
3
.
5
9

)
1
1
(
1
0
.
0
9

)
2
1
(
4
7
.
1
8

)
9
(
2
5
.
1
9

)
3
1
(
0
0
.
7
7

)
4
1
(
2
5
.
6
7

)
2
(
7
9
.
7
9

)
3
(
9
7
.
7
9

)
6
(
4
8
.
5
9

)
9
(
7
2
.
5
9

)
9
(
7
2
.
5
9

)
4
(
0
4
.
6
9

)
1
(
9
8
.
8
9

)
8
(
9
3
.
5
9

)
2
1
(
4
3
.
1
9

)
5
(
3
1
.
6
9

)
3
1
(
6
3
.
9
8

)
7
(
7
5
.
5
9

)
1
1
(
2
2
.
4
9

)
4
1
(
8
8
.
5
8

)
8
(
4
5
.
4
6

)
1
1
(
0
1
.
2
6

)
2
1
(
0
9
.
1
6

)
6
(
2
0
.
6
6

)
2
(
5
4
.
8
6

)
9
(
1
3
.
4
6

)
7
(
5
1
.
5
6

)
3
(
5
3
.
8
6

)
4
1
(
2
9
.
3
5

)
5
(
3
4
.
6
6

)
0
1
(
8
1
.
4
6

)
3
1
(
2
2
.
4
5

)
5
(
8
6
.
9
9

)
3
(
1
8
.
9
9

)
7
(
8
5
.
9
9

)
1
1
(
6
3
.
8
9

)
0
1
(
3
6
.
8
9

)
3
1
(
6
9
.
7
9

)
5
(
8
6
.
9
9

)
3
(
1
8
.
9
9

)
9
(
3
9
.
8
9

)
5
(
4
0
.
3
9

)
8
(
8
6
.
1
9

)
6
(
8
7
.
1
9

)
1
1
(
0
9
.
0
9

)
3
(
7
6
.
7
9

)
9
(
5
2
.
1
9

)
7
(
6
7
.
1
9

)
4
1
(
9
6
.
9
7

)
1
(
0
.
0
0
1

)
3
1
(
4
5
.
5
8

)
0
1
(
6
1
.
1
9

)
4
(
2
2
.
4
9

)
2
1
(
7
8
.
6
8

)
2
1
(
5
1
.
8
9

)
7
(
8
5
.
9
9

)
1
(
0
.
0
0
1

)
4
1
(
6
3
.
3
9

)
4
(
4
6
.
7
9

)
0
1
(
4
9
.
5
9

)
6
(
0
8
.
6
9

)
1
(
7
6
.
8
9

)
2
(
4
5
.
8
9

)
3
(
5
1
.
8
9

)
5
(
9
0
.
7
9

)
7
(
6
1
.
6
9

)
1
1
(
5
8
.
5
9

)
9
(
3
0
.
6
9

)
3
1
(
2
5
.
2
9

)
8
(
1
1
.
6
9

)
2
1
(
6
9
.
2
9

)
4
1
(
7
7
.
1
9

)
3
(
6
3
.
0
8

)
2
(
4
3
.
1
8

)
4
(
8
9
.
7
7

)
7
(
9
8
.
7
7

)
0
1
(
9
6
.
6
7

)
1
1
(
2
5
.
6
7

)
2
1
(
6
1
.
6
7

)
5
(
3
9
.
7
7

)
9
(
4
2
.
7
7

)
8
(
7
7
.
7
7

)
3
1
(
1
6
.
9
6

)
5
(
3
9
.
7
7

)
1
(
5
7
.
1
8

)
4
1
(
2
9
.
8
6

)
2
(
5
9
.
9
9

)
8
(
7
7
.
9
9

)
5
(
3
9
.
9
9

)
0
1
(
4
5
.
9
9

)
0
1
(
4
5
.
9
9

)
9
(
9
5
.
9
9

)
1
(
8
9
.
9
9

)
2
(
5
9
.
9
9

)
7
(
8
8
.
9
9

)
6
(
0
9
.
9
9

)
2
1
(
8
4
.
9
9

)
4
(
4
9
.
9
9

)
3
1
(
0
0
.
8
9

)
4
1
(
4
9
.
7
9

)
1
(
0
.
0
0
1

)
1
(
0
.
0
0
1

)
8
(
9
5
.
9
9

)
1
(
0
.
0
0
1

)
1
(
0
.
0
0
1

)
1
(
0
.
0
0
1

)
1
(
0
.
0
0
1

)
8
(
9
5
.
9
9

)
8
(
9
5
.
9
9

)
8
(
9
5
.
9
9

)
8
(
9
5
.
9
9

)
8
(
9
5
.
9
9

)
4
1
(
1
9
.
7
9

)
1
(
0
.
0
0
1

)
4
(
4
2
.
8
9

)
9
(
2
5
.
6
9

)
8
(
4
7
.
7
9

)
2
(
0
6
.
8
9

)
3
(
9
3
.
8
9

)
1
(
3
6
.
8
9

)
5
(
7
1
.
8
9

)
6
(
3
9
.
7
9

)
0
1
(
7
4
.
6
9

)
1
1
(
4
4
.
6
9

)
2
1
(
2
4
.
3
9

)
7
(
0
8
.
7
9

)
4
1
(
7
3
.
1
8

)
3
1
(
6
1
.
1
9

)
1
(
8
3
.
4
8

)
9
(
4
0
.
9
7

)
4
(
4
7
.
2
8

)
0
1
(
1
9
.
8
7

)
7
(
5
3
.
0
8

)
5
(
1
7
.
1
8

)
1
1
(
6
8
.
8
7

)
2
(
7
0
.
3
8

)
8
(
0
2
.
9
7

)
6
(
0
4
.
1
8

)
3
1
(
6
0
.
7
6

)
3
(
3
0
.
3
8

)
2
1
(
0
2
.
1
7

)
4
1
(
1
5
.
4
6

)
2
(
3
6
.
9
9

)
1
1
(
6
5
.
7
9

)
8
(
5
7
.
8
9

)
7
(
3
8
.
8
9

)
5
(
1
1
.
9
9

)
6
(
5
9
.
8
9

)
1
(
4
6
.
9
9

)
4
(
0
2
.
9
9

)
2
1
(
6
4
.
7
9

)
9
(
8
4
.
8
9

)
0
1
(
2
9
.
7
9

)
3
(
1
2
.
9
9

)
4
1
(
8
5
.
5
8

)
3
1
(
0
5
.
4
9

)
1
(
8
0
.
4
8

)
9
(
2
8
.
8
7

)
8
(
9
1
.
0
8

)
5
(
9
9
.
0
8

)
3
(
4
2
.
1
8

)
4
(
4
0
.
1
8

)
9
(
2
8
.
8
7

)
2
(
6
3
.
2
8

)
1
1
(
1
8
.
8
7

)
1
(
0
.
0
0
1

)
1
(
0
.
0
0
1

)
1
(
0
.
0
0
1

)
3
1
(
4
8
.
7
9

)
0
1
(
5
3
.
8
9

)
1
1
(
3
3
.
8
9

)
1
(
0
.
0
0
1

)
1
(
0
.
0
0
1

)
6
(
8
8
.
8
9

)
4
(
1
0
.
9
9

)
0
1
(
3
6
.
8
9

)
9
(
5
6
.
8
9

)
3
(
4
0
.
9
9

)
2
(
7
0
.
9
9

)
1
1
(
7
5
.
8
9

)
8
(
6
6
.
8
9

)
1
(
0
3
.
9
9

)
7
(
8
9
.
9
9

)
7
(
4
3
.
0
8

)
8
(
4
8
.
9
9

)
4
1
(
4
6
.
5
6

)
1
(
0
.
0
0
1

)
6
(
7
4
.
0
8

)
2
1
(
2
8
.
9
6

)
3
1
(
2
2
.
9
6

)
8
(
4
8
.
9
9

)
2
1
(
4
2
.
8
9

)
4
1
(
6
6
.
6
9

)
2
1
(
8
0
.
6
9

)
3
1
(
5
7
.
5
9

)
7
(
7
6
.
8
9

)
5
(
3
9
.
8
9

)
4
1
(
8
6
.
2
9

)
6
(
1
6
.
3
8

)
3
(
1
0
.
5
8

)
9
(
5
0
.
1
8

)
5
(
0
3
.
4
8

)
2
(
4
0
.
6
8

)
1
(
2
6
.
7
8

)
7
(
8
6
.
2
8

)
2
1
(
6
9
.
7
7

)
4
(
0
4
.
4
8

)
1
1
(
5
1
.
8
7

)
4
1
(
7
4
.
3
6

)
0
1
(
9
4
.
9
7

)
8
(
4
6
.
1
8

)
3
1
(
0
6
.
7
6

)
1
1
(
2
9
.
3
7

)
6
(
3
9
.
8
6

)
1
(
0
3
.
3
8

)
1
(
7
7
.
9
7

)
4
(
0
5
.
8
7

)
3
(
0
3
.
9
7

)
6
(
6
1
.
7
7

)
0
1
(
1
1
.
4
7

)
2
1
(
0
1
.
3
7

)
7
(
5
8
.
6
7

)
8
(
5
8
.
5
7

)
9
(
5
4
.
4
7

)
3
1
(
8
8
.
9
6

)
5
(
1
5
.
7
7

)
2
(
2
1
.
1
8

)
4
1
(
8
3
.
9
6

)
4
(
3
2
.
2
7

)
5
(
6
1
.
1
7

)
0
1
(
4
4
.
8
6

)
2
1
(
6
0
.
7
6

)
1
1
(
0
4
.
8
6

)
8
(
3
6
.
8
6

)
3
(
2
1
.
3
7

)
7
(
4
7
.
8
6

)
3
1
(
2
9
.
0
6

)
9
(
8
5
.
8
6

)
2
(
6
8
.
4
7

)
4
1
(
1
4
.
7
5

)
1
(
1
6
.
8
7

)
4
(
6
9
.
5
7

)
7
(
4
2
.
4
7

)
5
(
5
8
.
5
7

)
9
(
2
2
.
4
7

)
7
(
4
2
.
4
7

)
2
1
(
4
0
.
1
7

)
0
1
(
3
6
.
3
7

)
3
(
2
4
.
6
7

)
1
1
(
5
7
.
2
7

)
3
1
(
8
4
.
5
6

)
6
(
3
6
.
4
7

)
2
(
7
9
.
7
7

)
4
1
(
3
9
.
4
6

)
2
(
1
4
.
7
8

)
3
(
5
9
.
6
8

)
2
1
(
5
0
.
0
8

)
0
1
(
0
5
.
1
8

)
1
(
7
2
.
8
8

)
4
(
3
0
.
5
8

)
5
(
1
0
.
5
8

)
6
(
3
4
.
3
8

)
1
1
(
7
6
.
0
8

)
8
(
3
7
.
2
8

)
3
1
(
3
4
.
5
7

)
9
(
3
4
.
2
8

)
7
(
7
8
.
2
8

)
4
1
(
3
5
.
0
7

)
2
(
4
7
.
6
9

)
7
(
4
6
.
5
9

)
0
1
(
7
4
.
3
9

)
1
(
2
9
.
6
9

)
5
(
1
4
.
6
9

)
2
(
4
7
.
6
9

)
1
1
(
3
3
.
3
9

)
9
(
5
6
.
3
9

)
4
(
9
5
.
6
9

)
2
1
(
3
2
.
3
9

)
3
1
(
4
3
.
5
8

)
8
(
8
7
.
3
9

)
6
(
4
7
.
5
9

)
4
1
(
4
4
.
4
8

)
4
(
3
5
.
8
9

)
2
(
5
1
.
9
9

)
1
1
(
3
5
.
6
9

)
7
(
5
8
.
7
9

)
6
(
0
9
.
7
9

)
5
(
1
1
.
8
9

)
8
(
2
8
.
7
9

)
9
(
8
7
.
6
9

)
1
(
3
2
.
9
9

)
2
1
(
7
2
.
6
9

)
4
1
(
4
9
.
1
8

)
0
1
(
2
7
.
6
9

)
3
(
4
6
.
8
9

)
3
1
(
0
7
.
5
8

)
8
(
1
3
.
6
8

)
6
(
6
7
.
7
8

)
7
(
3
3
.
7
8

)
4
(
3
5
.
9
8

)
9
(
2
9
.
5
8

)
1
1
(
8
0
.
5
8

)
3
(
4
2
.
1
9

)
2
1
(
3
7
.
4
8

)
3
1
(
5
8
.
4
7

)
0
1
(
5
2
.
5
8

)
2
(
7
1
.
3
9

)
4
1
(
8
6
.
2
7

1
0

2
0

3
0

4
0

5
0

6
0

7
0

8
0

9
0

0
1

1
1

2
1

3
1

4
1

5
1

6
1

7
1

8
1

9
1

0
2

1
2

2
2

3
2

4
2

5
2

6
2

7
2

8
2

9
2

0
3

Proximity Weighted Evidential k NN

79

)
5
(
1
4
.
9
8

)
1
(
0
6
.
5
9

5
1
-
2
-
3
1

0
3
.
3

3
2
.
4

e
s
a
B

e
s
a
B

3
1
-
2
-
5
1

e
s
a
B

–

e
s
a
B

–

8
-
1
-
1
2

7
-
1
-
2
2

1
1
-
1
-
8
1

0
1
-
1
-
9
1

3
7
.
6

3
1
.
7

0
6
.
6

3
3
.
6

3
-
1
-
6
2

(cid:2)

(cid:2)

6
-
1
-
3
2

(cid:2)

(cid:2)

7
-
1
-
2
2

7
-
2
-
1
2

(cid:2)

(cid:2)

9
-
3
-
8
1

7
-
4
-
9
1

3
0
.
6

(cid:2)

7
-
2
-
1
2

9
-
0
-
1
2

3
7
.
6

7
5
.
6

7
-
0
-
3
2

0
8
.
8

0
3
.
2
1

1
-
1
-
8
2

4
-
2
-
4
2

(cid:2)

(cid:2)

8
-
0
-
2
2

(cid:2)

(cid:2)

3
-
0
-
7
2

(cid:2)

(cid:2)

0
-
1
-
9
2

(cid:2)

(cid:2)

7
-
0
-
3
2

6
-
0
-
4
2

0
-
1
-
9
2

3
-
0
-
7
2

(cid:2)

(cid:2)

8
-
0
-
2
2

(cid:2)

(cid:2)

0
-
1
-
9
2

(cid:2)

(cid:2)

L
-
T
W

-

L
-
T
W

-

t
s
e
T

.
r
F

t
s
e
T

.
r
F

3
9
.
6

3
9
.
7

0
1
.
3
1

k
n
a
R

.
g
v
A

h
c
a
e

r
o
f

t
l
u
s
e
r

t
s
e
b

e
h
T

.
)

%

(

C
U
A

f
o

s

m
r
e
t

n

i

s
t
e
s
a
t
a
d

d
e
c
n
a
l
a
b
m

i

n
o

s

m
h
t
i
r
o
g
l
a

t
n
e
r
e
ﬀ
d

i

g
n
o
m
a

n
o
s
i
r
a
p
m
o
c

e
c
n
a
m
r
o
f
r
e
P

.
2

e
l
b
a
T

.
g
n

i
l

p
m
a
s
E
T
O
M
S

y
b

d
e
w
o
l
l
o
f

N
N
k

s
t
n
e
s
e
r
p
e
r
N
N
k
+
T
M
S

.

d
l
o
b

n

i

s
i

t
e
s
a
t
a
d

N
N
k
E
P
m

N
N
k
E
P
s

N
N
k
+
T
M
S

T
D
w
D
H

i

T
D
D
H

T
D
P
C
C

N
N
K
E

N
N
K
W

N
N
E
k

N
N
K
W
C
C

N
N
K
D
M
G

N
N
k

B
N

5
.
4
C

x
e
d
n
I

80

Md. E. Kadir et al.

For comparison, all the classiﬁers are ranked on each dataset in terms of AUC,
with ranking of 1 is the best. We also perform Friedman tests on the ranks.
After rejecting the null hypothesis using Friedman test that all the classiﬁers
are equivalent, a post-hoc test called Nemenyi test [15] is used to determine the
performance of which classiﬁer is signiﬁcantly better than the others.

4.3 Result and Discussion

Table 2 represents the comparison of 14 classiﬁers over 30 imbalanced datasets.
The average ranks of these classiﬁers indicate that kNN is better performing algo-
rithm compared to other traditional classiﬁers on imbalanced datasets. Though
kNN performs better than C4.5, modiﬁcations of tree based algorithms for imbal-
anced datasets perform better than kNN. Moreover, kNN on SMOTE sampled
datasets performs slightly better than kNN without sampling.

Now, if we compare kNN with its diﬀerent variants, it can be observed that
kENN and WKNN improve the overall performance of traditional kNN although
another variant CCWKNN fails to improve the performance in most cases over
the experimented datasets. Moreover, it is investigated that, the recent general-
ized mean based kNN approach GMDKNN performs worse than kNN on imbal-
anced datasets. In contrast, we can observe from Table 2 that, EKNN performs
better than all other classiﬁers except the proposed sPEkNN and mPEkNN.
It indicates that, handling uncertainty can improve the performance of kNN
on imbalanced datasets. Finally, average ranks show that mPEkNN is the best
performing classiﬁer compared to others in the imbalanced datasets.

In addition, Table 2 summarizes the counts of Win-Tie-Loss (W-T-L) of
sPEkNN and mPEkNN against other classiﬁers which indicates that mPEkNN
performs better than other classiﬁers in most cases. From Win-Tie-Loss, it is
observed that mPEkNN wins in at most 29 datasets with no loss against C4.5
and GMDKNN classiﬁers. In the least case, mPEkNN performs better in 19
datasets and worse in 7 datasets compared to EKNN.

The results of Friedman test (Fr. Test) with two base classiﬁers (sPEkNN and
mPEkNN) are shown in the last two lines of the Table 2. From Friedman test with
14 classiﬁers and 30 datasets, we can conclude that, all the fourteen classiﬁers
are not equivalent. After rejecting that all fourteen classiﬁers perform equivalent,
Nemenyi test is performed to determine which classiﬁer performs signiﬁcantly
better than the others. A tick((cid:2)) sign under a classiﬁer indicates that Nemenyi
test suggests the performance of that classiﬁer is signiﬁcantly diﬀerent from
the base classiﬁer in pairwise comparison at 95% conﬁdence level. Nemenyi test
states that, sPEkNN performs signiﬁcantly better than all compared classiﬁers
except EKNN, CCPDT and HDDT. More importantly, the test suggests that
mPEkNN is the best performing classiﬁer among twelve classiﬁers.

Proximity Weighted Evidential k NN

81

4.4 Eﬀects of Neighborhood Size and Imbalance Ratio

Here, we show the eﬀects of neighborhood size and Imbalance Ration (IR) on the
performance of the proposed method compared to other kNN variants. Due to
page limitations, only one dataset (Ionosphere) is used to present the comparison
in terms of AUC with diﬀerent the values of k ranging from 1 to 20. It is clear
from Fig. 2a that sPEkNN and mPEkNN consistently perform better than the
other algorithms and are less sensitive to the value of k.

(a) Eﬀects of neighborhood size, k (b) Eﬀects of Imbalance Ratio, IR

Fig. 2. Performance comparison among the algorithms belonging in kNN family

To visualize the eﬀect of IR, we use a synthetic dataset of two-class problem
in a two-dimensional space where instances of each class are taken from two
Gaussian distributions. The characteristics of the dataset is given below where
class-A is the minority class and Class-B is the majority class.

(cid:6)T , μA
(cid:5)
(cid:5)−2 −2
3 3
2 =
(cid:5)
(cid:5)
(cid:6)T , μB
4 3
0 0
2 =
1 =

(cid:6)T , ΣA
(cid:6)T , ΣB

1 = 3I and ΣA
1 = 8I and ΣB

2 = I
2 = I

ηA
1 = 0.6, ηA

2 = 0.4, μA

1 =
2 = 0.1, μB

ηB
1 = 0.9, ηB

Here η represents the mixture proportion and I is the identity matrix. Diﬀerent
datasets of 1500 samples are generated varying the class imbalance ratio ranging
from 2 to 10. It is observable from Fig. 2b that, although the imbalance ratio
increases, the performance of mPEkNN remains more steady compared to other
kNN variants indicating less sensitivity of mPEkNN in these synthetic datasets.

5 Conclusion

This paper proposes an extended kNN algorithm to increase the performance of
existing kNN by making it vigorous to imbalance class problem. In PEkNN, for
a query instance, we calculate a conﬁdence for each neighbor instance from the
posterior probability of that instance which is then discounted by the proximity
of that instance from the query instance. We show that this proximity weighted
conﬁdence increases the likelihood of classifying a minority class instance. To
calculate the conﬁdence we used two methods one using single Gaussian model

82

Md. E. Kadir et al.

(sPEkNN) and other using Gaussian mixture model (mPEkNN). Results over
30 datasets provide the evidence that the proposed approach is better than
twelve relevant methods in imbalanced datasets. However, one limitation of the
proposed method is that we assume all the feature values as numeric. As future
research direction, we have plan to extend the work for categorical features.

Acknowledgments. This research is supported by the fellowship from ICT Division,
Ministry of Posts, Telecommunications and Information Technology, Bangladesh. Grant
No - 56.00.0000.028.33.093.19-427; Dated 20.11.2019.

