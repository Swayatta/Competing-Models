Proceedings of NAACL-HLT 2018, pages 1470–1480

New Orleans, Louisiana, June 1 - 6, 2018. c(cid:13)2018 Association for Computational Linguistics

1470

KBGAN:AdversarialLearningforKnowledgeGraphEmbeddingsLiweiCaiDepartmentofElectronicEngineeringTsinghuaUniversityBeijing100084Chinacai.lw123@gmail.comWilliamYangWangDepartmentofComputerScienceUniversityofCalifornia,SantaBarbaraSantaBarbara,CA93106USAwilliam@cs.ucsb.eduAbstractWeintroduceKBGAN,anadversariallearningframeworktoimprovetheperformancesofawiderangeofexistingknowledgegraphem-beddingmodels.Becauseknowledgegraphstypicallyonlycontainpositivefacts,samplingusefulnegativetrainingexamplesisanon-trivialtask.Replacingtheheadortailentityofafactwithauniformlyrandomlyselectedentityisaconventionalmethodforgenerat-ingnegativefacts,butthemajorityofthegen-eratednegativefactscanbeeasilydiscrimi-natedfrompositivefacts,andwillcontributelittletowardsthetraining.Inspiredbygenera-tiveadversarialnetworks(GANs),weuseoneknowledgegraphembeddingmodelasaneg-ativesamplegeneratortoassistthetrainingofourdesiredmodel,whichactsasthedis-criminatorinGANs.Thisframeworkisinde-pendentoftheconcreteformofgeneratoranddiscriminator,andthereforecanutilizeawidevarietyofknowledgegraphembeddingmod-elsasitsbuildingblocks.Inexperiments,weadversariallytraintwotranslation-basedmod-els,TRANSEandTRANSD,eachwithassis-tancefromoneofthetwoprobability-basedmodels,DISTMULTandCOMPLEX.Weeval-uatetheperformancesofKBGANonthelinkpredictiontask,usingthreeknowledgebasecompletiondatasets:FB15k-237,WN18andWN18RR.Experimentalresultsshowthatad-versarialtrainingsubstantiallyimprovestheperformancesoftargetembeddingmodelsun-dervarioussettings.1IntroductionKnowledgegraph(Dongetal.,2014)isapow-erfulgraphstructurethatcanprovidedirectac-cessofknowledgetousersviavariousapplica-tionssuchasstructuredsearch,questionanswer-ing,andintelligentvirtualassistant.AcommonrepresentationofknowledgegraphbeliefsisintheformofadiscreterelationaltriplesuchasLocate-dIn(NewOrleans,Louisiana).Amainchallengeforusingdiscreterepresen-tationofknowledgegraphisthelackofcapa-bilityofaccessingthesimilaritiesamongdiffer-ententitiesandrelations.Knowledgegraphem-bedding(KGE)techniques(e.g.,RESCAL(Nickeletal.,2011),TRANSE(Bordesetal.,2013),DIST-MULT(Yangetal.,2015),andCOMPLEX(Trouil-lonetal.,2016))havebeenproposedinrecentyearstodealwiththeissue.Themainideaistorepresenttheentitiesandrelationsinavec-torspace,andonecanusemachinelearningtech-niquetolearnthecontinuousrepresentationoftheknowledgegraphinthelatentspace.However,evensteadyprogresshasbeenmadeindevelopingnovelalgorithmsforknowledgegraphembedding,thereisstillacommonchal-lengeinthislineofresearch.Forspaceefﬁ-ciency,commonknowledgegraphssuchasFree-base(Bollackeretal.,2008),Yago(Suchaneketal.,2007),andNELL(Mitchelletal.,2015)bydefaultonlystoresbeliefs,ratherthandisbeliefs.Therefore,whentrainingtheembeddingmodels,thereisonlythenaturalpresenceofthepositiveexamples.Tousenegativeexamples,acommonmethodistoremovethecorrecttailentity,andran-domlysamplefromauniformdistribution(Bordesetal.,2013).Unfortunately,thisapproachisnotideal,becausethesampledentitycouldbecom-pletelyunrelatedtotheheadandthetargetre-lation,andthusthequalityofrandomlygener-atednegativeexamplesisoftenpoor(e.g,Locate-dIn(NewOrleans,BarackObama)).Otherapproachmightleverageexternalontologicalconstraintssuchasentitytypes(Krompaßetal.,2015)togen-eratenegativeexamples,butsuchresourcedoesnotalwaysexistoraccessible.Inthiswork,weprovideagenericsolutiontoimprovethetrainingofawiderangeofknowl-1471

ModelScorefunctionf(h,r,t)NumberofparametersTRANSE||h+r−t||1/2k|E|+k|R|TRANSD||(I+rphpT)h+r−(I+rptpT)t||1/22k|E|+2k|R|DISTMULT<h,r,t>(=Pki=1hiriti)k|E|+k|R|COMPLEX<h,r,¯t>(h,r,t∈Ck)2k|E|+2k|R|TRANSH||(I−rprpT)h+r−(I+rprpT)t||1/2k|E|+2k|R|TRANSR||Wrh+r−Wrt||1/2k|E|+(k2+k)|R|MANIFOLDE(hyperplane)|(h+rhead)T(t+rtail)−Dr|k|E|+(2k+1)|R|RESCALhTWrtk|E|+k2|R|HOLErT(h?t)(?iscircularcorrelation)k|E|+k|R|CONVEf(vec(f([¯h;¯r]∗ω))W)tk|E|+k|R|+kcmnTable1:Someselectedknowledgegraphembeddingmodels.Thefourmodelsabovethedoublelineareconsideredinthispaper.ExceptforCOMPLEX,allboldfacelowercaselettersrepresentvectorsinRk,andboldfaceuppercaselettersrepresentmatricesinRk×k.Iistheidentitymatrix.edgegraphembeddingmodels.Inspiredbytherecentadvancesofgenerativeadversarialdeepmodels(Goodfellowetal.,2014),weproposeanoveladversariallearningframework,namely,KBGAN,forgeneratingbetternegativeexam-plestotrainknowledgegraphembeddingmod-els.Morespeciﬁcally,weconsiderprobability-based,log-lossembeddingmodelsasthegener-atortosupplybetterqualitynegativeexamples,andusedistance-based,margin-lossembeddingmodelsasthediscriminatortogeneratetheﬁnalknowledgegraphembeddings.Sincethegenera-torhasadiscretegenerationstep,wecannotdi-rectlyusethegradient-basedapproachtoback-propagatetheerrors.Wethenconsideraone-stepreinforcementlearningsetting,anduseavariance-reductionREINFORCEmethodtoachievethisgoal.Empirically,weperformexperimentsonthreecommonKGEdatasets(FB15K-237,WN18andWN18RR),andverifytheadversariallearningapproachwithasetofKGEmodels.Ourexper-imentsshowthatacrossvarioussettings,thisad-versariallearningmechanismcansigniﬁcantlyim-provetheperformanceofsomeofthemostcom-monlyusedtranslationbasedKGEmethods.Ourcontributionsarethree-fold:•Wearetheﬁrsttoconsideradversariallearn-ingtogenerateusefulnegativetrainingexam-plestoimproveknowledgegraphembedding.•ThisadversariallearningframeworkappliestoawiderangeofKGEmodels,withouttheneedofexternalontologiesconstraints.•OurmethodshowsconsistentperformancegainsonthreecommonlyusedKGEdatasets.2RelatedWork2.1KnowledgeGraphEmbeddingsAlargenumberofknowledgegraphembeddingmodels,whichrepresententitiesandrelationsinaknowledgegraphwithvectorsormatrices,havebeenproposedinrecentyears.RESCAL(Nickeletal.,2011)isoneoftheearlieststudiesonma-trixfactorizationbasedknowledgegraphembed-dingmodels,usingabilinearformasscorefunc-tion.TRANSE(Bordesetal.,2013)istheﬁrstmodeltointroducetranslation-basedembedding.Latervariants,suchasTRANSH(Wangetal.,2014),TRANSR(Linetal.,2015)andTRANSD(Jietal.,2015),extendTRANSEbyprojectingtheembeddingvectorsofentitiesintovariousspaces.DISTMULT(Yangetal.,2015)simpliﬁesRESCALbyonlyusingadiagonalmatrix,andCOMPLEX(Trouillonetal.,2016)extendsDISTMULTintothecomplexnumberﬁeld.(Nickeletal.,2015)isacomprehensivesurveyonthesemodels.Someofthemorerecentmodelsachievestrongperformances.MANIFOLDE(Xiaoetal.,2016)embedsatripleasamanifoldratherthanapoint.HOLE(Nickeletal.,2016)employscircularcor-relationtocombinethetwoentitiesinatriple.CONVE(Dettmersetal.,2017)usesaconvolu-tionalneuralnetworkasthescorefunction.How-ever,mostofthesestudiesuseuniformsamplingtogeneratenegativetrainingexamples(Bordesetal.,2013).Becauseourframeworkisindepen-dentoftheconcreteformofmodels,allthesemod-elscanbepotentiallyincorporatedintoourframe-work,regardlessofthecomplexity.Asaproofofprinciple,ourworkfocusesonsimplermodels.Ta-ble1summarizesthescorefunctionsanddimen-sionsofallmodelsmentionedabove.1472

2.2GenerativeAdversarialNetworksanditsVariantsGenerativeAdversarialNetworks(GANs)(Good-fellowetal.,2014)wasoriginallyproposedforgeneratingsamplesinacontinuousspacesuchasimages.AGANconsistsoftwoparts,thegenera-torandthediscriminator.Thegeneratoracceptsanoiseinputandoutputsanimage.Thediscrimina-torisaclassiﬁerwhichclassiﬁesimagesas“true”(fromthegroundtruthset)or“fake”(generatedbythegenerator).WhentrainingaGAN,thegenera-torandthediscriminatorplayaminimaxgame,inwhichthegeneratortriestogenerate“real”imagestodeceivethediscriminator,andthediscriminatortriestotellthemapartfromgroundtruthimages.GANsarealsocapableofgeneratingsamplessat-isfyingcertainrequirements,suchasconditionalGAN(MirzaandOsindero,2014).ItisnotpossibletouseGANsinitsoriginalformforgeneratingdiscretesampleslikenaturallan-guagesentencesorknowledgegraphtriples,be-causethediscretesamplingsteppreventsgradi-entsfrompropagatingbacktothegenerator.SE-QGAN(Yuetal.,2017)isoneoftheﬁrstsuccess-fulsolutionstothisproblembyusingreinforce-mentlearning—Ittrainsthegeneratorusingpol-icygradientandothertricks.IRGAN(Wangetal.,2017)isarecentworkwhichcombinestwocate-goriesofinformationretrievalmodelsintoadis-creteGANframework.Likewise,ourframeworkreliesonpolicygradienttotrainthegeneratorwhichprovidesdiscretenegativetriples.ThediscriminatorinaGANisnotnecessarilyaclassiﬁer.WassersteinGANorWGAN(Arjovskyetal.,2017)usesaregressorwithclippedparam-etersasitsdiscriminator,basedonsolidanalysisaboutthemathematicalnatureofGANs.GOGAN(Juefei-Xuetal.,2017)furtherreplacesthelossfunctioninWGANwithmarginalloss.Althoughoriginatingfromverydifferentﬁelds,theformoflossfunctioninourframeworkturnsouttobemorecloselyrelatedtotheoneinGOGAN.3OurApproachesInthissection,weﬁrstdeﬁnetwotypesoftrainingobjectivesinknowledgegraphembeddingmod-elstoshowhowKBGANcanbeapplied.Then,wedemonstratealongoverlookedproblemaboutnegativesamplingwhichmotivatesustoproposeKBGANtoaddresstheproblem.Finally,wediveintothemathematical,andalgorithmicdetailsofKBGAN.3.1TypesofTrainingObjectivesForagivenknowledgegraph,letEbethesetofentities,Rbethesetofrelations,andTbethesetofgroundtruthtriples.Ingeneral,aknowledgegraphembedding(KGE)modelcanbeformulatedasascorefunctionf(h,r,t),h,t∈E,r∈Rwhichassignsascoretoeverypossibletripleintheknowledgegraph.Theestimatedlikelihoodofatripletobetruedependsonlyonitsscoregivenbythescorefunction.Differentmodelsformulatetheirscorefunctionbasedondifferentdesigns,andthereforeinterpretscoresdifferently,whichfurtherleadtovarioustrainingobjectives.Twocommonformsoftrain-ingobjectivesareparticularlyofourinterest:Marginallossfunctioniscommonlyusedbyalargegroupofmodelscalledtranslation-basedmodels,whosescorefunctionmodelsdistancebetweenpointsorvectors,suchasTRANSE,TRANSH,TRANSR,TRANSDandsoon.Inthesemodels,smallerdistanceindicatesahigherlikeli-hoodoftruth,butonlyqualitatively.Themarginallossfunctiontakesthefollowingform:Lm=X(h,r,t)∈T[f(h,r,t)−f(h0,r,t0)+γ]+(1)whereγisthemargin,[·]+=max(0,·)isthehingefunction,and(h0,r,t0)isanegativetriple.Thenegativetripleisgeneratedbyreplacingtheheadentityorthetailentityofapositivetriplewitharandomentityintheknowledgegraph,orformally(h0,r,t0)∈{(h0,r,t)|h0∈E}∪{(h,r,t0)|t0∈E}.Log-softmaxlossfunctioniscommonlyusedbymodelswhosescorefunctionhasprobabilisticin-terpretation.SomenotableexamplesareRESCAL,DISTMULT,COMPLEX.Applyingthesoftmaxfunctiononscoresofagivensetoftriplesgivestheprobabilityofatripletobethebestoneamongthem:p(h,r,t)=expf(h,r,t)P(h0,r,t0)expf(h0,r,t0).Thelossfunctionisthenegativelog-likelihoodofthisprob-abilisticmodel:Ll=X(h,r,t)∈T−logexpf(h,r,t)Pexpf(h0,r,t0)(h0,r,t0)∈{(h,r,t)}∪Neg(h,r,t)(2)whereNeg(h,r,t)⊂{(h0,r,t)|h0∈E}∪{(h,r,t0)|t0∈E}isasetofsampledcorruptedtriples.1473

Figure1:AnoverviewoftheKBGANframework.Thegenerator(G)calculatesaprobabilitydistributionoverasetofcandidatenegativetriples,thensampleonetriplesfromthedistributionastheoutput.Thediscriminator(D)receivesthegeneratednegativetripleaswellasthegroundtruthtriple(inthehexag-onalbox),andcalculatestheirscores.Gminimizesthescoreofthegeneratednegativetriplebypolicygradient,andDminimizesthemarginallossbetweenpositiveandnegativetriplesbygradientdescent.Otherformsoflossfunctionsexist,forexam-pleCONVEusesatriple-wiselogisticfunctiontomodelhowlikelythetripleistrue,butbyfarthetwodescribedabovearethemostcommon.Also,softmaxfunctiongivesanprobabilisticdistribu-tionoverasetoftriples,whichisnecessaryforageneratortosamplefromthem.3.2WeaknessofUniformNegativeSamplingMostpreviousKGEmodelsuseuniformnegativesamplingforgeneratingnegativetriples,thatis,re-placingtheheadortailentityofapositivetriplewithanyoftheentitiesinE,allwithequalprob-ability.Mostofthenegativetriplesgeneratedinthiswaycontributelittletolearninganeffectiveembedding,becausetheyaretooobviouslyfalse.Todemonstratethisissue,letusconsiderthefollowingexample.SupposewehaveagroundtruthtripleLocatedIn(NewOrleans,Louisiana),andcorruptitbyreplacingitstailentity.First,weremovethetailentity,leavingLo-catedIn(NewOrleans,?).BecausetherelationLo-catedInconstraintstypesofitsentities,“?”mustbeageographicalregion.Ifweﬁll“?”witharandomentitye∈E,theprob-abilityofehavingawrongtypeisveryhigh,resultinginridiculoustripleslikeLo-catedIn(NewOrleans,BarackObama)orLocate-dIn(NewOrleans,StarTrek).Suchtriplesarecon-sidered“tooeasy”,becausetheycanbeelim-inatedsolelybytypes.Incontrast,Locate-dIn(NewOrleans,Florida)isaveryusefulnegativetriple,becauseitsatisﬁestypeconstraints,butitcannotbeprovedwrongwithoutdetailedknowl-edgeofAmericangeography.IfaKGEmodelisfedwithmostly“tooeasy”negativeexamples,itwouldprobablyonlylearntorepresenttypes,nottheunderlyingsemantics.Theproblemislessseveretomodelsusinglog-softmaxlossfunction,becausetheytypicallysam-plestensorhundredsofnegativetriplesforonepositivetripleineachiteration,anditislikelytohaveafewusefulnegativesamongthem.Forin-stance,(Trouillonetal.,2016)foundthata100:1negative-to-positiveratioresultsinthebestper-formanceforCOMPLEX.However,formarginallossfunction,whosenegative-to-positiveratioisalways1:1,thelowqualityofuniformlysamplednegativescanseriouslydamagetheirperformance.3.3GenerativeAdversarialTrainingforKnowledgeGraphEmbeddingModelsInspiredbyGANs,weproposeanadversarialtrainingframeworknamedKBGANwhichusesaKGEmodelwithsoftmaxprobabilitiestopro-videhigh-qualitynegativesamplesforthetrain-ingofaKGEmodelwhosetrainingobjectiveismarginallossfunction.Thisframeworkisinde-pendentofthescorefunctionsofthesetwomod-els,andthereforepossessessomeextentofuniver-sality.Figure1illustratestheoverallstructureofKBGAN.InparalleltoterminologiesusedinGANliter-ature,wewillsimplycallthesetwomodelsgen-eratoranddiscriminatorrespectivelyintherestofthispaper.Weusesoftmaxprobabilisticmod-elsasthegeneratorbecausetheycanadequatelymodelthe“samplingfromaprobabilitydistribu-1474

Algorithm1:TheKBGANalgorithmData:trainingsetofpositivefacttriplesT={(h,r,t)}Input:Pre-trainedgeneratorGwithparametersθGandscorefunctionfG(h,r,t),andpre-traineddiscriminatorDwithparametersθDandscorefunctionfD(h,r,t)Output:Adversariallytraineddiscriminator1b←−0;//baselineforpolicygradient2repeat3Sampleamini-batchofdataTbatchfromT;4GG←−0,GD←−0;//gradientsofparametersofGandD5rsum←−0;//forcalculatingthebaseline6for(h,r,t)∈Tbatchdo7UniformlyrandomlysampleNsnegativetriplesNeg(h,r,t)={(h0i,r,t0i)}i=1...Ns;8Obtaintheirprobabilityofbeinggenerated:pi=expfG(h0i,r,t0i)PNsj=1expfG(h0j,r,t0j);9Sampleonenegativetriple(h0s,r,t0s)fromNeg(h,r,t)accordingto{pi}i=1...Ns.Assumeitsprobabilitytobeps;10GD←−GD+∇θD[fD(h,r,t)−fD(h0s,r,t0s)+γ]+;//accumulategradientsforD11r←−−fD(h0s,r,t0s),rsum←−rsum+r;//risthereward12GG←−GG+(r−b)∇θGlogps;//accumulategradientsforG13end14θG←−θG+ηGGG,θD←−θD−ηDGD;//updateparameters15b←rsum/|Tbatch|;//updatebaseline16untilconvergence;tion”processofdiscreteGANs,andweaimatimprovingdiscriminatorsbasedonmarginallossbecausetheycanbeneﬁtmorefromhigh-qualitynegativesamples.Notethatamajordifferencebe-tweenGANandourworkisthat,theultimategoalofourframeworkistoproduceagooddiscrimi-nator,whereasGANSareaimedattrainingagoodgenerator.Inaddition,thediscriminatorhereisnotaclassiﬁerasitwouldbeinmostGANs.Intuitively,thediscriminatorshouldassignarel-ativelysmalldistancetoahigh-qualitynegativesample.Inordertoencouragethegeneratortogen-erateusefulnegativesamples,theobjectiveofthegeneratoristominimizethedistancegivenbydis-criminatorforitsgeneratedtriples.Andjustliketheordinarytrainingprocess,theobjectiveofthediscriminatoristominimizethemarginallossbe-tweenthepositivetripleandthegeneratednega-tivetriple.Inanadversarialtrainingsetting,thegeneratorandthediscriminatorarealternativelytrainedtowardstheirrespectiveobjectives.SupposethatthegeneratorproducesaprobabilitydistributiononnegativetriplespG(h0,r,t0|h,r,t)givenapositivetriple(h,r,t),andgeneratesnegativetriples(h0,r,t0)bysam-plingfromthisdistribution.LetfD(h,r,t)bethescorefunctionofthediscriminator.Theob-jectiveofthediscriminatorcanbeformulatedasminimizingthefollowingmarginallossfunction:LD=X(h,r,t)∈T[fD(h,r,t)−fD(h0,r,t0)+γ]+(h0,r,t0)∼pG(h0,r,t0|h,r,t)(3)TheonlydifferencebetweenthislossfunctionandEquation1isthatitusesnegativesamplesfromthegenerator.Theobjectiveofthegeneratorcanbeformu-latedasmaximizingthefollowingexpectationofnegativedistances:RG=X(h,r,t)∈TE[−fD(h0,r,t0)](h0,r,t0)∼pG(h0,r,t0|h,r,t)(4)RGinvolvesadiscretesamplingstep,sowecannotﬁnditsgradientwithsimpledifferentiation.WeuseasimplespecialcaseofPolicyGradientTheorem1(Suttonetal.,2000)toobtainthegradi-entofRGwithrespecttoparametersofthegener-ator:∇GRG=X(h,r,t)∈TE(h0,r,t0)∼pG(h0,r,t0|h,r,t)[−fD(h0,r,t0)∇GlogpG(h0,r,t0|h,r,t)]’X(h,r,t)∈T1NX(h0i,r,t0i)∼pG(h0,r,t0|h,r,t),i=1...N[−fD(h0,r,t0)∇GlogpG(h0,r,t0|h,r,t)](5)1Aproofcanbefoundinthesupplementarymaterial1475

ModelHyperparametersConstraintsorRegularizationsTRANSEL1distance,k=50,γ=3||e||2≤1,||r||2≤1TRANSDL1distance,k=50,γ=3||e||2≤1,||r||2≤1,||ep||2≤1,||rp||2≤1DISTMULTk=50,λ=1/0.1L2regularization:Lreg=L+λ||Θ||22COMPLEX2k=50,λ=1/0.1L2regularization:Lreg=L+λ||Θ||22Table2:Hyperparametersettingsofthe4modelsweused.ForDISTMULTandCOMPLEX,λ=1isusedforFB15k-237andλ=0.1isusedforWN18andWN18RR.Allotherhyperparametersaresharedamongalldatasets.ListhegloballossdeﬁnedinEquation(2).Θrepresentsallparametersinthemodel.Dataset#r#ent.#train#val#testFB15k-23723714,541272,11517,53520,466WN181840,943141,4425,0005,000WN18RR1140,94386,8353,0343,134Table3:Statisticsofdatasetsweusedintheexper-iments.“r”:relations.wherethesecondapproximateequalitymeansweapproximatetheexpectationwithsamplinginpractice.NowwecancalculatethegradientofRGandoptimizeitwithgradient-basedalgorithms.PolicyGradientTheoremarisesfromreinforce-mentlearning(RL),sowewouldliketodrawananalogybetweenourmodelandanRLmodel.Thegeneratorcanbeviewedasanagentwhichinter-actswiththeenvironmentbyperformingactionsandimprovesitselfbymaximizingtherewardre-turnedfromtheenvironmentinresponseofitsac-tions.Correspondingly,thediscriminatorcanbeviewedastheenvironment.UsingRLterminolo-gies,(h,r,t)isthestate(whichdetermineswhatactionstheactorcantake),pG(h0,r,t0|h,r,t)isthepolicy(howtheactorchooseactions),(h0,r,t0)istheaction,and−fD(h0,r,t0)isthereward.ThemethodofoptimizingRGdescribedaboveiscalledREINFORCE(Williams,1992)algorithminRL.OurmodelisasimplespecialcaseofRL,calledone-stepRL.InatypicalRLsetting,eachactionperformedbytheagentwillchangeitsstate,andtheagentwillperformaseriesofactions(calledanepoch)untilitreachescertainstatesorthenumberofactionsreachesacertainlimit.However,intheanalogyabove,actionsdoesnotaffectthestate,andaftereachactionwerestartwithanotherunrelatedstate,soeachepochcon-sistsofonlyoneaction.ToreducethevarianceofREINFORCEal-gorithm,itiscommontosubtractabase-linefromthereward,whichisanarbitrarynumberthatonlydependsonthestate,with-outaffectingtheexpectationofgradients.2Inourcase,wereplace−fD(h0,r,t0)with−fD(h0,r,t0)−b(h,r,t)intheequationabovetointroducethebaseline.Toavoidintroducingnewparameters,wesimplyletbbeaconstant,theaveragerewardofthewholetrainingset:b=P(h,r,t)∈TE(h0,r,t0)∼pG(h0,r,t0|h,r,t)[−fD(h0,r,t0)].Inpractice,bisapproximatedbythemeanofrewardsofrecentlygeneratednegativetriples.Letthegenerator’sscorefunctiontobefG(h,r,t),givenasetofcandidatenegativetriplesNeg(h,r,t)⊂{(h0,r,t)|h0∈E}∪{(h,r,t0)|t0∈E},theprobabilitydistributionpGismodeledas:pG(h0,r,t0|h,r,t)=expfG(h0,r,t0)PexpfG(h∗,r,t∗)(h∗,r,t∗)∈Neg(h,r,t)(6)Ideally,Neg(h,r,t)shouldcontainallpossiblenegatives.However,knowledgegraphsareusu-allyhighlyincomplete,sothe”hardest”negativetriplesareverylikelytobefalsenegatives(truefacts).Toaddressthisissue,weinsteadgenerateNeg(h,r,t)byuniformlysamplingofNsentities(asmallnumbercomparedtothenumberofallpossiblenegatives)fromEtoreplacehort.Be-causeinreal-worldknowledgegraphs,trueneg-ativesareusuallyfarmorethanfalsenegatives,suchsetwouldbeunlikelytocontainanyfalsenegative,andthenegativeselectedbythegener-atorwouldlikelybeatruenegative.UsingasmallNeg(h,r,t)canalsosigniﬁcantlyreducecompu-tationalcomplexity.Besides,weadoptthe“bern”samplingtech-nique(Wangetal.,2014)whichreplacesthe“1”sidein“1-to-N”and“N-to-1”relationswithhigherprobabilitytofurtherreducefalsenega-tives.Algorithm1summarizesthewholeadversarialtrainingprocess.Boththegeneratorandthedis-2Aproofofsuchfactcanalsobefoundinthesupplemen-tarymaterial1476

criminatorrequirepre-training,whichisthesameasconventionallytrainingasingleKBEmodelwithuniformnegativesampling.Formallyspeak-ing,onecanpre-trainthegeneratorbyminimiz-ingthelossfunctiondeﬁnedinEquation(1),andpre-trainthediscriminatorbyminimizingthelossfunctiondeﬁnedinEquation(2).Line14inthealgorithmassumesthatweareusingthevanillagradientdescentastheoptimizationmethod,butobviouslyonecansubstituteitwithanygradient-basedoptimizationalgorithm.4ExperimentsToevaluateourproposedframework,wetestitsperformanceforthelinkpredictiontaskwithdif-ferentgeneratorsanddiscriminators.Forthegen-erator,wechoosetwoclassicalprobability-basedKGEmodel,DISTMULTandCOMPLEX,andforthediscriminator,wealsochoosetwoclassi-caltranslation-basedKGEmodel,TRANSEandTRANSD,resultinginfourpossiblecombinationsofgeneratoranddiscriminatorintotal.SeeTable1forabriefsummaryofthesemodels.4.1ExperimentalSettings4.1.1DatasetsWeusethreecommonknowledgebasecom-pletiondatasetsforourexperiment:FB15k-237,WN18andWN18RR.FB15k-237isasubsetofFB15kintroducedby(ToutanovaandChen,2015),whichremovedredundantrelationsinFB15kandgreatlyreducedthenumberofrela-tions.Likewise,WN18RRisasubsetofWN18in-troducedby(Dettmersetal.,2017)whichremovesreversingrelationsanddramaticallyincreasesthedifﬁcultyofreasoning.BothFB15kandWN18areﬁrstintroducedby(Bordesetal.,2013)andhavebeencommonlyusedinknowledgegraphre-searches.StatisticsofdatasetsweusedareshowninTable3.4.1.2EvaluationProtocolsFollowingpreviousworkslike(Yangetal.,2015)and(Trouillonetal.,2016),foreachrun,were-porttwocommonmetrics,meanreciprocalrank-ing(MRR)andhitsat10(H@10).Weonlyre-portscoresundertheﬁlteredsetting(Bordesetal.,2013),whichremovesalltriplesappearedintrain-ing,validating,andtestingsetsfromcandidatetriplesbeforeobtainingtherankofthegroundtruthtriple.4.1.3ImplementationDetails3Inthepre-trainingstage,wetraineverymodeltoconvergencefor1000epochs,anddivideev-eryepochinto100mini-batches.Toavoidoverﬁt-ting,weadoptearlystoppingbyevaluatingMRRonthevalidationsetevery50epochs.Wetriedγ=0.5,1,2,3,4,5andL1,L2distancesforTRANSEandTRANSD,andλ=0.01,0.1,1,10forDISTMULTandCOMPLEX,anddeterminedthebesthyperparameterslistedontable2,basedontheirperformancesonthevalidationsetaf-terpre-training.Duetolimitedcomputationre-sources,wedeliberatelylimitthedimensionsofembeddingstok=50,similartotheoneusedinearlierworks,tosavetime.Wealsoapplycer-tainconstraintsorregularizationstothesemodels,whicharemostlythesameasthosedescribedintheiroriginalpublications,andalsolistedontable2.Intheadversarialtrainingstage,wekeepallthehyperparamtersdeterminedinthepre-trainingstageunchanged.Thenumberofcandidateneg-ativetriples,Ns,issetto20inallcases,whichisproventobeoptimalamongthecandidatesetof{5,10,20,30,50}.Wetrainfor5000epochs,with100mini-batchesforeachepoch.WealsouseearlystoppinginadversarialtrainingbyevaluatingMRRonthevalidationsetevery100epochs.Weusetheself-adaptiveoptimizationmethodAdam(KingmaandBa,2015)foralltrainings,andalwaysusetherecommendeddefaultsettingα=0.001,β1=0.9,β2=0.999,=10−8.4.2ResultsResultsofourexperimentsaswellasbaselinesareshowninTable4.Allsettingsofadversarialtrainingbringapronouncedimprovementtothemodel,whichindicatesthatourmethodiscon-sistentlyeffectiveinvariouscases.TRANSEper-formsslightlyworsethanTRANSDonFB15k-237andWN18,butbetteronWN18RR.UsingDIST-MULTorCOMPLEXasthegeneratordoesnotaf-fectperformancegreatly.TRANSEandTRANSDenhancedbyKBGANcansigniﬁcantlybeattheircorrespondingbaselineimplementations,andoutperformstrongerbase-linesinsomecases.Asaprototypicalandproof-of-principleexperiment,wehaveneverexpectedstate-of-the-artresults.Beingsimplemodelspro-3TheKBGANsourcecodeisavailableathttps://github.com/cai-lw/KBGAN1477

FB15k-237WN18WN18RRMethodMRRH@10MRRH@10MRRH@10TRANSE-42.8†-89.2-43.2†TRANSD-45.3†-92.2-42.8†DISTMULT24.1‡41.9‡82.293.642.5‡49.1‡COMPLEX24.0‡41.9‡94.194.744.4‡50.7‡TRANSE(pre-trained)24.242.243.391.518.645.9KBGAN(TRANSE+DISTMULT)27.445.071.094.921.348.1KBGAN(TRANSE+COMPLEX)27.845.370.594.921.047.9TRANSD(pre-trained)24.542.749.492.819.246.5KBGAN(TRANSD+DISTMULT)27.845.877.294.821.447.2KBGAN(TRANSD+COMPLEX)27.745.877.994.821.546.9Table4:Experimentalresults.ResultsofKBGANareresultsofitsdiscriminator(ontheleftofthe“+”sign).Underlinedresultsarethebestonesamongourimplementations.Resultsmarkedwith†arepro-ducedbyrunningFast-TransX(Linetal.,2015)withitsdefaultparameters.Resultsmarkedwith‡arecopiedfrom(Dettmersetal.,2017).Allotherbaselineresultsarecopiedfromtheiroriginalpapers.Figure2:LearningcurvesofKBGAN.Allmetricsimprovesteadilyastrainingproceeds.posedseveralyearsago,TRANSEandTRANSDhastheirlimitationsinexpressivenessthatareun-likelytobefullycompensatedbybettertrainingtechnique.Infutureresearches,peoplemaytryemployingmoreadvancedmodelsintoKBGAN,andwebelieveithasthepotentialtobecomestate-of-the-art.Toillustrateourtrainingprogress,weplotper-formancesofthediscriminatoronvalidationsetoverepochs,whicharedisplayedinFigure2.Asallthesegraphsshow,ourperformancesareal-waysinincreasingtrends,convergingtoitsmax-imumastrainingproceeds,whichindicatesthatKBGANisarobustGANthatcanconvergetogoodresultsinvarioussettings,althoughGANsarewell-knownfordifﬁcultyinconvergence.FluctuationsinthesegraphsmayseemmoreprominentthanotherKGEmodels,butisconsiderednormalforanadversiallytrainedmodel.Notethatinsomecasesthecurvestilltendstoriseafter5000epochs.Wedonothavesufﬁcientcomputationresourcetotrainformoreepochs,butwebelievethattheywillalsoeventuallyconverge.1478

PositivefactUniformrandomsampleTrainedgenerator(condensationNN2,derivationallyrelatedform,distillVB4)familyarcidaeNN1repastNN1beaterNN2coverallNN1cashadvanceNN1reviviﬁcationNN1mouthpieceNN3liquidbodysubstanceNN1stiffenVB2hotupVB1(coloradoriverNN2,instancehypernym,riverNN1)lunarcalendarNN1umbellulariacalifornicaNN1tonalityNN1creepy-crawlyNN1moorVB3idahoNN1sayanmountainsNN1lowersaxonyNN1orderciconiiformesNN1jabNN3(meetingNN2,hypernym,socialgatheringNN1)cellularJJ1commercialactivityNN1giantcaneNN1streptomycesNN1tranquillizeVB1attachVB1bondNN6heavysparNN1satelliteNN1peepVB3Table5:ExamplesofnegativesamplesinWN18dataset.Theﬁrstcolumnisthepositivefact,andtheterminboldistheonetobereplacedbyanentityinthenexttwocolumns.Thesecondcolumnconsistsofrandomentitiesdrawnfromthewholedataset.Thethirdcolumncontainsnegativesamplesgeneratedbythegeneratorinthelast5epochsoftraining.Entitiesinitalicareconsideredtohavesemanticrelationtothepositiveone4.3CasestudyTodemonstratethatourapproachdoesgeneratebetternegativesamples,welistsomeexamplesoftheminTable5,usingtheKBGAN(TRANSE+DISTMULT)modelandtheWN18dataset.Allhy-perparametersarethesameasthosedescribedinSection4.1.3.Comparedtouniformrandomnegativeswhicharealmostalwaystotallyunrelated,thegenera-torgeneratesmoresemanticallyrelatednegativesamples,whichisdifferentfromtyperelatednessweusedasexampleinSection3.2,butalsohelpstraining.Intheﬁrstexample,twooftheﬁvetermsarephysicallyrelatedtotheprocessofdistillingliquids.Inthesecondexample,threeoftheﬁveentitiesaregeographicalobjects.Inthethirdex-ample,twooftheﬁveentitiesexpresstheconceptof“gather”.BecausewedeliberatelylimitedthestrengthofgeneratednegativesbyusingasmallNsasde-scribedinSection3.3,thesemanticrelationisprettyweak,andtherearestillmanyunrelateden-tities.However,empiricalresults(whenselectingtheoptimalNs)showsthatsuchsituationismorebeneﬁcialfortrainingthediscriminatorthangen-eratingevenstrongernegatives.5ConclusionsWeproposeanoveladversariallearningmethodforimprovingawiderangeofknowledgegraphembeddingmodels—Wedesignedagenerator-discriminatorframeworkwithdualKGEcompo-nents.Unlikerandomuniformsampling,thegen-eratormodelgenerateshigherqualitynegativeex-amples,whichallowthediscriminatormodeltolearnbetter.Toenablebackpropagationoferror,weintroducedaone-stepREINFORCEmethodtoseamlesslyintegratethetwomodules.Experimen-tally,wetestedtheproposedideaswithfourcom-monlyusedKGEmodelsonthreedatasets,andtheresultsshowedthattheadversariallearningframe-workbroughtconsistentimprovementstovariousKGEmodelsunderdifferentsettings.1479

