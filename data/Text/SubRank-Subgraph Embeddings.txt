SubRank: Subgraph Embeddings
via a Subgraph Proximity Measure

Oana Balalau1(B) and Sagar Goyal2

1 Inria and École Polytechnique, Palaiseau, France

oana.balalau@inria.fr

2 Microsoft, Vancouver, Canada

sagoya@microsoft.com

Abstract. Representation learning for graph data has gained a lot of
attention in recent years. However, state-of-the-art research is focused
mostly on node embeddings, with little eﬀort dedicated to the closely
related task of computing subgraph embeddings. Subgraph embeddings
have many applications, such as community detection, cascade predic-
tion, and question answering. In this work, we propose a subgraph to sub-
graph proximity measure as a building block for a subgraph embedding
framework. Experiments on real-world datasets show that our approach,
SubRank, outperforms state-of-the-art methods on several important
data mining tasks.

Keywords: Subgraph embeddings · Personalized PageRank

1

Introduction

In recent years we have witnessed the success of graph representation learning
in many tasks such as community detection [8,19], link prediction [10,20], graph
classiﬁcation [3], and cascade growth prediction [13]. A large body of work has
focused on node embeddings, techniques that represent nodes as dense vectors
that preserve the properties of nodes in the original graph [5,9]. Representation
learning of larger structures has generally been associated with embedding collec-
tions of graphs [3]. Paths, subgraphs and communities embeddings have received
far less attention despite their importance in graphs. In homogeneous graphs,
subgraph embeddings have been used in community prediction [1,8], and cas-
cade growth prediction [6,13]. In heterogeneous graphs, subgraphs embedding
have tackled tasks such as semantic user search [14] and question answering [4].
Nevertheless, the techniques proposed in the literature for computing sub-
graph embeddings have at least one of the following two drawbacks: i) they are
supervised techniques and such they are dependent on annotated data and do
not generalize to other tasks; ii) they can tackle only a speciﬁc type of subgraph.

O. Balalau and S. Goyal—Part of this work was done while the authors were at Max
Planck Institute for Informatics, Germany.
c(cid:2) Springer Nature Switzerland AG 2020
H. W. Lauw et al. (Eds.): PAKDD 2020, LNAI 12084, pp. 487–498, 2020.
https://doi.org/10.1007/978-3-030-47426-3_38

488

O. Balalau and S. Goyal

Approach. In this work, we tackle the problem of computing subgraph embed-
dings in an unsupervised setting, where embeddings are trained for one task
and will be tested on diﬀerent tasks. We propose a subgraph embedding method
based on a novel subgraph proximity measure. Our measure is inspired by the
random walk proximity measure Personalized PageRank [11]. We show that our
subgraph embeddings are comprehensive and achieve competitive performance
on three important data mining tasks: community detection, link prediction, and
cascade growth prediction.
Contributions. Our salient contributions in this work are:
• We deﬁne a novel subgraph to subgraph proximity measure;
• We introduce a framework that learns comprehensive subgraphs embeddings;
• In a thorough experimental evaluation, we highlight the potential of our

method on a variety of data mining tasks.

2 Related Work

Node Embeddings. Methods for computing node embeddings aim to repre-
sent nodes as low-dimensional vectors that summarize properties of nodes, such
as their neighborhood. The numerous embedding techniques diﬀer in the com-
putational model and in what properties of nodes are conserved. For example,
in matrix factorization approaches, the goal is to perform dimension reduction
on a matrix that encodes the pairwise proximity of nodes, where proximity is
deﬁned as adjacency [2], k-step transitions [7], or Katz centrality [16]. Random
walk approaches have been inspired by the important progress achieved in the
NLP community in computing word embeddings [15]. These techniques opti-
mize node embeddings such that nodes co-occurring in short random walks in
the graph have similar embeddings [10,18]. Another successful technique is to
take as input a node and an embedding similarity distribution and minimizes
the KL-divergence between the two distributions [19,20].
Subgraph Embeddings. A natural follow-up question is how to compute
embeddings for larger structures in the graph, such as paths, arbitrary sub-
graphs, motifs or communities. In [1], the authors propose a method inspired
by ParagraphVector [12], where each subgraph is represented as a collection of
random walks. Subgraph and node embeddings are learned such that given a
subgraph and a random walk, we can predict the next node in the walk using
the subgraph embedding and the node embeddings. The approach is tested on
link prediction and on community detection, using ego-networks to represent
nodes. In [13], the authors present an end-to-end neural framework that given
in input the cascade graph, predicts the future growth of the cascade for a given
time period. A cascade graph is sampled for a set of random walks, which are
given as input to a gated neural network to predict the future size of the cascade.
[6] is similarly an end-to-end neural framework for cascade prediction, but based
on the Hawkes process. The method transforms the cascade into diﬀusion paths,
where each path describes the process of information propagation within the

SubRank: Subgraph Embeddings via a Subgraph Proximity Measure

489

observation time-frame. Another very important type of subgraph is a commu-
nity and in [8] community embeddings are represented as multivariate Gaussian
distributions.
Graph Embeddings. Given a collection of graphs, a graph embedding tech-
nique will learn representations for each graph. In [3], the authors propose an
inductive framework for computing graph embeddings, based on training an
attention network to predict a graph proximity measure, such as graph edit dis-
tance. Graph embeddings are closely related to graph kernels, functions that
measure the similarity between pairs of graphs [21]. Graph kernels are used
together with kernel methods such as SVM to perform graph classiﬁcation [22].

3 Feature Learning Framework

Preliminaries. PageRank [17] is the stationary distribution of a random walk
in which, at a given step, with a probability α, a surfer teleports to a random
node and with probability 1 − α, moves along a randomly chosen outgoing edge
of the current node. In Personalized PageRank (PPR) [11], instead of teleporting
to a random node with probability α, the surfer teleports to a randomly chosen
node from a set of predeﬁned seed nodes. Let P r(u) be the PageRank of node u
and P P R(u, v) be the PageRank score of node v personalized for seed node u.
Problem Statement. Given a directed graph G = (V, E), a set of subgraphs
S1, S2,··· , Sk of G and an integer d, compute the d-dimensional embeddings of
the subgraphs.

3.1 Subgraph Proximity Measure

We deﬁne a subgraph proximity measure inspired by Personalized PageRank.
Let Si and Sj be two subgraphs in a directed graph G. Their proximity in the
graph is:

px(Si, Sj) =

P RSi

(vi)

(vj) · P P R(vi, vj),

P RSj

(1)

(cid:2)

vi∈Si

(cid:2)

vj∈Sj

(vi) represents the PageRank of node vi in the subgraph Si, and
where P RSi
P P R(vi, vj) the PageRank of node vj personalized for node vi in the graph G.
When considering how to deﬁne proximity between subgraphs, our intuition
is as follows: important nodes in subgraph Si should be close to important nodes
in subgraph Sj. This condition is fulﬁlled as PageRank will give high scores to
important nodes in the subgraphs and Personalized PageRank will give high
scores to nodes that are “close” or “similar”. We note that our measure is a
similarity measure, hence subgraphs that are similar will receive a high proximity
score. We choose the term proximity to emphasis that our measure relates to
nearness in the graph, as it is computed using random walks.

We can interpret Eq. 1 using random walks, as follows: Alice is a random
surfer in the subgraph Si, Bob is a random surfer in the subgraph Sj, and Carol

490

O. Balalau and S. Goyal

is a random surfer in graph G. Alice decides to send a message to Bob via Carol.
(vi)) and she will reach
Carol starts from the current node Alice is visiting (P RSi
a node vj ∈ Sj with probability P P R(vi, vj). Bob will be there to receive the
message with probability P RSj
Normalized Proximity. Given a collection of subgraphs S = {S1, S2,··· Sk},
we normalize the proximity px(Si, Sj),∀j ∈ 1, k such that it can be interpreted
as a probability distribution. The normalized proximity for a subgraph Si is:

(vj).

ˆpx(Si, Sj) =

(cid:3)

px(Si, Sj)
Sk∈S px(Si, Sk)

(2)

Rank of a Subgraph. Similarly to PageRank, our proximity can inform us
of the importance of a subgraph. The normalized proximity given a collection
of subgraphs S1, S2,··· Sk can be expressed as a stochastic matrix, where each
row i encodes the normalized proximity given subgraph Si. The importance of
subgraph Si can be computed by summing up the elements of column i.
Sampling According to the Proximity Measure. Given a subgraph Si
in input, we present a procedure for eﬃciently sampling px(Si,·) introduced in
Eq. 1. We suppose that all the Pagerank vectors of the subgraphs {S1, S2,··· Sk}
have been precomputed. We ﬁrst select a node ni in Si according to distribution
P RSi. Secondly, we start a random walk from ni in the graph G and we select
nj, the last node in the walk before the teleportation. Lastly, node nj may
2 ··· . We return a subgraph Sj according to the
belong to several subgraphs Sj
(nj),··· . The procedure doesn’t require
normalized distribution P RSj
computing the Personalized Pagerank vectors, which saves us O(n2) space. We
shall use this procedure for computing embeddings, thus avoiding computing
and storing the full proximity measure px.

1, Sj
(nj), P RSj

1

2

3.2 Subgraph Embeddings via SubRank
Given a graph G = (V, E) and set of subgraphs of G, S = {S1, S2,··· , Sk},
we learn their representations as dense vectors, i.e. as embeddings. We extend
the framework in [20] proposed for computing node embeddings to an approach
for subgraph embeddings. In [20], the authors propose to learn node embed-
dings such that the embeddings preserve an input similarity distribution between
nodes. The similarities of a node v to any other node in the graph are repre-
w∈V simG(v, w) = 1. The
sented by the similarity distribution simG, where
corresponding embedding similarity distribution is simE. The optimization func-
tion of the learning algorithm minimizes the Kullback-Leibler (KL) divergence
between the two proximity distributions:

(cid:3)

KL(simG(v,·), simE(v,·))

(cid:2)

v∈V

SubRank: Subgraph Embeddings via a Subgraph Proximity Measure

491

The authors propose several options for instantiating simG, such as Person-
alized PageRank and adjacency similarity. The similarity between embeddings,
simE, is the normalized dot product of the vectors.

In order to adapt this approach to our case, we deﬁne the subgraph-to-
subgraph proximity simG to be the normalized proximity presented in Eq. 2.
The embedding similarity simE is computed in the same manner and the opti-
mization function now minimizes the divergence between distributions deﬁned
on our input subgraphs, i.e. simG, simE : S × S (cid:4)→ [0, 1]. In our experimen-
tal evaluation we use this method, which we refer to as SubRank. We note
that simG will not be fully computed, but approximated using the sampling
procedure presented in Sect. 3.1.

3.3 Applications

Proximity of Ego-Networks. Two very important tasks in graph mining are
community detection and link prediction. Suppose Alice is a computer scientist
and she joins Twitter. She starts following the updates of Andrew Ng, but also
the updates of her friends, Diana and John. Bob is also a computer scientist on
Twitter and he follows Andrew Ng, Jure Leskovec and his friend Julia. As shown
in Fig. 1, there is no path in the directed graph between Alice and Bob. A path-
based similarity measure between nodes Alice and Bob, such as Personalized
PageRank, will return similarity 0, while it will return high values between Alice
and Andrew Ng and between Bob and Andrew Ng. An optimization algorithm for
computing node embeddings will have to address this trade-oﬀ, with a potential
loss in the quality of the representations. Thus, we might miss that both Alice
and Bob are computer scientists. To address this issue we capture the information
stored in the neighbors of the nodes by considering ego-networks. Therefore in
our work, we represent a node v as its ego network of size k (the nodes reachable
from v in k steps). In Sect. 4, we perform quantitative analysis to validate our
intuition.

Fig. 1. Illustrative example for ego-network proximity.

Proximity of Cascade Subgraphs. In a graph, an information cascade can be
modeled as a directed tree, where the root represents the original content creator,
and the remaining nodes represent the content reshares. When considering the

492

O. Balalau and S. Goyal

task of predicting the future size of the cascade, the nodes already in the cascade
are important, as it very likely their neighbors will be aﬀected by the information
propagation. However, nodes that have reshared more recently the information
are more visible to their neighbors. When running PageRank on a directed tree,
we observe that nodes on the same level have the same score, and the score of
nodes increases as we increase the depth. Hence, two cascade trees will have a
high proximity score ˆpx if nodes that have joined later the cascades (i.e. are
on lower levels in the trees) are “close” or “similar” according to Personalized
Pagerank. In Sect. 5, we perform quantitative analysis and we show that our
approach gives better results than a method that gives equal importance to all
nodes in the cascade.

4 Feature Learning for Ego-Networks

Datasets. We perform experiments on ﬁve real-world graphs, described below.
We report their characteristics in Table 1.
• Citeseer1 is a citation network created from the CiteSeer digital library. Nodes
are publications and edges denote citations. The node labels represent ﬁelds
in computer science.
• Cora (see footnote 1) is also a citation network and the node labels represent
• Polblogs2 is a directed network of hyperlinks between political blogs dis-
cussing US politics. The labels correspond to republican and democrat blogs.
• Cithep3 is a directed network of citations in high energy physics phenomenol-
• DBLP (see footnote 3) is a co-authorship network where two authors are
connected if they published at least one paper together. The communities are
conferences in which the authors have published.

ogy. The network does not have ground-truth communities.

subﬁelds in machine learning.

Table 1. Dataset description: type, vertices V , edges E, node labels L.

|E|

Dataset Type
Citeseer Citation
Citation
Cora
Polblogs Hyperlink
Cithep Citation
DBLP

|V |
|L|
3.3K 4.7K 6
2.7K 5.4K 7
1.4K 19K
2
1
34K 421k
Co-authorship 66K 542K 20

1 https://linqs.soe.ucsc.edu/data.
2 http://networkrepository.com/polblogs.php.
3 https://snap.stanford.edu/data.

SubRank: Subgraph Embeddings via a Subgraph Proximity Measure

493

Competitors. We evaluate our method, SubRank, against several state-of-the-
art methods for node and subgraph embedding computation. For each method,
we used the code provided by the authors. We compare with:
• DeepWalk [18] learns node embeddings by sampling random walks, and then
applying the SkipGram model. The parameters are set to the recommended
values, i.e. walk length t = 80, γ = 80, and window size w = 10.
• node2vec [10] is a hyperparameter-supervised approach that extends Deep-
Walk. We ﬁne-tuned the hyperparameters p and q on each dataset and task.
In addition, r = 10, l = 80, k = 10, and the optimization is run for an epoch.
• LINE [19] proposes two proximity measures for computing two d-dimensional
vectors for each node. In our experiments, we use the second-order proximity,
as it can be used for both directed and undirected graphs. We run experiments
with T = 1000 samples and s = 5 negative samples, as described in the paper.
• VERSE [20] learns node embeddings that preserve the proximity of nodes
in the graph. We use Personalized PageRank as a proximity measure, the
default option proposed in the paper. We run the learning algorithm for 105
iterations.
• VerseAvg is a adaption of VERSE, in which the embedding of a node is
• sub2vec [1] computes subgraph embeddings and for the experimental evalu-
ation, we compute the embeddings of the ego networks. Using the guidelines
of the authors, for Cora, Citeseer and Polblogs we select ego networks of size
2 and for the denser networks Cithep and DBLP, ego networks of size 1.

the average of the VERSE embeddings of the nodes in its ego network.

For the ﬁrst four methods, node embeddings are used to represent nodes. For
sub2vec, SubRank and VerseAvg, the ego network embedding is the node
representation. The embeddings are used as node features for community detec-
tion and link prediction. We compute 128 dimensional embeddings.
Parameter Setting for SubRank. We represent each node by its ego network
of size 1. We run the learning algorithm for 105 iterations. Our code is public.4
Running Time SubRank. In the interest of space, we report only the time
required by SubRank for computing ego network embeddings. We run the exper-
iments on a Intel Xeon CPU E5-2667 v4 @ 3.20 GHz, using 40 threads. The
running times are as follows: 1 m 40 s Citeseer, 1 m 26 s Cora, 49 s Polblogs, 19 m
39 s for Cithep and 39 m 45 s for DBLP.

4.1 Node Clustering
We assess the quality of the embeddings in terms of their ability to capture
communities in a graph. For this, we use the k-means algorithm to cluster the
nodes embedded in the d-dimensional space. In Table 2 we report the Normalized
Mutual Information (NMI) with respect to the original label distribution. On
Polblogs, SubRank has a low NMI, while on Citeseer and Cora it outperforms
the other methods. On DBLP it has a comparative performance with VERSE.
4 https://github.com/nyxpho/subrank.

494

O. Balalau and S. Goyal

Table 2. Normalized Mutual Information (NMI) for node clustering.

Method

DeepWalk 0.015
node2vec
0.023
LINE
0.084
VERSE
0.103
VerseAvg 0.125
sub2vec
0.007
SubRank
0.179

Dataset
Citeseer Cora Polblogs DBLP
0.314
0.336
0.284
0.363
0.360
0.001
0.357

0.013
0.018
0.100
0.013
0.208 0.448
0.257
0.024
0.318
0.310
0.004
0.001
0.347 0.021

4.2 Node Classiﬁcation

Node classiﬁcation is the task of predicting the correct node labels in a graph.
For each dataset, we try several conﬁgurations by varying the percentage of
nodes used in training. We evaluate the methods using the micro and macro F 1
score, and we report the micro F 1, as both measures present similar trends. The
results are presented in Table 3. On Citeseer and Cora SubRank signiﬁcantly
outperforms the other methods. On Polblogs, SubRank performs similarly to
the other baselines, even though the embeddings achieved a low NMI score. On
DBLP, SubRank is the second best method.

4.3 Link Prediction
To create training data for link prediction, we randomly remove 10% of edges,
ensuring that each node retains at least one neighbor. This set represents the
ground truth in the test set, while we take the remaining graph as the training
set. In addition, we randomly sample an equal number of node pairs that have
no edge connecting them as negative samples in our test set. We then learn
embeddings on the graph without the 10% edges. Next, for each edge (u, v)
in the training or the test set, we obtain the edge features by computing the
Hadamard product of the embeddings for u and v. The Hadamard product has
shown a better performance than other operators for this task [10,20]. We report
the accuracy of the link prediction task in Table 4. Our method achieves the best
performance on 4 out of 5 datasets.

5 Feature Learning for Information Cascades

Given in input: i) a social network G = (V, E), captured at a time t0, ii) a
set of information cascades C that appear in G after the timestamp t0, and
that are captured after t1 duration from their creation, iii) a time window t2,
our goal is to predict the growth of a cascade, i.e. the number of new nodes a

SubRank: Subgraph Embeddings via a Subgraph Proximity Measure

495

Table 3. F 1 micro score for the classiﬁcation task.

% of labelled nodes

% of labelled nodes

Method
1% 5% 10% 20%
DeepWalk 20.95 19.98 22.26 26.90
node2vec 31.20 31.11 28.24 30.15
LINE
30.81 33.58 44.81 53.62
VERSE 24.48 31.61 41.52 51.66
VerseAvg 29.46 38.51 46.12 55.77
sub2vec 18.23 20.17 22.04 22.79
SubRank 40.46 48.52 55.75 61.66

Method
1% 5% 10% 20%
DeepWalk 46.40 45.82 47.53 50.25
node2vec 46.40 44.26 46.55 50.16
LINE
44.46 46.28 57.54 61.74
VERSE 46.77 50.48 58.57 65.25
VerseAvg 53.11 57.83 65.75 70.14
sub2vec 25.55 46.38 46.42 46.42
SubRank 65.08 71.78 73.66 76.83

(a) F 1 micro score for
classiﬁcation in Citeseer

(b) F 1 micro score for
classiﬁcation in Cora

% of labelled nodes

% of labelled nodes

Method
1% 5% 10% 20%
DeepWalk 83.19 87.78 89.63 89.68
node2vec 85.83 88.91 89.93 90.52
LINE
73.50 87.50 89.70 89.34
VERSE 81.30 86.86 87.77 87.75
VerseAvg 87.12 88.77 86.26 88.42
sub2vec 51.55 51.20 51.23 51.42
SubRank 85.70 88.94 88.59 88.33

Method
1% 5% 10% 20%
DeepWalk 41.43 46.79 50.77 54.09
node2vec 42.61 48.00 51.51 54.92
LINE
25.02 33.93 37.96 41.31
VERSE 44.22 48.21 51.14 54.81
VerseAvg 45.41 51.69 55.22 58.46
sub2vec
9.60
SubRank 45.67 50.84 54.70 57.06

7.71

6.00

8.85

(c) F 1 micro score for classiﬁ-
cation in Polblogs

(d) F 1 micro score for classiﬁ-
cation in DBLP

cascade acquires, at t1 + t2 time from its creation. Note that given a cascade
c = (Vc, Ec) ∈ C, we know that the nodes Vc are present in V , however c can
contain new edges not present in E.
Datasets. We select for evaluation two datasets from the literature:
• AMiner [13] represents cascades of scientiﬁc citations. We use the simpliﬁed
version made available by the authors5. The dataset contains a global citation
graph and the cascades graphs. A node in a graph represents an author and
an edge from a1 to a2 represents the citation of a2 in an article of a1. A
cascade shows all the citations of a given paper. There are 9860 nodes in the
global graph and 560 cascade graphs that are split into training, test and
validation sets. The global network is based on citations between 1992 and
2002, while the training set consists of papers published from 2003 to 2007.
Papers published in 2008 and 2009 are used for validation and testing. The
cascade graphs are captured at the end of 1 year and we predict the increase
in citations after 1 and 2 years.

5 https://github.com/chengli-um/DeepCas.

496

O. Balalau and S. Goyal

Table 4. Accuracy for link prediction.

Method

Dataset
Cora Citeseer Polblogs Cithep DBLP
72.24
97.83
75.58
98.56
84.85
98.26
99.27
92.18
51.08
50.28
94.10 99.30

DeepWalk 67.42
50.75
node2vec
67.04
59.54
LINE
71.49
61.48
VERSE
70.17
67.16
sub2vec
52.46
50.32
SubRank
80.10 82.10

81.93
82.76
83.72
85.83
53.30
82.70

• Sina Weibo [6] consists of retweet cascades occurring on June 1, 2016 on
the social network. Each node in the graph represent a Sina Weibo user,
and an edge between u1 and u2 represent a retweet of u2 by u1. Each cascade
corresponds to the retweets of one message. The global network is constructed
by the union of the cascades occurring in the ﬁrst half of the day, while the
training, test and validation cascades are taken from the second half of the
day. The cascades are captured after 1 h from the initial post timestamp and
we predict the increase in retweets in 1 h, 2 h and by the end of the day.

Table 5. Mean squared error (MSE) for predicted increase in cascade size.

AMiner
1 year 2 years 1 h
2.764

Sina Weibo
2 h
9.544

6.978

1 day
Time period
DeepCas
13.284
DeepHawkes 2.088 1.790 2.403 2.368 3.714
VERSE
5.862
SubRank
4.818

2.313
3.580
1.984 1.809 3.354

4.243
3.797

2.946

2.181

Competitors. We compare SubRank with the following state-of-the-art meth-
ods for the task of predicting the future size of cascades:
• DeepCas [13] is an end-to-end neural network framework that given in input
the cascade graph, predicts the future growth of the cascade for a given period.
The parameters are set to the values speciﬁed in the paper: k = 200, T = 10,
mini-batch size is 5 and α = 0.01.
• DeepHawkes [6] is similarly an end-to-end deep learning framework for cas-
cade prediction based on the Hawkes process. We set the parameters to the
default given by the authors: the learning rate for user embeddings is 5×10−4
and the learning rate for other variables is 5 × 10−3.

SubRank: Subgraph Embeddings via a Subgraph Proximity Measure

497
• In addition, we consider the node embedding method VERSE [20], as one of
the top-performing baseline in the previous section. The node embeddings are
learned on the original graph and a cascade is represented as the average of the
embeddings of the nodes it contains. We then train a multi-layer perceptron
(MLP) regressor to predict the growth of the cascade.

Parameter Setting for SubRank. We recall that our subgraph proximity
measure requires the computation of PPR of nodes in the graph and the PR of
nodes in the subgraphs. For this task, we consider the PPR of nodes in the global
graph and the PR of nodes in the cascades. We obtain the cascade embeddings
which are then used to train an MLP regressor. For both VERSE and SubRank
we perform a grid search for the optimal parameters of the regressor.

We report the mean squared error (MSE) on the logarithm of the cascade
growth value, as done in previous work on cascade prediction [6,13] in Table 5.
We observe that SubRank out-performs VERSE thus corroborating our intu-
ition that nodes appearing later in a cascade should be given more importance.
The best MSE overall is obtained by the end-to-end framework DeepHawkes
which is expected as the method is tailored for the task. We note, however, that
SubRank achieves the best results on AMiner.

6 Conclusion

In this work, we introduce a new measure of proximity for subgraphs and a frame-
work for computing subgraph embeddings. In a departure from previous work,
we focus on general-purpose embeddings, and we shed light on why our method
is suited for several data mining tasks. Our experimental evaluation shows that
the subgraph embeddings achieve competitive performance on three downstream
applications: community detection, link prediction, and cascade prediction.

