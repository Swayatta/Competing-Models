Using Question Answering Rewards to Improve Abstractive

Summarization

Chulaka Gunasekara, Guy Feigenblat, Benjamin Sznajder, Ranit Aharonov,

Sachindra Joshi
IBM Research AI

chulaka.gunasekara@ibm.com, {guyf, benjams}@il.ibm.com

{ranit.aharonov2@, jsachind@in.}ibm.com

Abstract

Neural abstractive summarization models have
drastically improved in the recent years. How-
ever, the summaries generated by these models
generally suffer from issues such as: not cap-
turing the critical facts in source documents,
and containing facts that are inconsistent with
the source documents. In this work, we present
a general framework to train abstractive sum-
marization models to alleviate such issues. We
ﬁrst train a sequence-to-sequence model to
summarize documents, and then further train
this model in a Reinforcement Learning set-
ting with question-answering based rewards.
We evaluate the summaries generated by the
this framework using multiple automatic mea-
sures and human judgements. The experimen-
tal results show that the question-answering
rewards can be used as a general framework
to improve neural abstractive summarization.
Particularly, the results from human evalua-
tions show that the summaries generated by
our approach are preferred over 30% of the
time over the summaries generated by general
abstractive summarization models.

Original Document/Dialog
Charlee:
Curtis:
Charlee: Yes. One of my subjects at the university that I attend

I’m in class. Theatre in Portuguese lol.
Realllly?

is portuguese theatre.

Charlee: We are preparing for a performance.
Curtis: What performance is this? Are you devising it?
Charlee: A polish one translated into portuguese.
Curtis:
Charlee: Mro˙zek.

Thats quite cool. Who is the writer?

Ground truth (human) summary

Charlee is attending Portuguese theater as a subject at
university. He and other students are preparing a play
by Mro˙zek translated into Portuguese.

Generated Summary 1: Failing to capture critical facts

Charlee is preparing for a performance in Portuguese.
The writer is Mro˙zek.

Generated Summary 2: Inconsistent facts with the original document

Charlee and Curtis are preparing for a performance
in Portuguese. The performance is a Polish one trans-
lated into Portuguese.

Generated Summary 3: A summary generated with our approach
Charlee is in Portuguese theater class preparing for a
Portuguese translation of a Polish play. The writer is
Mro˙zek.

Figure 1: A document, its corresponding ground truth
summary and model generated summaries.

Introduction

1
Although neural abstractive summarization has
seen drastic improvements over the recent years
(Nallapati et al., 2016; See et al., 2017; Paulus
et al., 2018; Shi et al., 2021), these systems still
have multiple drawbacks. One such common draw-
back is that the generated summaries frequently
fail to capture critical facts in source documents
(low recall) (Scialom et al., 2021). On the other
hand, neural abstractive summarization models are
known to generate content which are inconsistent
with the source document (low precision). This
is commonly known as hallucination (Kryscinski
et al., 2020, 2019). Some studies (Cao et al., 2018)
claim that nearly 30% of the outputs of common
abstractive summarization models suffer from this
problem.

Figure 1 shows a source document, the ground
truth summary and few summaries generated by
neural models. In the Generated Summary 1, the
model fails to capture some of the crucial facts in
the original dialog, such as the play is translated. In
Generated Summary 2, although the model success-
fully identiﬁes the fact that the play is a translated,
it incorrectly mentions that both Charlie and Curtis
are performing. Due to such common factuality
related issues, neural abstractive summarization
models are hardly usable in real-world applications
(Scialom et al., 2021).

In this work, we propose a general framework to
alleviate factuality related issues and improve the
quality of the abstractive summarization by using
question-answering(QA) based rewards. First, we
train a sequence-to-sequence(seq2seq) summary
generation model to take a document as the input

FindingsoftheAssociationforComputationalLinguistics:EMNLP2021,pages518–526November7–11,2021.©2021AssociationforComputationalLinguistics518and generate a summary as the output. Next, we
improve the precision and recall of the summary
generation model using a QA framework as fol-
lows. To improve the precision of the model, we
ﬁrst generate questions and corresponding answers
for each generated summary. Next, we evaluate the
answers that we get for the same questions from
the ground truth summaries. If a generated sum-
mary contains factually incorrect information, this
would lead to having different answers from the
ground truth summary for some of the generated
questions. We use the similarity of answers to cal-
culate a reward to improve the precision. Similarly,
to improve the recall of the summarization model,
we generate questions and corresponding answers
from the ground truth summaries and evaluate the
answers we obtain for the same questions from
the generated summaries. If the generated sum-
mary does not contain some key information as cap-
tured in the ground truth summary, then this would
lead to obtaining different answers from the ground
truth summary for some of the generated questions.
We use the similarity of answers to calculate a
reward to improve the recall. The calculated re-
wards were used in a Reinforcement Learning (RL)
based framework to improve the summary genera-
tion model. In Figure 1 we show an example output
from our approach, which does not contain the fac-
tuality related issues shown above. We evaluate the
summaries generated by our approach using mul-
tiple automatic measures and human judgements,
and show that the QA can be used as a general
framework to improve abstractive summarization.
In summary, our key contributions are: (1) We
introduce a Reinforcement Learning framework,
which uses QA rewards to improve the recall and
precision of abstractive summarization. (2) The
framework is evaluated on three commonly used
transformer based summarization models on two
public datasets. (3) The evaluation of generated
summaries on several automatic measures and hu-
man judgements show the effectiveness of our
method.
In particular, the human judges prefer
summaries generated by our approach more than
30% of the time, over the summaries generated by
general abstractive summarization models.

2 Related Work

There have been previous work on improving the
factual consistency of abstractive summarization
models. Cao et al. (2018) used an approach with

two encoders, one to encode the source document,
and another to encode the facts, and a decoder to at-
tend to the outputs of the two encoders when gener-
ating the summary. Zhu et al. (2020) used OpenIE
to extract facts and used them in the form of knowl-
edge graphs to improve abstractive summarization.
Arumae and Liu (2019) used facts obtained from
question-answering rewards to improve extractive
summarization. Huang et al. (2020) used multi-
choice cloze rewards, in addition to the knowledge
graphs to improve the factual consistency. Li et al.
(2018) incorporated entailment knowledge into ab-
stractive summarization to improve factual correct-
ness.

There have been several work proposed to eval-
uate the factuality of summarization algorithms,
as more common n-gram based metrics, such as
ROUGE (Lin, 2004), are known to perform poorly
for this purpose. Most recent approaches proposed
for evaluating the factuality are based on QA frame-
works (Chen et al., 2018; Eyal et al., 2019; Wang
et al., 2020; Deutsch et al., 2020; Durmus et al.,
2020; Scialom et al., 2021). The evaluation met-
rics proposed by the the above studies measure to
which extent a generated summary provides suf-
ﬁcient information to answer questions posed on
its ground truth summary and whether the ques-
tions generated on the generated summary can be
answered by the ground truth summary.

3

Improving Summarization with QA
Rewards

In general, abstractive summarization models are
trained to minimize the cross entropy loss of the
reference summary at the word-level, which does
not necessarily reward models for being factually
accurate with high precision and recall (Maynez
et al., 2020). Hence, to improve the factual accu-
racy of abstractive summarization, we propose a
general framework which uses QA based rewards
and RL based training. Our proposed framework is
illustrated in Figure 2, and below we describe the
critical components of the framework.

3.1 Summary Generator
Recent work have leveraged pre-trained Trans-
former (Vaswani et al., 2017) models for abstrac-
tive summarization (Lewis et al., 2019; Zhang et al.,
2020). In this work, as the ﬁrst step of summary
generation, we train a transformer based seq2seq
model (S), where the source document is fed as

519Figure 2: The training process for the summarization framework with QA rewards

the input, and the model is trained to generate the
summary token-by-token. The model is trained to
optimize the cross entropy loss. During inference,
we use top-p nucleus sampling (Holtzman et al.,
2019) as the decoding mechanism, with p=0.95.

3.2 Question-Answer Generator
The QA Generator is utilized to generate questions
and answers from the original and generated sum-
maries. We generate questions and corresponding
answers from the original summary and evaluate
the answers obtained for those questions from the
generated summary. Similarly, we generate ques-
tions and corresponding answers from the gener-
ated summary and evaluate the answers obtained
for those questions from the original summary. The
functionality of the QA framework is explained
in Algorithm 1. To generate questions and corre-
sponding answers, we use an answer aware ques-
tion generation model1, which is ﬁne-tuned on t5-
base (Raffel et al., 2020) model. To identify the
answer for a generated question from a summary,
we use a extractive QA model2, which is trained on
the SQuAD task (Rajpurkar et al., 2018).

3.3 Reward Model
We use the similarity between the answers obtained
by generated and ground truth summaries as the re-
ward function. A generated summary is considered
relevant if the questions posed by the ground truth
summary can be answered correctly by the gener-
ated summary, as this shows the critical information
queried by the question is present in the generated
summary. Similarly, a generated summary is con-
sidered factual if a question generated on the gen-
erated summary can be correctly answered by the
ground truth summary, as the questions generated
on a hallucinated summary will not be correctly an-
swered by the original summary. In this study, use

1

2

https://huggingface.co/valhalla/t5-base-qg-hl

https://huggingface.co/distilbert-base-cased-distilled-squad

Algorithm 1: QA Framework for factuality
based reward calculation
Input: Trained Summarization Model (S),

Question-Answer Generation Model (QA),
Answer Generation Model (A), Input
Document (D),
Ground Truth Summary (Gt), Textual
Similarity Function (T)

Output: Reward value(R) for Generated Summary

(Ga)

1 Obtain the Generated Summary Ga = S(D)
2 Generate the questions and the corresponding answers

from Ga, Gt.
(I) QGa , AGa = QA(Ga)
(II) QGt , AGt = QA(Gt)
where, QGa represents the question set generated
for the text Ga and AGa represents the
corresponding answer set.

3 Ask the QGa from the Gt, and obtain the

(cid:48) using A. Similarly,
corresponding answer set AGa
ask QGt from the Ga, and obtain the corresponding
answer set AGt
(I) AGa
(II) AGt

(cid:48) using A.
(cid:48) = A(Gt, QGa )
(cid:48) = A(Ga, QGt )

4 Calculate the reward for Ga by the similarity between
(cid:48) and AGa as well as similarity between AGt
(cid:48)
(cid:48), AGt)]

AGa
and AGt.
R = Average[T (AGa

(cid:48), AGa) + T (AGt

the Normalized Levenshtein distance (Yujian and
Bo, 2007) as the similarity measure 3. An example
for using QA for reward calculation is provided in
Section B of the appendix. The reward 1 is used by
the RL framework (shown in Figure 2) to further
train the summary generation model S.

3.4 Policy training
We use proximal policy optimization (PPO) (Schul-
man et al., 2017) as the optimizer for the policy
training, as it prevents the generator from moving
too far away from the pretrained language model
(Wu et al., 2020). We used a publicly available PPO
implementation4 in this study. This approach of

3We also considered cosine similarity of BERT embed-
dings as a distance measure. However the results were not
signiﬁcantly better than using Normalized Levenshtein dis-
tance.

4

https://github.com/lvwerra/trl

520QA based optimization following general seq2seq
training was used to make this framework appli-
cable across different abstractive summarization
models.

4 Evaluation and Results

We evaluate our QA based summarization frame-
work on three common neural abstractive summa-
rization models: GPT-2 (Radford et al., 2019),
BART (Lewis et al., 2019) and PEGASUS (Zhang
et al., 2020). The experiments are performed on
two different abstractive summarization datasets:
(1) XSUM (Narayan et al., 2018): consists of 227k
news articles covering a wide variety of subjects
along with human written single-sentence sum-
maries, (2) SAMSUM (Gliwa et al., 2019): conver-
sation summarization dataset, containing over 13k
open-domain conversations and summaries created
by humans.

The documents in the XSUM data are fed to the
models unaltered. For SAMSUM data, we ﬁrst pre-
process the conversations by replacing the personal
names (ex: John) with unique tags (ex:<person_0 >),
and then accumulate the utterances in each con-
versation as follows before feeding them to the
models: <person_1>utterance_1 <person_2>utterance_2
<person_1>utterance_3 .... In this implementation, we
generate one QA pair per sentence in a summary.
In addition to that, we also ﬁlter out answers that
are long (over 5 words), as we believe such long
answers do not correspond to the factuality, which
is the focus of this study. The average number of
QA pairs per summary are 2.5 and 1.4 for SAM-
SUM and XSUM datasets respectively. The QA
based reward process is less expensive in this study,
since the number of QA pairs generated are low
compared to the studies that generate QA pairs on
source documents (not on summaries). We evaluate
each model, ﬁrst, with general method of training:
generate the summary given the document, then,
with further RL based training with QA rewards
that we propose. The hyper-parameters used in
training are available in the Section A of the ap-
pendix.

Evaluation with ROUGE scores: We ﬁrst eval-
uate the models using the ROUGE scores. The ob-
tained results are reported in Tables 1 and 2. Each
table contains two sections, where the ﬁrst section
shows the accuracy before training with QA based
rewards, and the second section shows the results
after RL based training with QA rewards. The

results suggest that for both datasets, each model
signiﬁcantly improves (p < 0.05) its summariza-
tion accuracy using our QA framework.

Factuality based evaluation: We evaluate the
results obtained from our models using the fac-
tuality based evaluation framework proposed by
Scialom et al. (2021). This measure provides bet-
ter correlation with human judgments over four
evaluation dimensions (consistency, coherence, ﬂu-
ency, and relevance) (Scialom et al., 2021), and
provides precision, recall and F1 for a generated
summary given a reference. The results obtained
on the two datasets are shown in Table 3. Sim-
ilar to the ROUGE based evaluation, the results
here clearly indicate that the for both datasets, each
model improves its accuracy using our QA frame-
work.

Human Evaluation: We further conducted hu-
man evaluations to study the quality of the models.
We focused on the two models that obtained the
best scores in our automatic evaluations: PEGA-
SUS and BART, and compared the quality of sum-
maries between the original model to our model
optimized with QA rewards. For this assessment
we ﬁrst randomly sampled 30 records from the test
sets of SAMSUM and XSUM (overall 60 records).
Then, we generated 4 types of summaries: PE-
GASUS, PEGASUS-QA, BART, BART-QA. We
followed the evaluation protocol similar to (Wang
et al., 2020), in which, the annotators were pre-
sented with a document, a ground truth summary
and a model summary, and were asked to make
two decisions: (1) which model summary is more
factual consistent with the given document, and
(2) which model summary is of a higher quality,
taking into account Informativeness, Fluency, and
Succinctness. The annotators were instructed to se-
lect one summary or indicate that both summaries
are equally good or bad. To achieve a high quality
standard we recruited 6 NLP experts, and collected
3 human judgments per each summary. To obtain
a single score per summary, we took the majority
vote of the collected assessments. More details
about human evaluation is available at Section C
of the Appendix.

Table 4 describes the results of this assessment.
The values represent the number of times that a
model was selected as strictly better than its coun-
terpart out of 30 annotated summaries. Differences
between QA based reward generation model to
the original model is statistically signiﬁcant (with

521Model
R-1
GPT-2
42.90
BART
52.85
PEGASUS
52.86
GPT-2-QA
44.94
BART-QA
55.50
PEGASUS-QA 55.43

R-2
20.75
32.05
32.36
22.27
33.91
34.81

R-L
33.94
44.06
44.76
35.24
46.20
47.04

R-SU4
19.97
29.58
30.28
21.46
31.75
32.46

Model
R-1
GPT-2
25.30
BART
45.58
PEGASUS
47.33
GPT-2-QA
28.73
BART-QA
46.98
PEGASUS-QA 48.11

R-2
5.61
22.47
24.59
7.41
23.14
25.13

R-L
18.87
37.61
39.43
21.01
38.31
41.06

R-SU4
8.11
22.38
24.16
9.85
23.96
25.28

Table 1: Abstractive summarizers on SAMSUM

Table 2: Abstractive summarizers on XSUM

Model

P
27.88
GPT-2
40.93
BART
46.64
PEGASUS
28.79
GPT-2-QA
BART-QA
43.10
PEGASUS-QA 47.89

SAMSUM

R
24.64
35.98
36.89
28.47
41.56
38.96

F-1
26.26
38.46
41.77
28.63
42.33
42.93

P

11.52
35.40
38.12
14.11
39.30
41.30

XSUM

R

10.17
29.36
32.69
13.55
31.96
34.24

F-1
10.85
32.38
35.40
13.82
35.63
37.77

Table 3: Results of QA based evaluation

Model

Factual Consistency
SAMSUM XSUM
6 (20%)
1 (3%)
BART
15 (50%)
16 (53%)
BART-QA
5 (17%)
PEGASUS
7 (23%)
PEGASUS-QA 16 (53%)
14 (47%)

Quality

SAMSUM XSUM
6 (20%)
4 (13%)
18 (60%)
16 (53%)
5 (17%)
4 (13%)
13 (43%)
14 (47%)

Table 4: Results of human evaluation

p < 0.05). These results indicate that QA based
rewards helps to signiﬁcantly improve summary
generation model, considering both factual consis-
tency and general quality aspects.

Examples: In Figure 3 we show some examples
of model improvements after RL based training
with QA rewards. For each model, we show as
Original, the summary produced by the model be-
fore RL training and, as After RL, the summary
produced by the model after RL training.

5 Conclusion

We investigated the problem of low recall and pre-
cision of factuality in neural abstractive summa-
rization models, and proposed a framework to alle-
viate this issue which uses QA based rewards. The
proposed framework is evaluated on three com-
monly used transformer based summarization mod-
els and on two publicly available datasets. The au-
tomatic evaluations were performed using ROUGE
scores, as well as question answering based eval-
uation framework and the results suggest that the
our method improves the summarization accuracy
and factuality. The human evaluation on the gen-
erated summaries also suggest that our approach
produces summaries with signiﬁcantly high factual
consistency and quality.

Original Document/Dialog
person_0: hey babe, what do you want for dinner tonight?
person_1: gah, don’t even worry about it tonight
person_0: what do you mean? everything ok?
person_1: not really, but it’s ok, don’t worry about cooking

though, I’m not hungry

person_0: Well what time will you be home?
person_1:
person_0: you sure? Maybe you want me to pick you up?
person_1: no no it’s alright. I’ll be home soon, i’ll tell you when

soon, hopefully

I get home.

person_0: Alright, love you.
person_1:

love you too.

Ground truth summary

person_1 will be home soon and she will let person_0
know.

GPT-2 Model
Original

After RL

person_1 wants to grab something for dinner with
person_0. person_0 is not hungry. She will pick up
something for dinner when she gets home.
person_1 is away for the evening. person_0 wants to
pick him up and person_1 will let him know when he
gets home.

BART Model
Original

After RL

person_1 is not hungry tonight. She will be home
soon.
person_1 doesn’t want person_0 to cook anything for
dinner tonight. She will be home soon and will tell
person_0 when she gets home.

Pegasus Model
Original

After RL

person_1 will be home soon. person_0 will pick her
up.
person_1 will tell person_0 when he gets home.

Figure 3: Model improvements after QA based rewards
- SAMSUM data
6 Ethics

study we used the publicly avail-
In this
SAMSUM
(https://huggingface.
able
and XSUM (https:
co/datasets/samsum)
//github.com/EdinburghNLP/XSum)
datasets.
For the human evaluation, in order to meet a high
quality standard, we recruited 6 NLP researchers,
who have graduate degree in NLP and Machine
Learning. Before the ofﬁcial evaluation started, we
sampled 10 tasks to get an estimate of the duration
of the task and to make sure the instructions are
clear enough.

522