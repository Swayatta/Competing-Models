Robust Attribute and Structure

Preserving Graph Embedding

Bhagya Hettige(B), Weiqing Wang, Yuan-Fang Li, and Wray Buntine

{bhagya.hettige,teresa.wang,yuanfang.li,wray.buntine}@monash.edu

Monash University, Melbourne, Australia

Abstract. Graph embedding methods are useful for a wide range of
graph analysis tasks including link prediction and node classiﬁcation.
Most graph embedding methods learn only the topological structure
of graphs. Nevertheless, it has been shown that the incorporation of
node attributes is beneﬁcial in improving the expressive power of node
embeddings. However, real-world graphs are often noisy in terms of
structure and/or attributes (missing and/or erroneous edges/attributes).
Most existing graph embedding methods are susceptible to this noise, as
they do not consider uncertainty during the modelling process. In this
paper, we introduce RASE, a Robust Attribute and Structure preserving
graph Embedding model. RASE is a novel graph representation learn-
ing model which eﬀectively preserves both graph structure and node
attributes through a uniﬁed loss function. To be robust, RASE uses a
denoising attribute auto-encoder to deal with node attribute noise, and
models uncertainty in the embedding space as Gaussians to cope with
graph structure noise. We evaluate the performance of RASE through
an extensive experimental study on various real-world datasets. Results
demonstrate that RASE outperforms state-of-the-art embedding meth-
ods on multiple graph analysis tasks and is robust to both structure and
attribute noise.

Keywords: Robust graph embedding · Node classiﬁcation · Link

prediction

1 Introduction

Much real-world data can be naturally delineated as graphs, e.g. citation net-
works [1,7,16], social-media networks [2,18] and language networks [16]. Graph
embedding methods [6,7,13,16] have been proposed as an eﬀective way of learn-
ing low-dimensional representations for nodes to enable down-stream machine
learning tasks, such as link prediction and node classiﬁcation, on these complex
graph data. Most existing graph embedding methods learn node embeddings
from graph topological structure only [6,13,16,17]. However, nodes in a graph
usually have supplementary attribute information which can be utilized in graph
embedding along with the graph structure to produce more meaningful node
embeddings [7,11,15,21].
c(cid:2) Springer Nature Switzerland AG 2020
H. W. Lauw et al. (Eds.): PAKDD 2020, LNAI 12085, pp. 593–606, 2020.
https://doi.org/10.1007/978-3-030-47436-2_45

594

B. Hettige et al.

Graphs constructed from the real-world data are usually non-deterministic
and ambiguous [14], manifested by uncertain and ambiguous edges and/or
node attributes. For example, most knowledge graphs follow the “Open World
Assumption” [14] (i.e. the unobserved edges are unknown instead of untrue),
so that graph structures are far from complete and many edges are missing.
Also, much attribute information is abstracted from free text (e.g. users’ post
on social media) and is usually imprecise or ambiguous due to the limitations in
data sources or abstraction tools. We term this non-deterministic and ambiguous
phenomenon in graph structure and node attributes as “structure noise” and
“attribute noise” respectively.

A great challenge that the existing graph embedding methods face when
incorporating both graph structure and node attributes, is the noise prevalent
in these two aspects which can mislead the embedding technique to result in
learning invalid latent information. Recently, several studies have been proposed
to model the uncertainty present in graph data [1,8,11,20]. Most of these work,
including VGAE [11], and Graph2Gauss [1], focuses on modelling the uncer-
tainty of the node embeddings by representing the nodes with a probabilistic
distribution in the embedding space. Since these studies attempts to preserve
the observable graph structural proximity by measuring the distance between
probability distribution embeddings, uncertainty modelling of these methods
can only capture structure noise. Therefore, they do not explicitly account for
the node attribute noise which is common in the real-world graphs.

In this work, we introduce RASE, a novel graph embedding framework to
address the aforementioned challenges. RASE learns robust node representa-
tions via carefully-designed strategies, exploiting both graph structure and node
attributes simultaneously. Attribute noise is modelled with a denoising attribute
auto-encoder to maintain the discreteness and sparseness of textual data by
introducing a noise in the input through a binomial distribution. Structure noise
is modelled in the latent layer by modelling the embeddings as Gaussian dis-
tributions. To preserve the transitivity in the embedding space with a linear
computational cost, 2-Wasserstein distance is used as the similarity measure
between the distributions in Gaussian space. Extensive experiments have been
conducted on ﬁve diﬀerent real-world datasets. The experimental results show
that our method signiﬁcantly outperforms state-of-the-art methods in generat-
ing eﬀective embeddings for node classiﬁcation and link prediction. Moreover, we
introduce a novel experimental setting to simulate random structure noise and
random attribute noise to demonstrate the robustness of our model in embedding
noisy graphs.

2 Related Work

There are three lines of eﬀort most related to this work: structure-preserving graph
embedding, attributed graph embedding and noise modelled graph embedding.

Robust Attribute and Structure Preserving Graph Embedding

595

Structure-Preserving Graph Embedding: These embedding methods attempt
to conserve observable graph structure properties in the embedding space.
LINE [16] learns from structural closeness considering ﬁrst- and second-order
proximity. DeepWalk [13] and node2vec [6] learn node embeddings from random
walk sequences with a technique similar to Skip-Gram [12]. DVNE [20] uses an
auto-encoder architecture to encode and reconstruct the graph structure. All
these algorithms focus on graph structure only.
Attributed Graph Embedding: Recent studies [1,7,8,11,15,19,21] show that the
incorporation of node attributes along with graph structure produces better node
embeddings. TADW [19] incorporates text attributes and graph structure with
low-rank matrix factorization. GraphSAGE [7] is a CNN-based technique that
samples and aggregates neighbouring node attributes. Graph2Gauss [1] ﬁnds
the neighbours in each hop up to a pre-deﬁned number of hops which is space
ineﬃcient. Also, it uses node attributes for embedding initialization and does not
explicitly preserve attributes when learning embeddings. VGAE [11] is a graph
convolution network (GCN) method, which aggregates neighbouring attributes.
In most studies, node attributes are only used for embedding initialization, but
not during model training. DANE [4] proposes a deep non-linear architecture to
preserve both aspects.
Noise Modelled Graph Embedding: Most of the existing graph embedding meth-
ods represent nodes as point vectors in the embedding space, ignoring the uncer-
tainty of the embeddings. In contrast, Graph2Gauss [1], VGAE [11], DVNE [20]
and GLACE [8] capture the uncertainty of graph structure by learning node
embeddings as Gaussian distributions. DVNE [20] proposes to measure distribu-
tional distance using the Wasserstein metric as it preserves transitivity. A recent
study [3] learns a discrete probability distribution on the graph edges. However,
these works ignore the modelling of the uncertainty of node attributes.

3 Methodology
Problem Formulation. Let G = (V, E, X) be an attributed graph, where
V is the set of nodes, E is the set of edges in which each ordered pair of nodes
(i, j) ∈ E is associated with a weight wij > 0 for edge from i to j, and X|V|×D
is the node attribute matrix, where xi ∈ X is a D-dimensional attribute vector
of node i. We learn to embed each node i ∈ V as a low-dimensional Gaussian
distribution zi = N (μi, σ2
L×L with the embedding
dimension L (cid:3) |V|, D. The learning goal is such that, nodes that are closer in
the graph and have similar attributes are closer in the embedding space, and
node embeddings are robust to structure noise and attribute noise.

i )1, where μi ∈ R

L, σ2

i ∈ R

1 We learn diagonal covariance vector, σ2
reduce the number of parameters to learn.

i ∈ R

L, instead of a covariance matrix to

596

B. Hettige et al.

3.1 RASE Architecture

Figure 1 shows the architecture of RASE which is an end-to-end embedding
framework that learns from both node attributes and graph structure, with two
main components: Node Attribute Learning and Graph Structure Learning. To
deal with attribute noise, RASE corrupts node attributes by introducing a ran-
dom noise εi sampled from a binomial distribution, which are then projected
to a low-dimensional representation ui. RASE takes this ui as input and simul-
taneously performs node attribute learning and graph structure learning. By
reconstructing the node attributes from ui, the model preserves attributes (with
Euclidean distance to preserve transitivity) while being robust to attribute noise.
RASE models uncertainty of the graph structure noise by learning Gaussian
embeddings and capturing neighbourhood information measured with Wasser-
stein metric to preserve transitivity property in the embedding space.

Node Attribute Learning. We learn node attributes in an unsupervised man-
ner. To deal with noisy attributes, we slightly corrupt the attribute vectors using
a random noise. In most real-world graphs, node attributes can be very sparse,
since they are either tf-idf vectors of textual features or one-hot vectors of cate-
gorical features. A Gaussian noise would substantially change a sparse attribute
vector and would not characterise the trends observed in the real data. Thus, we
draw noise from a binomial distribution as a masking noise but it still depicts the
original data trends. Accordingly, we inject some impurity to the original node
attribute vector xi ∈ R
D by sampling a random binary noise vector εi ∈ {0, 1}D
from a binomial distribution B with D (i.e. attribute vector dimension) trials
and p success probability. We set p ∈ (0.90, 0.98) to ensure that the noise is small
and its introduction does not change the data trends. We produce the corrupted
attribute vector x(cid:3)
sentation ui ∈ R

The corrupted attribute vector is transformed into an intermediate repre-
m where m is a reduced vector dimension using an encoding

i ∈ R

D by performing Hadamard product: x(cid:3)

i = xi ⊗ εi.

Fig. 1. RASE architecture.

Robust Attribute and Structure Preserving Graph Embedding

597

D → R
m. Subsequently, this intermediate vec-
transformation function, g : R
m → R
D, to reconstruct the attribute
tor is fed as input to a decoder, h : R
vector ˆxi ∈ R
D. Note that, these encoder and decoder functions can easily be
implemented with MLP layers or sophisticated GCN layers [11] and to capture
the non-linearity in data we can have deep neural networks. But we observe
that MLP architecture is more simple and eﬃcient, hence scalable on large-scale
graphs. We deﬁne the attribute reconstruction loss as the Euclidean distance
between the original and reconstructed attribute vectors:

La =

(cid:2)

i∈V

(cid:6)xi − ˆxi(cid:6)2

(1)

L1 regularization has been adopted as we have sparse attribute vectors con-
structed from textual data. By minimizing the attribute reconstruction loss we
encourage the encoder g to generate robust latent representations, ui, which are
used as inputs to the Graph Structure Learning component.

Graph Structure Learning. We use the intermediate vector, ui, from the
auto-encoder in the Node Attribute Learning component, as it encodes attribute
latent relationships between nodes. We deﬁne two parallel transformations to
model a node’s embedding as a Gaussian distribution to account for structural
uncertainty (due to noise), i.e. fμ and fσ, that learn the mean vector μi and the
i for inter-
i of ui respectively. To obtain positive σ2
diagonal covariance vector σ2
pretable uncertainty we choose activation function at the output layer accord-
ingly. Thus, the ﬁnal latent representation of node i is zi = N (μi, σ2
i ), where:

μi = fμ(ui) and σ2

i = fΣ(ui)

(2)

wij

To preserve the structural proximity of nodes in the graph, we assume that the
nodes which are connected with a higher edge weight are more likely to be similar
and we attempt to pull the embeddings of these nodes closer in the embedding
space. We deﬁne the prior probability for connected nodes as ˆP (i, j) =
Σ(k,l)∈Ewkl
where wij is the weight of the edge (i, j) ∈ E. Since RASE’s node embeddings
are Gaussians, we choose a probability distance metric to compute the distance
between nodes. Thus, motivated by DVNE [20], to preserve transitivity prop-
erty in the embedding space, we choose the Wasserstein distance: 2-nd moment
(W2). This metric allows to discover speciﬁc relations between nodes based on
their semantic relations and similarities by leveraging the geometric properties
of the embedding space. As a result, when we model the explicit local neigh-
bourhood edges, implicit global neighbourhood proximity can be modelled due
to triangle inequality property. We deﬁne δ(zi, zj) as the W2 distance for our
node embeddings, i and j. Modelling only the diagonal covariance vectors results
in σ2

i . Hence, W2 computation [5] simpliﬁes to:

j σ2

i σ2

j = σ2

δ(zi, zj) = W2(zi, zj) = ((cid:6) μi − μj (cid:6)2

2 + (cid:6) σi − σj (cid:6)2

F )1/2

(3)

598

B. Hettige et al.

The likelihood of an edge between nodes i and j is deﬁned as the similarity of
the two node embeddings [16]:

P (i, j) = Sigmoid(−δ(zi, zj))) =

1

1 + exp (δ(zi, zj))

(4)

We minimize the distance between the prior and the observed probability dis-
tributions for the edges to preserve the node proximity in the embedding space.
Since ˆP and P are discrete probability distributions, we deﬁne structural loss
using KL divergence:

Ls = DKL( ˆP||P ) =

(cid:2)

(i,j)∈E

ˆP (i, j) log

(cid:4)

(cid:3) ˆP (i, j)
P (i, j)

∝ − (cid:2)
(i,j)∈E

wij log P (i, j)

(5)

For regularization of Ls, instead of regularizing mean and covariance functions
separately, RASE uses the strategy similar to [10] minimizing KL divergence
between the learned Gaussian representation and the standard normal distribu-
tion. Thus, it will ensure that the ﬁnal latent space will be closer to a standard
Gaussian space other than pushing values in both mean vectors and variance
vectors to be small. Diﬀerent from RASE, Graph2Gauss [1] does not regularize
the Gaussian functions. The regularization for node i is:

DKL(zi||N (0, 1)) =

(cid:3)

1
2

σ2

i + μ2

i − ln(σi

(cid:4)
2) − 1

(6)

By minimizing the overall structural loss function, we attempt to construct an
embedding space where nodes that are similar in terms of graph structure are
also similar in the embedding space and robust to noisy graph structure.

Uniﬁed Training and Optimization. To jointly preserve node attributes and
graph structure, we deﬁne a uniﬁed loss function by combining Eq. 1 and Eq. 5
with hyperparameter α > 0. For simplicity, we omit the regularization terms in
the two components of RASE in the overall loss function to be minimized:

L = αLa + Ls = α

(cid:2)

i∈V

(cid:6)xi − ˆxi(cid:6)2 − (cid:2)
(i,j)∈E

wij log P (i, j)

(7)

For large graphs, this uniﬁed loss function is computationally expensive, since
it has to compute the attribute reconstruction loss (La) for all the nodes and
the structural loss (Ls) for all the edges. To optimize La, we sample only a
batch of nodes in each epoch. To optimize Ls, we employ the negative sampling
approach [12] and sample K negative edges for each edge in the training batch.

Robust Attribute and Structure Preserving Graph Embedding

599

Table 1. Statistics of the real-world graphs.

|V |

|E|

Dataset
Social media networks

D

#Labels

BlogCatalog 5,196
7,535
Flickr
Citation networks

369,435 8,189
6
239,738 12,047 9

Cora
Citeseer
Pubmed

8,416
5,358

2,995
4,230
18,230 79,612

2,879
602
500

7
6
3

Therefore, for each edge (i, j) in the batch, b ⊂ E, with the noise distribution,
Pn(v) for v ∈ V, we can compute the structural loss as:

log σ(−δ(zi, zj)) +

K(cid:2)

n=1

Evn∼Pn(v) log σ(δ(zi, zvn))

(8)

4 Experiments

We evaluate RASE against state-of-the-art baselines in several graph analy-
sis tasks, node classiﬁcation, link prediction and robustness on several public
datasets. Source code for RASE is publicly available at https://github.com/
bhagya-hettige/RASE.

4.1 Datasets

Social Media Networks [9] (Table 1): Nodes on these networks are users.
The following relationships are used to construct the edges. Attributes on Blog-
Catalog and Flickr are constructed with keywords in users’ blog description and
users’ predeﬁned tags of interests, respectively. Node labels are users’ interest
topics on BlogCatalog and groups users joined in Flickr. Citation Networks [1]
(Table 1): Nodes denote papers and edges represent citation relations. We use
tf-idf word vectors of the paper’s abstract as node attributes. Each paper is
assigned a label based on the topic of the paper.

4.2 Compared Algorithms

state-of-the-art graph embedding meth-
We compare RASE to several
ods: structure-based non-attributed embedding methods (node2vec, LINE
and DVNE); attributed embedding methods
(GraphSAGE, VGAE and
Graph2Gauss); and uncertainty modelling embedding methods (DVNE, VGAE
and Graph2Gauss).

600

B. Hettige et al.

node2vec [6] is a random walk based node embedding method that maxi-
mizes the likelihood of preserving nodes’ neighbourhood. LINE [16] preserves
ﬁrst- and second-order proximity. We report results on a concatenated represen-
tation of the two proximities (as suggested). DVNE [20] learns Gaussian dis-
tributions in the Wasserstein space from plain graphs. GraphSAGE [7] is an
attributed graph embedding method which learns by sampling and aggregating
features of local neighbourhoods. We use its unsupervised version, since all other
methods are unsupervised. VGAE [11] is an attributed GCN-based embedding
method which implements an auto-encoder model with Gaussian node embed-
dings. Graph2Gauss (G2G) [1] is an attributed embedding method which
represents each node as a Gaussian and preserves the graph structure based
on a ranking scheme of multiple neighbouring hops. In addition, we evaluate
task performance on node attributes as input features instead of learning node
embeddings, for down-stream machine learning tasks.

RASE is our full model which jointly preserves node attribute and graph
structure, and is robust to noise in real-world graphs. We also consider a non-
robust version, RASE(¬R), for an ablation study. RASE(¬R) does not model
attribute noise and learns point vectors, thus also ignoring structural uncertainty.

4.3 Experimental Settings

For all the models that learn point vectors, we set L = 128 as the embedding
dimension. For a fair evaluation, we set L = 64 in methods learning probability
distributions, including ours, so that the parameters learned per node is still 128
(μi ∈ R64 and σ2
i ∈ R64). The other parameters for baselines are referred from
the papers and tuned to be optimal. α is tuned to be optimal using grid search
on a validation set. We report the results averaged over 10 trials.

4.4 Node Classiﬁcation

In this task, each method learns the embeddings in an unsupervised manner, and
a logistic regression (LR) classiﬁer is trained on these embeddings to classify each
node into their associated class label. We randomly sample diﬀerent percentages
of labeled nodes (i.e. 1%, 2%, . . . , 10%) from the graph as training set for the
classiﬁer, and use the rest for evaluation. We report micro- and macro-F1 scores
which have been widely used in multi-class classiﬁcation evaluation [16]. We only
present micro-F1 in Fig. 2, and a similar trend is observed in macro-F1.

Based on the results in Fig. 2, we can see that RASE consistently outperforms
all the baselines in all the datasets with all the training ratios. Furthermore,
in all ﬁve datasets, RASE has demonstrated a larger improvement margin to
the baselines when only smaller numbers of nodes are used for training, e.g., a
174.9% improvement over best performing baseline in Flickr at 1% labeled nodes.
This performance improvement is due to the attribute preserving component,
which learns meaningful latent representations from node attributes. Moreover,
denoising the attributes in this process also helps our model to deal with scarce
data which is common in the real-world graphs. Also, our proposed structure

Robust Attribute and Structure Preserving Graph Embedding

601

(a) BlogCatalog

(b) Flickr

(c) Cora

(d) Citeseer

(e) Pubmed

Fig. 2. Node classiﬁcation performance measured by micro-F1 score (y-axis) in terms of
percentage of labelled nodes (x-axis). RASE’s improvements are statistically signiﬁcant
for p < 0.01 by a paired t-test.

learning method has captured useful local and global node similarities (due to
the transitivity-preserving property in W2 metric).
Overall, RASE(¬R) manifests superiority among the non-probabilistic meth-
ods (i.e. node2vec, LINE and GraphSAGE) consistently outperforming them in
all datasets. Interestingly, on BlogCatalog, Flickr and Cora, RASE(¬R) also sub-
stantially outperforms the probabilistic models DVNE, VGAE and G2G. This
emphasizes the eﬀectiveness of our attribute preservation and structure learning
method, even in the absence of uncertainty modelling.

4.5 Link Prediction

This task aims to predict future links using the graph structure and attributes.
We randomly select 20% edges and an equal number of non-edges, and combine
the two as the test set. The remaining 80% are used for training. Then, the
node embeddings are used to compute the similarity between each test node
pair, which is regarded as the likelihood of a link’s existence between them.
In Gaussian embedding methods, we use negative Wasserstein distance (RASE,
DVNE) and negative KL divergence (G2G) to rank the node pairs [1,20]. For
other methods, we use dot product similarity of node embeddings. We measure
AUC and AP scores [1,11]. For brevity reasons, we only present citation networks
in Table 2, and the trend is similar in the social media networks.

602

B. Hettige et al.

RASE clearly outperforms the state-of-the-art methods by a signiﬁcant mar-
gin in all the graphs, demonstrating the eﬀectiveness of our model in cap-
turing structural and attribute information. RASE outperforms RASE(¬R),
showing that accounting for structure and attribute noise collectively is ben-
eﬁcial. This is also validated by the performance gain of the uncertainty mod-
elling methods, VGAE and G2G, over the non-robust RASE(¬R). Moreover,
the methods that learn from graph structure only (i.e. node2vec, LINE and
DVNE) are signiﬁcantly outperformed by the attributed embedding methods
(i.e. RASE, RASE(¬R), GraphSAGE, VGAE and G2G). RASE(¬R) is the best
performing model among the non-probabilistic methods (i.e. node2vec, LINE
and GraphSAGE), demonstrating that it has learnt meaningful structural simi-
larities between nodes along with node attributes.

Table 2. Link prediction performance.

Algorithm

Cora
AUC AP

Citeseer
AUC AP

Pubmed
AUC AP

node2vec
LINE
DVNE

79.11
79.12
65.73

Attributes
88.06
GraphSAGE 81.76
93.53
VGAE
95.92
G2G
RASE(¬R)
RASE

77.99
78.91
70.33

83.66
83.19
95.33
95.82

79.91
71.20
68.16

81.53
83.33
95.46
96.28

82.08
72.11
73.42

75.60
85.38
96.47
96.54

91.18
75.32
50.66

82.98
89.43
96.11
95.75

91.49
76.81
50.78

77.71
90.90
96.09
95.65

96.18

95.60

95.42
93.84
96.88 96.82 97.82 97.69 96.40 96.21

96.25

94.54

4.6 Robustness

We evaluate RASE and state-of-the-art baselines to see how they can deal with
noise in graphs. In this section, we introduce a novel evaluation task to assess
the robustness of graph embeddings to random structure and attribute noise. We
inject some random noise into the graphs by intentionally corrupting the graph
structure and node attributes. This experiment is conducted on all datasets. We
report the results on Citeseer, since all the datasets demonstrate similar trends.
Structural Noise: We corrupt the graph structure by hiding randomly selected
edges 50% (to mimic missing edges which we also use as the test set) and ran-
domly adding some non-existing edges (edges not in the original graph to mimic
erroneous edges). We vary the percentage of noisy edges added to the graph
from 0%–50%, and observe AUC and AP decline with the increasing noise in
link prediction task. The results are presented in Table 3.

From Table 3, we see that RASE performs the best in all the structural noise
percentages, showing that it is robust to noisy graph structures. In addition to

Robust Attribute and Structure Preserving Graph Embedding

603

this, with the increase in noise ratio from 0% to 50%, RASE’s AUC degrada-
tion is only 3.8%. Also, RASE outperforms its non-robust version, RASE(¬R),
which shows that the proposed uncertainty modelling technique to mitigate
structure noise is eﬀective. In contrast, though DVNE, VGAE and G2G also
model uncertainty in the embeddings, their performance degradation is quite
signiﬁcant (7.4%, 6.5% and 14.3% in AUC respectively) when the noise ratio is
increased from 0% to 50%. VGAE is based on GCN, which aggregates the neigh-
bouring attributes into a convex embedding. Thus, it is heavily aﬀected by noisy
neighbours, as errors get further exaggerated. The hop-based structural ranking
in G2G is sensitive to false neighbourhoods. Furthermore, the square-exponential
loss function used for pair-wise ranking in both G2G and DVNE does not have
a ﬁxed margin and pushes the distance of the negative edges to inﬁnity with
an exponentially decreasing force [1]. Hence, these methods are highly sensitive
to erroneous and missing edges. In contrast, RASE is mildly aﬀected due to
its carefully designed structural loss function and the extra information learned
from the neighbourhood via the transitivity property of W2 metric.

Table 3. Link prediction performance in Citeseer with structural noise.

Noisy edge % 0%

10%

20%

30%

40%

50%

AUC AP AUC AP AUC AP AUC AP AUC AP AUC AP

node2vec

73.0 76.8 66.7 70.6 62.8 66.3 57.6 61.3 56.7 60.2 56.2 59.1

LINE

DVNE

53.1 50.0 52.2 49.2 52.2 48.8 52.6 49.5 51.1 48.4 51.2 48.6

57.9 59.4 56.0 57.1 52.9 55.2 55.5 56.6 53.2 55.4 53.6 55.4

GraphSAGE 75.5 78.2 73.9 76.5 73.1 75.4 73.0 74.9 71.8 73.4 71.3 72.1

VGAE

90.2 92.5 88.6 91.1 86.9 89.8 85.8 88.9 86.0 88.7 84.3 87.5

G2G
RASE(¬R)
RASE

91.3 91.9 84.8 87.2 79.7 83.2 77.6 81.8 76.4 80.2 78.2 81.4

90.1 91.2 90.2 90.6 89.7 89.3 88.8 89.4 87.0 87.7 86.1 86.5
96.0 95.9 94.9 94.9 94.3 94.5 93.5 93.5 92.6 93.2 92.2 92.5

Attribute Noise: To evaluate the robustness of the methods to random
attribute noise, we corrupt the node attribute vectors randomly. Then, we assess
node classiﬁcation performance of the learned embeddings on these corrupted
graphs. Speciﬁcally, we sample a masking noise from a binomial distribution
with D (i.e. attribute dimension) trials and p = 0.70 probability, and perform
Hadamard product with attribute vectors of some randomly selected nodes.
Thus, approximately 30% of the attributes for each selected node are corrupted.
We also vary the percentage of nodes corrupted from 0%–50% to investigate the
micro- and macro-F1 decline. Since we are interested in evaluating the attribute
robustness of the embedding methods, we experiment with attributed embedding
methods only. The results are reported in Table 4.

Table 4 shows that RASE is robust to random node attribute noise, having
the highest macro- and micro-F1 scores steadily across all noisy node percent-
ages. Moreover, RASE only shows a 3.3% degradation in micro-F1, when we

604

B. Hettige et al.

increase the proportion of corrupted nodes from 0% to 50%. The small degra-
dation can be attributed to the node attribute denoising step. RASE also out-
performs its non-robust counterpart, showing the eﬀectiveness of attribute noise
modelling component. GraphSAGE shows a poorer node classiﬁcation perfor-
mance when compared to others, which shows that noisy attributes has misled
the model to learn inexact node embeddings. Negative eﬀect of noisy attributes
of neighbouring nodes in GCN’s aggregation step causes the lower performance
of VGAE when its performance is compared against RASE and G2G. Over-
all, G2G shows a modest micro-F1 decline (i.e. 4.9% from 0% to 50%), since
the Gaussian node embeddings have captured the attribute uncertainty via the
variance terms.

4.7 Visualization

We visualize the node embeddings produced by RASE on Cora and Blogcat. We
train RASE with L = 128 and μ vectors are projected to two dimensions using
t-SNE (Fig. 3). RASE produces an adequate visualization with tightly clustered
nodes of the same class label with clearly visible boundaries.

Table 4. Node classiﬁcation performance in Citeseer with 30% attribute noise.

Corrupted node % micro (mi)- and macro(ma)- F1 score

0%

10%

20%

30%

40%

50%

mi ma mi ma mi ma mi ma mi ma mi ma

GraphSAGE

42.9 13.2 42.3 14.8 41.8 13.3 41.7 16.2 41.6 16.2 41.3 16.2

VGAE

G2G
RASE(¬R)
RASE

77.9 77.0 77.0 77.0 76.1 76.1 75.2 75.3 74.0 74.2 73.8 73.7

84.1 84.2 82.3 82.4 81.5 81.5 79.8 79.9 79.0 78.9 79.9 79.9

82.0 82.2 81.2 81.4 80.2 80.5 79.9 79.9 78.1 78.2 77.7 77.6
85.8 85.8 85.0 85.0 83.8 83.8 83.5 83.4 82.5 82.4 83.0 82.9

(a) Cora

(b) Blogcat

Fig. 3. Visualization of RASE embeddings. Colour of a node denotes its class label.
(Color ﬁgure online)

Robust Attribute and Structure Preserving Graph Embedding

605

4.8 Parameter Sensitivity Analysis

We study the sensitivity of attribute reconstruction learning weight (α) and
embedding dimension (L) in RASE. Figure 4 shows micro-F1 score for node
classiﬁcation task on Citeseer averaged over 10 trials. In general, α > 0 shows
better performance than α = 0, demonstrating the positive eﬀect of learning from
node attributes. The impact of attribute preservation is optimal near α = 10 in
RASE and α = 40 in RASE(¬R). Also, RASE performs increasingly better when
the embedding dimension L is increased, since larger dimensions can encode more
meaningful latent information. When L ≥ 32, RASE and RASE(¬R) are already
complex enough to handle the data and further increments are less helpful.

Fig. 4. Parameter sensitivity analysis. Micro-F1 in node classiﬁcation on Citeseer.

5 Conclusion

In this work, we present RASE, an end-to-end embedding framework for
attributed graphs. RASE learns robust node embeddings by preserving both
graph structure and node attributes considering random structure and attribute
noise. RASE has been evaluated w.r.t. several state-of-the-art methods in diﬀer-
ent graph analysis tasks, and the results demonstrate that RASE signiﬁcantly
outperforms all the evaluated baselines.

Acknowledgements. This work has been supported by the Monash Institute of Med-
ical Engineering (MIME), Australia.

