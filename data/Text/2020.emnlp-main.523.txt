LUKE: Deep Contextualized Entity Representations with

Entity-aware Self-attention

Ikuya Yamada1,2
ikuya@ousia.jp

Akari Asai3

akari@cs.washington.edu

Hiroyuki Shindo4,2
shindo@is.naist.jp

Hideaki Takeda5
takeda@nii.ac.jp

Yuji Matsumoto2
matsu@is.naist.jp

1Studio Ousia 2RIKEN AIP 3University of Washington

4Nara Institute of Science and Technology 5National Institute of Informatics

Abstract

The proposed model

Entity representations are useful in natural lan-
guage tasks involving entities.
In this paper,
we propose new pretrained contextualized rep-
resentations of words and entities based on
the bidirectional transformer (Vaswani et al.,
2017).
treats words
and entities in a given text as independent to-
kens, and outputs contextualized representa-
tions of them. Our model is trained using a
new pretraining task based on the masked lan-
guage model of BERT (Devlin et al., 2019).
The task involves predicting randomly masked
words and entities in a large entity-annotated
corpus retrieved from Wikipedia. We also
propose an entity-aware self-attention mecha-
nism that is an extension of the self-attention
mechanism of the transformer, and consid-
ers the types of tokens (words or entities)
when computing attention scores. The pro-
posed model achieves impressive empirical
performance on a wide range of entity-related
tasks. In particular, it obtains state-of-the-art
results on ﬁve well-known datasets: Open En-
tity (entity typing), TACRED (relation classi-
ﬁcation), CoNLL-2003 (named entity recog-
nition), ReCoRD (cloze-style question an-
swering), and SQuAD 1.1 (extractive ques-
tion answering). Our source code and pre-
trained representations are available at https:
//github.com/studio-ousia/luke.
Introduction

1
Many natural language tasks involve entities, e.g.,
relation classiﬁcation, entity typing, named entity
recognition (NER), and question answering (QA).
Key to solving such entity-related tasks is a model
to learn the effective representations of entities.
Conventional entity representations assign each en-
tity a ﬁxed embedding vector that stores informa-
tion regarding the entity in a knowledge base (KB)
(Bordes et al., 2013; Trouillon et al., 2016; Yamada
et al., 2016, 2017). Although these models capture

the rich information in the KB, they require entity
linking to represent entities in a text, and cannot
represent entities that do not exist in the KB.

By contrast, contextualized word representa-
tions (CWRs) based on the transformer (Vaswani
et al., 2017), such as BERT (Devlin et al., 2019),
and RoBERTa (Liu et al., 2020), provide effec-
tive general-purpose word representations trained
with unsupervised pretraining tasks based on lan-
guage modeling. Many recent studies have solved
entity-related tasks using the contextualized rep-
resentations of entities computed based on CWRs
(Zhang et al., 2019; Peters et al., 2019; Joshi et al.,
2020). However, the architecture of CWRs is not
well suited to representing entities for the follow-
ing two reasons: (1) Because CWRs do not out-
put the span-level representations of entities, they
typically need to learn how to compute such rep-
resentations based on a downstream dataset that is
typically small. (2) Many entity-related tasks, e.g.,
relation classiﬁcation and QA, involve reasoning
about the relationships between entities. Although
the transformer can capture the complex relation-
ships between words by relating them to each other
multiple times using the self-attention mechanism
(Clark et al., 2019; Reif et al., 2019), it is difﬁcult
to perform such reasoning between entities because
many entities are split into multiple tokens in the
model. Furthermore, the word-based pretraining
task of CWRs is not suitable for learning the repre-
sentations of entities because predicting a masked
word given other words in the entity, e.g., predict-
ing “Rings” given “The Lord of the [MASK]”, is
clearly easier than predicting the entire entity.

In this paper, we propose new pretrained contex-
tualized representations of words and entities by
developing LUKE (Language Understanding with
Knowledge-based Embeddings). LUKE is based
on a transformer (Vaswani et al., 2017) trained
using a large amount of entity-annotated corpus

Proceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages6442–6454,November16–20,2020.c(cid:13)2020AssociationforComputationalLinguistics6442Figure 1: Architecture of LUKE using the input sentence “Beyonc´e lives in Los Angeles.” LUKE outputs contextu-
alized representation for each word and entity in the text. The model is trained to predict randomly masked words
(e.g., lives and Angeles in the ﬁgure) and entities (e.g., Los Angeles in the ﬁgure). Downstream tasks are solved
using its output representations with linear classiﬁers.

obtained from Wikipedia. An important difference
between LUKE and existing CWRs is that it treats
not only words, but also entities as independent
tokens, and computes intermediate and output rep-
resentations for all tokens using the transformer
(see Figure 1). Since entities are treated as to-
kens, LUKE can directly model the relationships
between entities.

LUKE is trained using a new pretraining task, a
straightforward extension of BERT’s masked lan-
guage model (MLM) (Devlin et al., 2019). The task
involves randomly masking entities by replacing
them with [MASK] entities, and trains the model
by predicting the originals of these masked enti-
ties. We use RoBERTa as base pre-trained model,
and conduct pretraining of the model by simulta-
neously optimizing the objectives of the MLM and
our proposed task. When applied to downstream
tasks, the resulting model can compute representa-
tions of arbitrary entities in the text using [MASK]
entities as inputs. Furthermore, if entity annotation
is available in the task, the model can compute en-
tity representations based on the rich entity-centric
information encoded in the corresponding entity
embeddings.

Another key contribution of this paper is that
it extends the transformer using our entity-aware
self-attention mechanism. Unlike existing CWRs,
our model needs to deal with two types of tokens,
i.e., words and entities. Therefore, we assume that
it is beneﬁcial to enable the mechanism to easily
determine the types of tokens. To this end, we
enhance the self-attention mechanism by adopting
different query mechanisms based on the attending
token and the token attended to.

We validate the effectiveness of our proposed
model by conducting extensive experiments on ﬁve
standard entity-related tasks: entity typing, relation
classiﬁcation, NER, cloze-style QA, and extractive
QA. Our model outperforms all baseline models, in-
cluding RoBERTa, in all experiments, and obtains
state-of-the-art results on ﬁve tasks: entity typing
on the Open Entity dataset (Choi et al., 2018), rela-
tion classiﬁcation on the TACRED dataset (Zhang
et al., 2017), NER on the CoNLL-2003 dataset
(Tjong Kim Sang and De Meulder, 2003), cloze-
style QA on the ReCoRD dataset (Zhang et al.,
2018a), and extractive QA on the SQuAD 1.1
dataset (Rajpurkar et al., 2016). We publicize
our source code and pretrained representations at
https://github.com/studio-ousia/luke.

The main contributions of this paper are summa-

rized as follows:
• We propose LUKE, a new contextualized repre-
sentations speciﬁcally designed to address entity-
related tasks. LUKE is trained to predict ran-
domly masked words and entities using a large
amount of entity-annotated corpus obtained from
Wikipedia.

• We introduce an entity-aware self-attention
mechanism, an effective extension of the original
mechanism of transformer. The proposed mech-
anism considers the type of the tokens (words or
entities) when computing attention scores.

• LUKE achieves strong empirical performance
and obtains state-of-the-art results on ﬁve pop-
ular datasets: Open Entity, TACRED, CoNLL-
2003, ReCoRD, and SQuAD 1.1.

6443Question AnsweringNamed Entity RecognitionRelation ClassiﬁcationC2ABeyoncéA[CLS]C1A[MASK]Token Emb.Position Emb.Entity Type Emb.C3AinC4C5ALosC6A[MASK]BUBeyoncéBU[MASK]A[SEP]C7D2TransformereeC2ABeyoncéA[CLS]C1BU[MASK]A[SEP]C7Transformer with Entity-aware Self-attentionePredict livesPredict Los_AngelesPretrainingEntity TypingPredict LOCATIONBeyoncé[CLS][MASK]inLos[MASK]Beyoncé[MASK][SEP]Beyoncé[CLS][SEP]............[MASK]hw1hw2hw3hw4hw5hw6hw7he1he2he1hw7hw2hw1WordsEntitiesWordsEntitiesD5 + D62D5 + D62Predict Angeles2 Related Work

Static Entity Representations Conventional en-
tity representations assign a ﬁxed embedding to
each entity in the KB. They include knowledge
embeddings trained on knowledge graphs (Bor-
des et al., 2013; Yang et al., 2015; Trouillon et al.,
2016), and embeddings trained using textual con-
texts or descriptions of entities retrieved from a KB
(Yamada et al., 2016, 2017; Cao et al., 2017; Ganea
and Hofmann, 2017). Similar to our pretraining
task, NTEE (Yamada et al., 2017) and RELIC
(Ling et al., 2020) use an approach that trains en-
tity embeddings by predicting entities given their
textual contexts obtained from a KB. The main
drawbacks of this line of work, when representing
entities in text, are that (1) they need to resolve
entities in the text to corresponding KB entries to
represent the entities, and (2) they cannot represent
entities that do not exist in the KB.

Contextualized Word Representations Many
recent studies have addressed entity-related tasks
based on the contextualized representations of en-
tities in text computed using the word representa-
tions of CWRs (Zhang et al., 2019; Baldini Soares
et al., 2019; Peters et al., 2019; Joshi et al., 2020;
Wang et al., 2019b, 2020). Representative ex-
amples of CWRs are ELMo (Peters et al., 2018)
and BERT (Devlin et al., 2019), which are based
on deep bidirectional long short-term memory
(LSTM) and the transformer (Vaswani et al., 2017),
respectively. BERT is trained using an MLM, a
pretraining task that masks random words in the
text and trains the model to predict the masked
words. Most recent CWRs, such as RoBERTa
(Liu et al., 2020), XLNet (Yang et al., 2019), Span-
BERT (Joshi et al., 2020), ALBERT (Lan et al.,
2020), BART (Lewis et al., 2020), and T5 (Raffel
et al., 2020), are based on transformer trained using
a task equivalent to or similar to the MLM. Similar
to our proposed pretraining task that masks entities
instead of words, several recent CWRs, e.g., Span-
BERT, ALBERT, BART, and T5, have extended the
MLM by randomly masking word spans instead of
single words.

Furthermore, various recent studies have ex-
plored methods to enhance CWRs by injecting
them with knowledge from external sources, such
as KBs. ERNIE (Zhang et al., 2019) and Know-
BERT (Peters et al., 2019) use a similar idea to
enhance CWRs using static entity embeddings sep-

arately learned from a KB. WKLM (Xiong et al.,
2020) trains the model to detect whether an en-
tity name in text is replaced by another entity
name of the same type. KEPLER (Wang et al.,
2019b) conducts pretraining based on the MLM
and a knowledge-embedding objective (Bordes
et al., 2013). K-Adapter (Wang et al., 2020) was
proposed concurrently with our work, and extends
CWRs using neural adapters that inject factual and
linguistic knowledge. This line of work is related
to ours because our pretraining task also enhances
the model using information in the KB.

Unlike the CWRs mentioned above, LUKE
uses an improved transformer architecture with
an entity-aware self-attention mechanism that is
designed to effectively solve entity-related tasks.
LUKE also outputs entity representations by learn-
ing how to compute them during pretraining. It
achieves superior empirical results to existing
CWRs and knowledge-enhanced CWRs in all of
our experiments.

3 LUKE

Figure 1 shows the architecture of LUKE. The
model adopts a multi-layer bidirectional trans-
former (Vaswani et al., 2017).
It treats words
and entities in the document as input tokens, and
computes a representation for each token. For-
mally, given a sequence consisting of m words
w1, w2, ..., wm and n entities e1, e2, ..., en, our
model computes D-dimensional word representa-
tions hw1, hw2, ..., hwm, where hw ∈ RD, and en-
tity representations he1, he2, ..., hen, where he ∈
RD. The entities can be Wikipedia entities (e.g.,
Beyonc´e in Figure 1) or special entities (e.g.,
[MASK]).

Input Representation

3.1
The input representation of a token (word or entity)
is computed using the following three embeddings:
• Token embedding represents the corresponding
token. We denote the word token embedding
by A ∈ RVw×D, where Vw is the number of
words in our vocabulary. For computational ef-
ﬁciency, we represent the entity token embed-
ding by decomposing it into two small matrices,
B ∈ RVe×H and U ∈ RH×D, where Ve is the
number of entities in our vocabulary. Hence, the
full matrix of the entity token embedding can be
computed as BU.

6444• Position embedding represents the position of
the token in a word sequence. A word and an en-
tity appearing at the i-th position in the sequence
are represented as Ci ∈ RD and Di ∈ RD, re-
spectively. If an entity name contains multiple
words, its position embedding is computed by
averaging the embeddings of the corresponding
positions, as shown in Figure 1.

• Entity type embedding represents that the to-
ken is an entity. The embedding is a single vector
denoted by e ∈ RD.
The input representation of a word and that of
an entity are computed by summing the token and
position embeddings, and the token, position, and
entity type embeddings, respectively. Following
past work (Devlin et al., 2019; Liu et al., 2020), we
insert special tokens [CLS] and [SEP] into the
word sequence as the ﬁrst and last words, respec-
tively.

3.2 Entity-aware Self-attention
The self-attention mechanism is the foundation of
the transformer (Vaswani et al., 2017), and relates
tokens each other based on the attention score be-
tween each pair of tokens. Given a sequence of
input vectors x1, x2, ..., xk, where xi ∈ RD, each
of the output vectors y1, y2, ..., yk, where yi ∈ RL,
is computed based on the weighted sum of the trans-
formed input vectors. Here, each input and output
vector corresponds to a token (a word or an entity)
in our model; therefore, k = m + n. The i-th
output vector yi is computed as:

k(cid:88)

yi =

eij =

αijVxj

j=1

Kx(cid:62)
j Qxi√
L

αij = softmax(eij)

where Q ∈ RL×D, K ∈ RL×D, and V ∈ RL×D
denote the query, key, and value matrices, respec-
tively.

Because LUKE handles two types of tokens (i.e.,
words and entities), we assume that it is beneﬁcial
to use the information of target token types when
computing the attention scores (eij). With this in
mind, we enhance the mechanism by introducing
an entity-aware query mechanism that uses a dif-
ferent query matrix for each possible pair of token
types of xi and xj. Formally, the attention score

eij is computed as follows:



Kx(cid:62)
Kx(cid:62)
Kx(cid:62)
Kx(cid:62)

eij =

j Qxi,
j Qw2exi,
j Qe2wxi,
j Qe2exi,

if both xi and xj are words
if xi is word and xj is entity
if xi is entity and xj is word
if both xi and xj are entities
where Qw2e, Qe2w, Qe2e ∈ RL×D are query ma-
trices. Note that the computational costs of the
original mechanism and our proposed mechanism
are identical except the additional cost of comput-
ing gradients and updating the parameters of the
additional query matrices at the training time.

3.3 Pretraining Task
To pretrain LUKE, we use the conventional MLM
and a new pretraining task that is an extension of
the MLM to learn entity representations. In partic-
ular, we treat hyperlinks in Wikipedia as entity an-
notations, and train the model using a large entity-
annotated corpus retrieved from Wikipedia. We
randomly mask a certain percentage of the entities
by replacing them with special [MASK] entities1
and then train the model to predict the masked enti-
ties. Formally, the original entity corresponding to
a masked entity is predicted by applying the soft-
max function over all entities in our vocabulary:

m = layer norm(cid:0)gelu(Whhe + bh)(cid:1)

ˆy = softmax(BTm + bo)

where he is the representation corresponding to
the masked entity, T ∈ RH×D and Wh ∈ RD×D
are weight matrices, bo ∈ RVe and bh ∈ RD are
bias vectors, gelu(·) is the gelu activation function
(Hendrycks and Gimpel, 2016), and layer norm(·)
is the layer normalization function (Lei Ba et al.,
2016). Our ﬁnal loss function is the sum of
MLM loss and cross-entropy loss on predicting
the masked entities, where the latter is computed
identically to the former.

3.4 Modeling Details
Our model conﬁguration follows RoBERTaLARGE
(Liu et al., 2020), pretrained CWRs based on a bidi-
rectional transformer and a variant of BERT (De-
vlin et al., 2019). In particular, our model is based
on the bidirectional transformer with D = 1024

1Note that LUKE uses two different [MASK] tokens: the
[MASK] word for MLM and the [MASK] entity for our pro-
posed pretraining task.

6445hidden dimensions, 24 hidden layers, L = 64 atten-
tion head dimensions, and 16 self-attention heads.
The number of dimensions of the entity token em-
bedding is set to H = 256. The total number of
parameters is approximately 483 M, consisting of
355 M in RoBERTa and 128 M in our entity em-
beddings. The input text is tokenized into words
using RoBERTa’s tokenizer with the vocabulary
consisting of Vw = 50K words. For computational
efﬁciency, our entity vocabulary does not include
all entities but only the Ve = 500K entities most
frequently appearing in our entity annotations. The
entity vocabulary also includes two special entities,
i.e., [MASK] and [UNK].

The model

is trained via iterations over
Wikipedia pages in a random order for 200K steps.
To reduce training time, we initialize the parame-
ters that LUKE have in common with RoBERTa
(parameters in the transformer and the embeddings
for words) using RoBERTa. Following past work
(Devlin et al., 2019; Liu et al., 2020), we mask 15%
of all words and entities at random. If an entity does
not exist in the vocabulary, we replace it with the
[UNK] entity. We perform pretraining using the
original self-attention mechanism rather than our
entity-aware self-attention mechanism because we
want an ablation study of our mechanism but can
not afford to run pretraining twice. Query matri-
ces of our self-attention mechanism (Qw2e, Qe2w,
and Qe2e) are learned using downstream datasets.
Further details of our pretraining are described in
Appendix A.

4 Experiments

We conduct extensive experiments using ﬁve entity-
related tasks: entity typing, relation classiﬁcation,
NER, cloze-style QA, and extractive QA. We use
similar model architectures for all tasks based on
a simple linear classiﬁer on top of the representa-
tions of words, entities, or both. Unless otherwise
speciﬁed, we create the input word sequence by
inserting tokens of [CLS] and [SEP] into the
original word sequence as the ﬁrst and the last to-
kens, respectively. The input entity sequence is
built using [MASK] entities, special entities intro-
duced for the task, or Wikipedia entities. The token
embedding of a task-speciﬁc special entity is ini-
tialized using that of the [MASK] entity, and the
query matrices of our entity-aware self-attention
mechanism (Qw2e, Qe2w, and Qe2e) are initialized
using the original query matrix Q.

Prec. Rec. F1
Name
77.4 60.6 68.0
UFET (Zhang et al., 2019)
76.4 71.0 73.6
BERT (Zhang et al., 2019)
78.4 72.9 75.6
ERNIE (Zhang et al., 2019)
KEPLER (Wang et al., 2019b)
77.2 74.2 75.7
KnowBERT (Peters et al., 2019) 78.6 73.7 76.1
K-Adapter (Wang et al., 2020)
79.3 75.8 77.5
77.6 75.0 76.2
RoBERTa (Wang et al., 2020)
79.9 76.6 78.2
LUKE

Table 1: Results of entity typing on the Open Entity
dataset.

Because we use RoBERTa as the base model in
our pretraining, we use it as our primary baseline
for all tasks. We omit a description of the baseline
models in each section if they are described in
Section 2. Further details of our experiments are
available in Appendix B.

4.1 Entity Typing

We ﬁrst conduct experiments on entity typing,
which is the task of predicting the types of an en-
tity in the given sentence. Following Zhang et al.
(2019), we use the Open Entity dataset (Choi et al.,
2018), and consider only nine general entity types.
Following Wang et al. (2020), we report loose
micro-precision, recall, and F1, and employ the
micro-F1 as the primary metric.

Model We represent the target entity using the
[MASK] entity, and enter words and the entity in
each sentence into the model. We then classify the
entity using a linear classiﬁer based on the corre-
sponding entity representation. We treat the task as
multi-label classiﬁcation, and train the model using
binary cross-entropy loss averaged over all entity
types.

Baselines UFET (Choi et al., 2018) is a conven-
tional model that computes context representations
using the bidirectional LSTM. We also use BERT,
RoBERTa, ERNIE, KnowBERT, KEPLER, and K-
Adapter as baselines.

Results Table 1 shows the experimental results.
LUKE signiﬁcantly outperforms our primary base-
line, RoBERTa, by 2.0 F1 points, and the previ-
ous best published model, KnowBERT, by 2.1 F1
points. Furthermore, LUKE achieves a new state
of the art by outperforming K-Adapter by 0.7 F1
points.

6446Name
BERT (Zhang et al., 2019)
C-GCN (Zhang et al., 2018b)
ERNIE (Zhang et al., 2019)
SpanBERT (Joshi et al., 2020)
MTB (Baldini Soares et al., 2019)
KnowBERT (Peters et al., 2019)
KEPLER (Wang et al., 2019b)
K-Adapter (Wang et al., 2020)
RoBERTa (Wang et al., 2020)
LUKE

Prec. Rec. F1
67.2 64.8 66.0
69.9 63.3 66.4
70.0 66.1 68.0
70.8 70.9 70.8
71.5
71.6 71.4 71.5
70.4 73.0 71.7
68.9 75.4 72.0
70.2 72.4 71.3
70.4 75.1 72.7

-

-

Name
F1
LSTM-CRF (Lample et al., 2016) 91.0
92.2
ELMo (Peters et al., 2018)
92.8
BERT (Devlin et al., 2019)
93.1
Akbik et al. (2018)
93.5
Baevski et al. (2019)
RoBERTa
92.4
94.3
LUKE

Table 3: Results of named entity recognition on the
CoNLL-2003 dataset.

Table 2: Results of relation classiﬁcation on the TA-
CRED dataset.

report the span-level F1.

4.2 Relation Classiﬁcation
Relation classiﬁcation determines the correct rela-
tion between head and tail entities in a sentence.
We conduct experiments using TACRED dataset
(Zhang et al., 2017), a large-scale relation classiﬁ-
cation dataset containing 106,264 sentences with
42 relation types. Following Wang et al. (2020),
we report the micro-precision, recall, and F1, and
use the micro-F1 as the primary metric.

introduce

two special

Model We
entities,
[HEAD] and [TAIL], to represent the head and
the tail entities, respectively, and input words and
these two entities in each sentence to the model.
We then solve the task using a linear classiﬁer
based on a concatenated representation of the
head and tail entities. The model is trained using
cross-entropy loss.

Baselines C-GCN (Zhang et al., 2018b) uses
graph convolutional networks over dependency tree
structures to solve the task. MTB (Baldini Soares
et al., 2019) learns relation representations based
on BERT through the matching-the-blanks task us-
ing a large amount of entity-annotated text. We also
compare LUKE with BERT, RoBERTa, SpanBERT,
ERNIE, KnowBERT, KEPLER, and K-Adapter.

Results The experimental results are presented
in Table 2. LUKE clearly outperforms our pri-
mary baseline, RoBERTa, by 1.4 F1 points, and
the previous best published models, namely MTB
and KnowBERT, by 1.2 F1 points. Furthermore, it
achieves a new state of the art by outperforming
K-Adapter by 0.7 F1 points.

4.3 Named Entity Recognition
We conduct experiments on the NER task using the
standard CoNLL-2003 dataset (Tjong Kim Sang
and De Meulder, 2003). Following past work, we

Model Following Sohrab and Miwa (2018), we
solve the task by enumerating all possible spans
(or n-grams) in each sentence as entity name can-
didates, and classifying them into the target entity
types or non-entity type, which indicates that the
span is not an entity. For each sentence in the
dataset, we enter words and the [MASK] entities
corresponding to all possible spans. The represen-
tation of each span is computed by concatenating
the word representations of the ﬁrst and last words
in the span, and the entity representation corre-
sponding to the span. We classify each span using
a linear classiﬁer with its representation, and train
the model using cross-entropy loss. We exclude
spans longer than 16 words for computational efﬁ-
ciency. During the inference, we ﬁrst exclude all
spans classiﬁed into the non-entity type. To avoid
selecting overlapping spans, we greedily select a
span from the remaining spans based on the logit of
its predicted entity type in descending order if the
span does not overlap with those already selected.
Following Devlin et al. (2019), we include the max-
imal document context in the target document.

Baselines LSTM-CRF (Lample et al., 2016) is
a model based on the bidirectional LSTM with con-
ditional random ﬁelds (CRF). Akbik et al. (2018)
address the task using the bidirectional LSTM with
CRF enhanced with character-level contextualized
representations. Similarly, Baevski et al. (2019)
use the bidirectional LSTM with CRF enhanced
with CWRs based on a bidirectional transformer.
We also use ELMo, BERT, and RoBERTa as base-
lines. To conduct a fair comparison with RoBERTa,
we report its performance using the model de-
scribed above with the span representation com-
puted by concatenating the representations of the
ﬁrst and last words of the span.

Results The experimental results are shown in
Table 3. LUKE outperforms RoBERTa by 1.9 F1

6447points. Furthermore, it achieves a new state of the
art on this competitive dataset by outperforming
the previous state of the art reported in Baevski
et al. (2019) by 0.8 F1 points.

4.4 Cloze-style Question Answering
We evaluate our model on the ReCoRD dataset
(Zhang et al., 2018a), a cloze-style QA dataset con-
sisting of over 120K examples. An interesting char-
acteristic of this dataset is that most of its questions
cannot be solved without external knowledge. The
following is an example question and its answer in
the dataset:
Question: According to claims in the suit,
“Parts of ’Stairway to Heaven,’ instantly recog-
nizable to the music fans across the world, sound
almost identical to signiﬁcant portions of ‘X.”’
Answer: Taurus
Given a question and a passage, the task is to ﬁnd
the entity mentioned in the passage that ﬁts the
missing entity (denoted by X in the question above).
In this dataset, annotations of entity spans (start and
end positions) in a passage are provided, and the
answer is contained in the provided entity spans
one or multiple times. Following past work, we
evaluate the models using exact match (EM) and
token-level F1 on the development and test sets.

Model We solve this task by assigning a
relevance score to each entity in the pas-
sage and selecting the entity with the high-
est score as the answer. Following Liu et al.
(2020), given a question q1, q2, ..., qj, and a
passage p1, p2, ..., pl, the input word sequence
is constructed as:
[CLS]q1, q2, ..., qj[SEP]
[SEP]p1, p2, ..., pl[SEP]. Further, we input
[MASK] entities corresponding to the missing en-
tity and all entities in the passage. We compute the
relevance score of each entity in the passage using
a linear classiﬁer with the concatenated representa-
tion of the missing entity and the corresponding en-
tity. We train the model using binary cross-entropy
loss averaged over all entities in the passage, and
select the entity with the highest score (logit) as the
answer.

Baselines DocQA+ELMo (Clark and Gardner,
2018) is a model based on ELMo, bidirectional
attention ﬂow (Seo et al., 2017), and self-attention
mechanism. XLNet+Veriﬁer (Li et al., 2019) is a
model based on XLNet with rule-based answer ver-
iﬁcation, and is the winner of a recent competition

Name

DocQA+ELMo (Zhang et al., 2018a)
BERT (Wang et al., 2019a)
XLNet+Veriﬁer (Li et al., 2019)
RoBERTa (Liu et al., 2020)
RoBERTa (ensemble) (Liu et al., 2020)
LUKE

Dev
F1

Test
EM

Test
Dev
F1
EM
44.1 45.4 45.4 46.7
71.3 72.0
80.6 82.1 81.5 82.7
89.0 89.5

-

-

-

-

-

-

90.0 90.6
90.8 91.4 90.6 91.2

Table 4: Results of cloze-style question answering on
the ReCoRD dataset. All models except RoBERTa (en-
semble) are based on a single model.

based on this dataset (Ostermann et al., 2019). We
also use BERT and RoBERTa as baselines.

Results The results are presented in Table 4.
LUKE signiﬁcantly outperforms RoBERTa, the
best baseline, on the development set by 1.8 EM
points and 1.9 F1 points. Furthermore, it achieves
superior results to RoBERTa (ensemble) on the test
set without ensembling the models.

4.5 Extractive Question Answering
Finally, we conduct experiments using the well-
known Stanford Question Answering Dataset
(SQuAD) 1.1 consisting of 100K question/answer
pairs (Rajpurkar et al., 2016). Given a question
and a Wikipedia passage containing the answer,
the task is to predict the answer span in the pas-
sage. Following past work, we report the EM and
token-level F1 on the development and test sets.

Model We construct the word sequence from the
question and the passage in the same way as in the
previous experiment. Unlike in the other experi-
ments, we input Wikipedia entities into the model
based on entity annotations automatically gener-
ated on the question and the passage using a map-
ping from entity names (e.g., “U.S.”) to their refer-
ent entities (e.g., United States). The mapping is
automatically created using the entity hyperlinks in
Wikipedia as described in detail in Appendix C. We
solve this task using the same model architecture as
that of BERT and RoBERTa. In particular, we use
two linear classiﬁers independently on top of the
word representations to predict the span boundary
of the answer (i.e., the start and end positions), and
train the model using cross-entropy loss.

Baselines We compare our models with the re-
sults of recent CWRs, including BERT, RoBERTa,
SpanBERT, XLNet, and ALBERT. Because the re-
sults for RoBERTa and ALBERT are reported only
on the development set, we conduct a comparison

6448Name

BERT (Devlin et al., 2019)
SpanBERT (Joshi et al., 2020)
XLNet (Yang et al., 2019)
ALBERT (Lan et al., 2020)
RoBERTa (Liu et al., 2020)
LUKE

-

-

Dev
F1

Test
EM

Test
Dev
F1
EM
84.2 91.1 85.1 91.8
88.8 94.6
89.0 94.5 89.9 95.1
89.3 94.8
88.9 94.6
89.8 95.0 90.2 95.4

-
-

-
-

Table 5: Results of extractive question answering on
the SQuAD 1.1 dataset.

Name

CoNLL-2003

(Test F1)

SQuAD
(Dev EM)

SQuAD
(Dev F1)

LUKE w/o entity inputs
LUKE

92.9
94.3

89.2
89.8

94.8
95.0

Table 6: Ablation study of our entity representations.

with these models using this set. To conduct a
fair compassion with RoBERTa, we use the same
model architecture and hyper-parameters as those
of RoBERTa (Liu et al., 2020).

Results The experimental results are presented
in Table 5. LUKE outperforms our primary base-
line, RoBERTa, by 0.9 EM points and 0.4 F1 points
on the development set. Furthermore, it achieves a
new state of the art on this competitive dataset by
outperforming XLNet by 0.3 points both in terms
of EM and F1. Note that XLNet uses a more so-
phisticated model involving beam search than the
other models considered here.

5 Analysis

In this section, we provide a detailed analysis of
LUKE by reporting three additional experiments.

5.1 Effects of Entity Representations

To investigate how our entity representations in-
ﬂuence performance on downstream tasks, we per-
form an ablation experiment by addressing NER on
the CoNLL-2003 dataset and extractive QA on the
SQuAD dataset without inputting any entities. In
this setting, LUKE uses only the word sequence to
compute the representation for each word. We ad-
dress the tasks using the same model architectures
as those for RoBERTa described in the correspond-
ing sections. As shown in Table 6, this setting
clearly degrades performance, i.e., 1.4 F1 points on
the CoNLL-2003 dataset and 0.6 EM points on the
SQuAD dataset, demonstrating the effectiveness of
our entity representations on these two tasks.

5.2 Effects of Entity-aware Self-attention
We conduct an ablation study of our entity-aware
self-attention mechanism by comparing the perfor-
mance of LUKE using our mechanism with that
using the original mechanism of the transformer.
As shown in Table 7, our entity-aware self-attention
mechanism consistently outperforms the original
mechanism across all tasks. Furthermore, we ob-
serve signiﬁcant improvements on two kinds of
tasks, relation classiﬁcation (TACRED) and QA
(ReCoRD and SQuAD). Because these tasks in-
volve reasoning based on relationships between
entities, we consider that our mechanism enables
the model (i.e., attention heads) to easily focus on
capturing the relationships between entities.

5.3 Effects of Extra Pretraining
As mentioned in Section 3.4, LUKE is based on
RoBERTa with pretraining for 200K steps using
our Wikipedia corpus. Because past studies (Liu
et al., 2020; Lan et al., 2020) suggest that simply
increasing the number of training steps of CWRs
tends to improve performance on downstream tasks,
the superior experimental results of LUKE com-
pared with those of RoBERTa may be obtained
because of its greater number of pretraining steps.
To investigate this, we train another model based
on RoBERTa with extra pretraining based on the
MLM using the Wikipedia corpus for 200K train-
ing steps. The detailed conﬁguration used in the
pretraining is available in Appendix A.

We evaluate the performance of this model on
the CoNLL-2003 and SQuAD datasets using the
same model architectures as those for RoBERTa
described in the corresponding sections. As shown
in Table 8, the model achieves similar performance
to the original RoBERTa on both datasets, which
indicates that the superior performance of LUKE
is not owing to its longer pretraining.

6 Conclusions

In this paper, we propose LUKE, new pretrained
contextualized representations of words and enti-
ties based on the transformer. LUKE outputs the
contextualized representations of words and en-
tities using an improved transformer architecture
with using a novel entity-aware self-attention mech-
anism. The experimental results prove its effective-
ness on various entity-related tasks. Future work
involves applying LUKE to domain-speciﬁc tasks,
such as those in biomedical and legal domains.

6449Name

Open Entity

(Test F1)

TACRED
(Test F1)

CoNLL-2003

(Test F1)

ReCoRD
(Dev EM)

ReCoRD
(Dev F1)

SQuAD
(Dev EM)

SQuAD
(Dev F1)

Original Attention
Entity-aware Attention

77.9
78.2

72.2
72.7

94.1
94.3

90.1
90.8

90.7
91.4

89.2
89.8

94.7
95.0

Table 7: Ablation study of our entity-aware self-attention mechanism.

Name

CoNLL-2003

(Test F1)

SQuAD
(Dev EM)

SQuAD
(Dev F1)

RoBERTa w/ extra training
RoBERTa
LUKE

92.5
92.4
94.3

89.1
88.9
89.8

94.7
94.6
95.0

Table 8: Results of RoBERTa additionally trained using
our Wikipedia corpus.

