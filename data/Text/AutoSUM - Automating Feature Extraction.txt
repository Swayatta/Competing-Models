0
2
0
2

 

y
a
M
5
2

 

 
 
]

R

I
.
s
c
[
 
 

1
v
8
8
8
1
1

.

5
0
0
2
:
v
i
X
r
a

AutoSUM: Automating Feature Extraction and

Multi-user Preference Simulation for

Entity Summarization

Dongjun Wei1,2 (cid:63), Yaxin Liu1,2 (cid:63), Fuqing Zhu1, Liangjun Zang1,

Wei Zhou1, Yijun Lu3, and Songlin Hu1,2

1 Institute of Information Engineering, Chinese Academy of Sciences, China
2 School of Cyber Security, University of Chinese Academy of Sciences, China

3 Alibaba Cloud Computing Co. Ltd., Beijing, China

{weidongjun, liuyaxin, zhufuqing, zangliangjun, zhouwei,

husonglin}@iie.ac.cn

yijun.lyj@alibaba-inc.com

Abstract. With the growth of knowledge graphs, entity descriptions are
becoming extremely lengthy. Entity summarization task, aiming to gen-
erate diverse, comprehensive and representative summaries for entities,
has received an increasing interest recently. In most previous methods,
features are usually extracted by the hand-crafted templates. Then the
feature selection and multi-user preference simulation take place, depend-
ing too much on human expertise. In this paper, a novel integration me-
thod called AutoSUM is proposed for automatic feature extraction and
multi-user preference simulation to overcome the drawbacks of previous
methods. There are two modules in AutoSUM: extractor and simulator.
The extractor module operates automatic feature extraction based on
a BiLSTM with a combined input representation including word em-
beddings and graph embeddings. Meanwhile, the simulator module au-
tomates multi-user preference simulation based on a well-designed two-
phase attention mechanism (i.e., entity-phase attention and user-phase
attention). Experimental results demonstrate that AutoSUM produces
the state-of-the-art performance on two widely used datasets (i.e., DB-
pedia and LinkedMDB) in both F-measure and MAP. The source code
and outputs are available at https://github.com/WeiDongjunGabriel/
AutoSUM.
Keywords: Entity summarization · Feature extraction · Preference sim-
ulation · Attention Mechanism · Knowledge graphs

1

Introduction

Semantic data enables users or machines to comprehend and manipulate the
conveyed information quickly [10]. In major knowledge graphs, semantic data
describes entities by Resource Description Framework (RDF) triples, referred

(cid:63) Equal contribution. Fuqing Zhu is the corresponding author.

2

Wei D, Liu Y, et al.

as triples [4]. With the growth of knowledge graphs, entity descriptions are be-
coming extremely lengthy [23]. Since Google ﬁrst released the knowledge graph,
“get the best summary” for entities has been one of the main contributions in
Google Search4 [25]. Speciﬁcally, Google Search returns a top-k subset of triples
which can best describe the entity from a query on the right-hand side of the
result pages [15]. Motivated by the success of Google Search, entity summariza-
tion task has received an increasing interest recently [7, 25], it aims to generate
diverse, comprehensive and representative summaries for entities. In addition,
entity summarization has been integrated into various applications such as doc-
ument browsing, Question Answering (QA), etc [15].

Most previous entity summarization methods are adopted from random sur-
fer [4], clustering [9, 10] and Latent Dirichlet Allocation (LDA) [19] models, de-
pending too much on the hand-crafted templates for feature extraction as well as
human expertise for feature selection. Meanwhile, entities are capable to repre-
sent diverse information (or multi-aspect information) in knowledge graphs [21],
resulting in diﬀerent user preference (sometimes multi-user preference [27]). Take
entity Triathlon_ at_the_2000_Summer_Olympics_Men’s in DBpedia5 for in-
stance, diﬀerent users may prefer to the medal, event or type of this entity, re-
spectively. In order to generate more diverse summaries, the speciﬁc model needs
to be selected for providing a more distinguishable multi-user preference simula-
tion [9,21]. However, due to the countless quantities and unpredictable types of
entities in real large-scale knowledge graphs, extracting discriminative features
or selecting suitable models based on human expertise could be arduous [15].

In this paper, a novel integration method called AutoSUM is proposed for au-
tomatic feature extraction and multi-user preference simulation to overcome the
drawbacks of above previous models. There are two modules in AutoSUM: ex-
tractor and simulator. The extractor module operates automatic feature extrac-
tion based on a BiLSTM with a combined input representation including word
embeddings and graph embeddings. Meanwhile, the simulator module automates
multi-user preference simulation based on a well-designed two-phase attention
mechanism (i.e., entity-phase attention and user-phase attention). Experimental
results demonstrate that AutoSUM produces the state-of-the-art performance on
two widely used datasets (i.e., DBpedia and LinkedMDB 6) in both F-measure
and MAP.

2 Related Work

Previous entity summarization methods mainly rely on human expertise. To ﬁnd
the most central triples, RELIN [4] and SUMMARUM [24] compute the related-
ness and informativeness based on the features extracted from hand-crafted tem-
plates. Meanwhile, FACES [9] and ES-LDA [19] introduce a clustering algorithm
and LDA model for capturing multi-aspect information, respectively. In order to
4 https://www.google.com
5 https://wiki.dbpedia.org
6 http://data.linkedmdb.org

AutoSUM

3

generate more diverse summaries, the speciﬁc models need to be selected for pro-
viding a more distinguishable multi-user preference simulation [9, 19]. However,
due to the countless quantities and unpredictable types of entities in the real
large-scale knowledge graphs, extracting discriminative features and selecting
suitable models based on human expertise could be arduous.

Recently, deep learning methods relieve the dependency on human exper-
tise in Natural Language Processing (NLP) [17] community. To generate the
summaries without human expertise, an entity summarization method with a
single-layer attention (ESA) [29] is proposed to calculate the attention score for
each triple. Then top-k triples which have the highest attention scores are se-
lected as the ﬁnal results. However, ESA cannot extract features and capture
multi-aspect information with the single-layer attention mechanism. Following
ESA work, our proposed AutoSUM automates feature extraction and multi-user
preference based on a novel extractor-simulator structure. In extractor, a BiL-
STM with a combined input representation is utilized for feature extraction.
The word embeddings and graph embeddings are included. Meanwhile, in sim-
ulator, a two-phase attention mechanism is designed for multi-user preference
simulation.

3 Proposed Model

3.1 Problem Description

An RDF triple is composed of a subject, a predicate, and an object. In ma-
jor knowledge graphs, an entity of which is then deﬁned as a subject with all
predicates and corresponding objects to those predicates. When a user queries
an entity in a knowledge graph, a set of triples {t1, t2,··· , tn} related with the
entity will be returned, referred as an entity description document d, where ti
is the i-th triple in d. Following Google Search [7, 15], given a positive integer
k, the summary of an entity is a top-k subset of d which can best describe the
entity.

3.2 Overview

As shown in Figure 1, AutoSUM has a novel extractor-simulator structure. The
extractor extracts the features of triples in d as h = {h1, h2,··· , hn}, where hi
is the feature vector of ti. Given h, the simulator calculates the attention scores
a = {a1, a2,··· , an}, where ai is the attention score of ti. Then top-k triples with
the highest attention scores will be selected as the summary of an entity.

3.3 Extractor

The extractor module in AutoSUM aims at extracting features of triples auto-
matically. In this section, we introduce the input representation and the auto-
matic feature extraction in details.

4

Wei D, Liu Y, et al.

Fig. 1. The architecture of AutoSUM.

Input Representation As discussed above, the triples related with an entity
share the same subject with diﬀerent predicates and corresponding objects to
those predicates. In order to map predicates and objects into a continuous vector
space for feature extraction, we apply a combined input representation method
including word embeddings and graph embeddings. Then we concatenate the
embeddings of the predicates and corresponding objects as the representation
for each triple.

Word Embedding: Learning word embeddings has been an eﬀective me-
thod to enhance the performance of entity sumamrizers. In ES-LDAext [20],
Pouriyeh et al. stated the key point of learning word embeddings was the deﬁ-
nition for “words”. Following Pouriyeh’s work, we extract predicates and objects
of triples as our words. Take “http://dbpedia.org/ontology/goldMedalist” for in-
stance, we extract âĂĲgoldMedalistâĂİ as the word for the above predicate.
Given the embeddings of words, we then initialize a word embedding (lookup)
table for future training.

Graph Embedding: Obviously, simple word embeddings cannot represent
triples with a graph structure. To fully encode the graph information, we utilize
a graph embedding technique called TransE [3] to pretrain the whole knowledge
graph in the dataset. Given the embeddings of tirples, we then initialize a graph
embedding table for future training.

Automatic Feature Extraction In Named Entity Recognition (NER) task,
the bidirectional LSTM (BiLSTM) has been widely used for automatic feature
extraction [14]. For instance, in order to automatically extract features from a
small and supervised training corpus, an LSTM-CRF model was proposed by
Lample et al. [14], utilizing a BiLSTM for feature extraction and conditional ran-
dom ﬁelds [13] for entity recognition. The BiLSTM extracted representative and
contextual features of a word, aligning with other words in the same sentence [8].
As for summarizing entities, we also apply a BiLSTM to extract features of a
triple, aligning with other triples related with the same entity. Speciﬁcally, due
to the uncertain timing sequence of triples, we ﬁrst map (serialize) the triples
into a sequence comes randomly. Then we feed the input representation of triples

Word EmbeddingGraph EmbeddingEntityLong Short-Term Memory Long Short-Term Memory++ExtractorSerializationKnowledgeGraphRDFtriplesqueryFCNMulti-aspect Attention Subject-phase AttentionMSimulateGeneral Attention SoftmaxLong Short-Term MemoryLong Short-Term Memory++User-phase AttentionFeaturesOutputs(RDF set)(Attentionscores)SimulatorAutoSUM

5

in the sequence to the BiLSTM, and take the outputs as the extracted features
for those triples.

3.4 Simulator

The simulator in AutoSUM aims at simulating multi-user preference based on
a well-designed two-phase attention mechanism (i.e., entity-phase attention and
user-phase attention). Entity-phase attention captures multi-aspect information
from an entity, user-phase attention then simulates multi-user preference based
on the captured information. In this section, we present the details of entity-
phase attention and user-phase attention.

Entity-Phase Attention The intuition of entity-phase attention is straight-
forward. Since the single-layer attention mechanism in ESA [29] cannot cap-
ture multi-aspect information, we then design a multi-aspect attention mecha-
nism with multiple (stacked) attention layers to overcome the drawback of ESA.
One seminal work using stacked attention layers is neural machine translation
(NMT) [17], where the stacked attention layers (Transformer) [26] are utilized to
capture the multi-aspect information from a sentence. To our knowledge, we are
the ﬁrst to utilize the stacked attention layers to capture the multi-aspect infor-
mation from an entity. Speciﬁcally, diﬀerent attention layers capture information
from an entity in diﬀerent aspects. In each attention layer, a general attention
function [17] is utilized to calculate the relevance between each triple and the
information captured from the attention layer, termed attention scores. Here,
instead of combining all attention layers to generate overall attention scores of
Transformer [26], we directly output the attention scores from each attention
layer for multi-user preference simulation in user-phase attention. Notice that
the number of the attention layers is a hyper-parameter which can be tuned
during training.

User-Phase Attention When users browse triples, they will allocate high
preference values (more attention) to triples which are more related with the
information they are interested in [9]. Meanwhile, as described above, entity-
phase attention consists of diﬀerent attention layers for capturing information in
diﬀerent aspects. In each attention layer, a general attention function is utilized
to allocate higher attention scores to the triples which are more relevant to the
information captured from the attention layer. To simulate the preference of
users who are interested in the information captured by the current attention
layer, user-phase attention assigns the user preference values of each triple with
the same attention scores from the attention layer. Then diﬀerent distributions
of attention scores in diﬀerent attention layers simulate the diﬀerent preference
of diﬀerent users (multi-user preference).

After simulating the multi-user preference, we have to allocate diﬀerent at-
tention scores for diﬀerent user preference rather than treating them equally. The
main reason is that some user preference may represent the preference of most

6

Wei D, Liu Y, et al.

users for an entity, while others may represent the preference of few users for the
same entity. Allocating proper attention scores for each user preference is crit-
ical to generate a more comprehensive entity summarization result. Therefore,
we combine a BiLSTM with a general attention score function for allocation.
In NER, a BiLSTM can maintain the independence and capture the intrinsic
relationships among words [8]. Similarly, a BiLSTM is adopted in user-phase
attention to preserve independence as well as capture the intrinsic relationships
between diﬀerent user preference. Then the outputs of the BiLSTM are taken
as the inputs to a general attention score function, in order to allocate attention
scores for each user preference. At last, we integrate all the user preference based
on the allocated attention scores. In addition, due to the uncertain order in user
preference like triples, we also randomly map the user preference into a sequence
as our input of the BiLSTM.

3.5 The Complete Pipeline
In this section, we demonstrate the complete pipeline of AutoSUM. As described
in Section 3.1, the input of AutoSUM is an entity description document d =
{t1, t2,··· , tn}. Here, ti is the i-th triple in d, which is composed of a same subject
s, a predicate pi and an object oi. Given d, we ﬁrst split d into a predicate set
p = {p1, p2,··· , pn} and an object set o = {o1, o2,··· , on}, respectively. Given p
and o, we combine word embeddings and graph embeddings to map pi and oi
into a continuous vector space and concatenate them as ei, recursively. Given
e = {e1, e2,··· , en}, we randomly map e into a sequence q = (q1, q2,··· , qn).
Then we apply a BiLSTM to extract the features vector hi of qi as follows,

−→
hi = LST ML(qi,
←−
hi = LST MR(qi,
←−
hi], c = [−→c ,←−c ],

−−→
hi−1), i ∈ [1, n],
←−−
hi−1), i ∈ [1, n],

−→
hi,

hi = [

(1)
where −→c and ←−c are the ﬁnal hidden states in forward and backward LSTM
networks. Given h = {h1, h2,··· , hn} and c, we utilize the multi-aspect atten-
tion mechanism to capture multi-aspect information. Speciﬁcally, for the j-th
attention layer in multi-aspect attention mechanism, we calculate the attention
score si

j for triple ti with a general score attention function as follows,

j = scorej(hj, c) = hT
si
sj = [s1

i Wjc, i ∈ [1, n], j ∈ [1, m],
j ,··· , sn
j , s2
j ],

(2)
where Wj is a parameter matrix of the general attention score function in the
j-th attention layer, and m is the number of attention layers in the multi-aspect
attention mechanism. Given s = {s1, s2,··· , sm}, we then simulate the prefer-
ence of the j-th user uj who is interested in the information of triple ti captured
by the j-th attention layer as follows,

ui
j = si

j, i ∈ [1, n], j ∈ [1, m],

uj = [u1

j , u2

j ,··· , un
j ],

(3)

where ui
u1, u2,··· , um }, we randomly map u into a sequence q∗ = (q∗
utilize a BiLSTM to encode uj into u∗

j is the preference value allocated to triple ti by uj. Given u = {
m) and

2,··· , q∗

j as follows,

1, q∗

AutoSUM

7

−→
−−→
j−1), j ∈ [1, m],
u∗
u∗
j = LST ML(q∗
j ,
←−
←−−
j−1), j ∈ [1, m],
u∗
u∗
j = LST MR(q∗
j ,
←−
u∗
j ], c∗ = [

u∗
i = [

←−
c∗ ],

−→
u∗
j ,

−→
c∗ ,

(4)

−→
c∗ and

←−
c∗ are the ﬁnal hidden states from forward and backward LSTM
where
networks. Then we calculate the attention score for user preference as follows,

a∗ = [u∗

1, u∗

2,··· , u∗

m]W ∗c∗T

(5)
where W ∗ is a parameter matrix of the general attention score function. Having
obtained a∗, we integrate diﬀerent user preference to generate the ﬁnal attention
score for each triple ti in d as follows,

,

a = Sof tmax([u1, u2,··· , um]a∗T

) = [a1, a2,··· , an].

(6)

Finally, we employ cross-entropy loss and deﬁne the loss function L for Auto-
SUM,

L(a, a) = CrossEntropy(a, a).

(7)
Here, a = {a1, a2,··· , an} is a gold(real) attention score vector associated with
above entity from ESBM dataset. Speciﬁcally, we count the frequency of the i-th
triple ti selected by users in ESBM dataset following ESA work, denoted as ci.
Then the gold attention score αi of ti is formulated as follows,

ci(cid:80)n

i=1 ci

.

(8)

αi =

4 Experiments

4.1 Experimental Setup

Dataset In this paper, we utilize ESBM dataset v1.1, which consists of 6.8k
triples related with 125 entities from DBpedia [2] and 2.6k triples related with
50 entities from LinkedMDB [5]. Given an entity, ESBM asks 5 diﬀerent users
to select top-5 and top-10 triples which can best describe the entity. In addition,
ESBM provides an evaluator for the comparison of diﬀerent entity summariza-
tion methods. Both datasets and evaluator can be accessed from the ESBM
website7.

7 http://ws.nju.edu.cn/summarization/esbm/

8

Wei D, Liu Y, et al.

Baselines Our baselines consist of some existing state-of-the-art entity summa-
rization methods, including RELIN [4], DIVERSUM [21], CD [30], FACES [9],
LinkSUM [23], MPSUM [28] and ESA [29]. MPSUM 8 is an open source imple-
mentation of ES-LDA. To provide ablation studies, we also modify the original
AutoSUM into 5 diﬀerent versions, denoted as AutoSUM1∼5, which will be futher
illustrated in Section 4.3.

Evaluation Methodology Summarization tasks can be mainly divided into
extractive and non-extractive tasks [1, 16], which orient to unstructured and
structured data, respectively. Sydow et al. [22] stated that entity summariza-
tion task could be treated as an extractive task of information retrieval (IR).
IR returns the most relevant documents for a query, while entity summariza-
tion selects the top-k triples related with an entity. Following previous work,
we utilize F-measure and mean average precision (MAP) metrics for evaluation,
which are two standard evaluation metrics in IR [12, 15]. F-measure is the har-
monic mean of recall and precision, and MAP is the mean average of precision.
Meanwhile, given the limited number of entities in ESBM, we conduct 5-fold
cross-validation to reduce the risk of overﬁtting without losing the number of
learning instances [11]. Speciﬁcally, the entities in ESBM are divided into 5 folds
randomly. The parameters for each model are tuned on 4-of-5 folds. The ﬁnal
fold in each case is utilized to evaluate the optimal parameters. Since ESA has
signiﬁcantly better than all other state-of-the-art methods in our baselines, we
then compare the statistical signiﬁcance among ESA and AutoSUMs (i.e., the
original AutoSUM and the modiﬁed AutoSUM1∼5, respectively) utilizing Stu-
dent’s paired t-test (p-value ≤ 0.05) [12].

Experimental Details For experimental details, we tune the parameters on
a validation set (i.e., a part of the training set). Speciﬁcally, to learn graph
embeddings, we utilize TransE to pretrain the whole ESBM dataset. Here, the
dimension of each triple is set to 100. As for word embeddings, we initialize
the lookup table randomly, where the dimension of each word is set to 100.
Then we apply a BiLSTM with a single layer in each LSTM cell for feature
extraction, where the number of the layers in multi-aspect mechanism is set to
6. In addition, the graph embedding of each triple is ﬁxed after pretraining, while
all other parameters in AutoSUM are initialized randomly and tuned without
weight sharing. We train the AutoSUM model for 200 epochs, and report the
results of the best epoch under early stopping.

4.2 Experimental Results

As shown in Table 1 and 2, AutoSUM is signiﬁcantly better than some existing
state-of-art methods in our baselines.

8 https://github.com/WeiDongjunGabriel/MPSUM

9
Table 1. F-measure comparison for top-5 and top-10 entity summarization. ↑ % is the
relative improvement of AutoSUM, and (+/-) is the indicator of signiﬁcant improve-
ment or degradation with respect to ESA (p-value ≤ 0.05).

AutoSUM

↑ %

DBpedia

LinkedMDB

ALL

0.455
0.507
0.517
0.428
0.488
0.479
0.510
0.525

0.203
0.207
0.211
0.169
0.313
0.140
0.270
0.320

Model
k = 5 k = 10 k = 5 k = 10 k = 5 k = 10 min max avg
0.242
RELIN
25
DIVERSUM 0.249
12
0.287
CD
10
0.270
FACES
23
0.280
17
FACES-E
0.274
LINKSUM
18
0.289
MPSUM
11
ESA
0.310
8
0.387+ 0.569+ 0.443+ 0.556+ 0.403+ 0.565+ -
AutoSUM
0.303− 0.425− 0.316
0.442− 0.290− 0.462− 22
AutoSUM1
0.375+ 0.463− 0.333− 0.517+ 6
0.316+ 0.538
AutoSUM2
0.221− 0.390− 0.330+ 0.406− 0.252− 0.394− 34
AutoSUM3
0.394− 0.270− 0.411− 36
0.254− 0.417− 0.309
AutoSUM4
0.325+ 0.532+ 0.343− 0.413+ 0.323
0.502+ 7
AutoSUM5

118
114
110
162
48
216
64
38
-
40
22
75
52
35

0.258
0.358
0.328
0.263
0.393
0.279
0.380
0.403

0.231
0.237
0.252
0.241
0.289
0.236
0.301
0.312

0.399
0.464
0.455
0.381
0.461
0.421
0.479
0.491

72
54
52
73
38
80
35
26
-
31
16
49
43
21

Comparison with Traditional Methods: Compared with traditional me-
thods depending on manual feature extraction and multi-user preference simu-
lation, AutoSUM automates the above processes without any human expertise
eﬀectively. The average improvement of AutoSUM over the best outperforming
traditional methods is 38% and 36%, in terms of F-measure and MAP, respec-
tively.

Table 2. MAP comparison for top-5 and top-10 entity summarization. ↑ % is the rel-
ative improvement of AutoSUM, and (+/-) is the indicator of signiﬁcant improvement
or degradation with respect to ESA (p-value ≤ 0.05).

↑ %

DBpedia

LinkedMDB

ALL

0.519
0.499
-
0.382
0.564
0.271
0.568
0.582

0.241
0.266
-
0.155
0.341
0.141
0.351
0.367

Model
k = 5 k = 10 k = 5 k = 10 k = 5 k = 10 min max avg
RELIN
0.342
DIVERSUM 0.310
CD
FACES
FACES-E
LinkSUM
MPSUM
ESA
AutoSUM
AutoSUM1
AutoSUM2
AutoSUM3
AutoSUM4
AutoSUM5

-
0.255
0.388
0.242
0.386
0.392
0.459+ 0.647+ 0.517+ 0.600+ 0.476+ 0.633+ -
0.419− 0.508− 0.420+ 0.522+ 0.389− 0.563
10
0.598− 0.431+ 0.525+ 0.412− 0.578+ 8
0.404
0.291− 0.456− 0.383+ 0.488+ 0.317− 0.465− 23
0.346− 0.480− 28
0.333− 0.486− 0.376− 0.467
11
0.412+ 0.550
0.473
0.405+ 0.582

115
94
-
234
64
267
47
41
-
27
20
58
38
40

55
53
-
114
36
132
30
23
-
18
14
41
34
21

0.335
0.390
-
0.273
0.435
0.279
0.435
0.465

0.313
0.298
-
0.227
0.375
0.213
0.349
0.386

0.466
0.468
-
0.351
0.527
0.345
0.532
0.549

25
30
-
69
15
68
14
11

0.368

Comparison with Deep Learning Methods: Compared with ESA, which
calculates attention scores without feature extraction and multi-user preference,
AutoSUM achieves the state-of-the-art performance. The average improvement

10

Wei D, Liu Y, et al.

of AutoSUM over ESA is 26% and 23%, in terms of F-measure and MAP, re-
spectively.

In addition, we track the attention scores of entity Triathlon (Triathlon_at_
the_ 2000_Summer_Olympics_Men’s) in user-phase attention, as shown in Fig-
ure 2. We can observe that the user-phase attention simulates 3 groups of user
preference of the entity, and the entity-phase attention allocates high attention
scores to users who prefer medal as well as event than property, which is in
accordance with the preference of most users in real world.

Fig. 2. The attention scores of Triathlon_at_the_2000_Summer_Olympics_Men’s.

4.3 Ablation Studies

In this section, we provide ablation studies to demonstrate the eﬀectiveness of
the primary modules in AutoSUM.

AutoSUM1: To evaluate the features extracted by AutoSUM, AutoSUM1
removes the BiLSTM in extractor and feeds the input representation of triples
into simulator directly. Experimental results show the original AutoSUM is
signiﬁcantly better than AutoSUM1, proving that the BiLSTM extracts high-
quality features for user-preference simulation.

AutoSUM2 and AutoSUM4: To explore whether the attention scores of
diﬀerent user preference are appropriate, AutoSUM2 removes the BiLSTM in
simulator and allocates equal attention scores for each user preference. Mean-
while, we also attempt to replace the BiLSTM with an FCN, referred as Auto-
SUM4. As shown in Table 1 and 2, the original AutoSUM gains a signiﬁcant im-
provement over AutoSUM2 and AutoSUM4, indicating the BiLSTM with a gen-
eral attention function allocates appropriate attention scores for each user pref-
erence. In addition, we can observe that the performance of FCN (AutoSUM2)
is even worse than allocating equal attention scores (AutoSUM4) in our experi-
ments.

AutoSUM3: For comparison, AutoSUM4 removes the BiLSTM in both ex-
tractor and simulator. Experimental results show that the performance of Auto-
SUM3 is worse than AutoSUM1 and AutoSUM2, which remove the BiLSTM

user 1user 2user 3user 4user 5user 6goldMedalistnextEventbronzeMedalistpreviousEventhomepagesilverMedalsubjecttypethumbnaillabelmedalpropertyeventmedalpropertyeventAutoSUM

11

in extractor and simulator respectively, further proving the irreplaceable role of
BiLSTM in AutoSUM.

AutoSUM5 To explore whether the multi-aspect mechanism captures the
multi-aspect information from an entity, we replace the multi-aspect mechanism
with a single-aspect mechanism, i.e., setting the number of attention layers to 1.
As shown in Table 1 and 2, we can observe that the original AutoSUM outper-
forms AutoSUM5 in both F-measure and MAP. Experimental results indicate
that the multi-aspect attention mechanism successfully captures the multi-aspect
information. We also notice that AutoSUM5 with a single-layer attention mech-
anism still outperforms all other methods in our baselines including ESA.

5 Conclusion

In this paper, we propose a novel integration model called AutoSUM to au-
tomate feature extraction and multi-user preference simulation for entity sum-
marization. The performance of our proposed AutoSUM is signiﬁcantly better
than other state-of-the-art methods in both F-measure and MAP. Meanwhile,
suﬃcient ablation studies are provided to demonstrate the eﬀectiveness of each
module in AutoSUM. In the future, we expect to expand the ESBM dataset and
introduce the notion of AutoSUM into other applications such as recommender
systems [6, 18].

Acknowledgment

This research is supported in part by the Beijing Municipal Science and Tech-
nology Project under Grant Z191100007119008.

