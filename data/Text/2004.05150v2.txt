Longformer: The Long-Document Transformer

Arman Cohan∗
Iz Beltagy∗
Allen Institute for Artiﬁcial Intelligence, Seattle, WA, USA
{beltagy,matthewp,armanc}@allenai.org

Matthew E. Peters∗

0
2
0
2
 
c
e
D
2

 

 
 
]
L
C
.
s
c
[
 
 

2
v
0
5
1
5
0

.

4
0
0
2
:
v
i
X
r
a

Abstract

Transformer-based models are unable to pro-
cess long sequences due to their self-attention
operation, which scales quadratically with the
sequence length. To address this limitation,
we introduce the Longformer with an attention
mechanism that scales linearly with sequence
length, making it easy to process documents of
thousands of tokens or longer. Longformer’s
attention mechanism is a drop-in replacement
for the standard self-attention and combines
a local windowed attention with a task moti-
vated global attention. Following prior work
on long-sequence transformers, we evaluate
Longformer on character-level language mod-
eling and achieve state-of-the-art results on
text8 and enwik8.
In contrast to most
prior work, we also pretrain Longformer and
ﬁnetune it on a variety of downstream tasks.
Our pretrained Longformer consistently out-
performs RoBERTa on long document tasks
and sets new state-of-the-art results on Wiki-
Hop and TriviaQA. We ﬁnally introduce the
Longformer-Encoder-Decoder (LED), a Long-
former variant for supporting long document
generative sequence-to-sequence tasks, and
demonstrate its effectiveness on the arXiv sum-
marization dataset.1

1

Introduction

Transformers (Vaswani et al., 2017) have achieved
state-of-the-art results in a wide range of natu-
ral language tasks including generative language
modeling (Dai et al., 2019; Radford et al., 2019)
and discriminative language understanding (De-
vlin et al., 2019). This success is partly due to
the self-attention component which enables the net-
work to capture contextual information from the
entire sequence. While powerful, the memory and
computational requirements of self-attention grow

∗ Equal contribution.
1https://github.com/allenai/longformer

full

Runtime and memory of

self-
Figure 1:
attention and different
implementations of Long-
former’s self-attention; Longformer-loop is non-
vectorized, Longformer-chunk is vectorized, and
Longformer-cuda is a custom cuda kernel im-
plementations. Longformer’s memory usage scales
linearly with the sequence length, unlike the full
self-attention mechanism that runs out of memory
for long sequences on current GPUs.
Different
implementations vary in speed, with the vectorized
Longformer-chunk being the fastest. More details
are in section 3.2.

quadratically with sequence length, making it infea-
sible (or very expensive) to process long sequences.
To address this limitation, we present Long-
former, a modiﬁed Transformer architecture with
a self-attention operation that scales linearly with
the sequence length, making it versatile for pro-
cessing long documents (Fig 1). This is an advan-
tage for natural language tasks such as long docu-
ment classiﬁcation, question answering (QA), and
coreference resolution, where existing approaches
partition or shorten the long context into smaller
sequences that fall within the typical 512 token
limit of BERT-style pretrained models. Such parti-
tioning could potentially result in loss of important
cross-partition information, and to mitigate this
problem, existing methods often rely on complex
architectures to address such interactions. On the
other hand, our proposed Longformer is able to
build contextual representations of the entire con-
text using multiple layers of attention, reducing the

need for task-speciﬁc architectures.

Model

attention char-LM other pretrain
matrix

tasks
no
no
no
no
no
no
MT
QA

yes
yes
yes
yes
yes
yes
yes
no
yes multiple

no
no
no
no
no
no
no
yes
yes

Recent work has addressed the computational in-
efﬁciency of Transformers on long sequences (see
Tab. 1). However, they primarily focus on autore-
gressive language modeling (LM), while the appli-
cation of long document transformers to document-
level NLP tasks in the transfer learning setting
(Dai and Le, 2015; Peters et al., 2018; Howard
and Ruder, 2018; Devlin et al., 2019) has remained
largely unexplored. We address this gap and show
that Longformer’s attention mechanism can act as
a drop-in replacement for the self-attention mecha-
nism in pretrained Transformers, and leads to gains
across a suite of document NLP tasks.

Longformer’s attention mechanism is a combina-
tion of a windowed local-context self-attention and
an end task motivated global attention that encodes
inductive bias about the task. Through ablations
and controlled trials we show both attention types
are essential – the local attention is primarily used
to build contextual representations, while the global
attention allows Longformer to build full sequence
representations for prediction.

We ﬁrst evaluate Longformer on autoregressive
character-level language modeling using a com-
bination of windowed and a new dilated attention
pattern, allowing the model to process sequences of
up to 32K characters on modern GPUs. We achieve
state-of-the-art results on text8 and enwik8
benchmark datasets, demonstrating the effective-
ness of Longformer in long document modeling.

Then, to evaluate Longformer’s ability to re-
place the full self-attention operation of existing
pretrained models, we pretrain it with the masked
language modeling (MLM) objective, continuing
from the RoBERTa (Liu et al., 2019) released
checkpoint. After pretraining, we apply it to
downstream language tasks through ﬁnetuning and
demonstrate that Longformer consistently outper-
forms RoBERTa on a wide range of document-level
natural language tasks including text classiﬁcation,
QA, and coreference resolution, achieving state-of-
the-art results on two of these datasets.

We ﬁnally introduce a variant of Longformer
which instead of an encoder-only Transformer
architecture,
it follows an encoder-decoder ar-
chitecture similar to the original Transformer
model (Vaswani et al., 2017), and it
is in-
tended for sequence-to-sequence (seq2seq) learn-
ing (Sutskever et al., 2014). We call this model
Longformer-Encoder-Decoder (LED) that uses

Transformer-XL (2019)
Adaptive Span (2019)
Compressive (2020)
Reformer (2020)
Sparse (2019)
Routing (2020)
BP-Transformer (2019)
Blockwise (2019)
Our Longformer

ltr
ltr
ltr

sparse
sparse
sparse
sparse
sparse
sparse

Table 1: Summary of prior work on adapting Trans-
formers for long documents. ltr: left-to-right.

Longformer’s efﬁcient attention pattern on the en-
coder network, allowing it to address long docu-
ment seq2seq tasks such as summarization. We
demonstrate the effectiveness of LED on the arXiv
summarization dataset (Cohan et al., 2018).

2 Related Work

Long-Document Transformers Tab. 1 summa-
rizes recent prior work on long documents. Two
types of self-attention approaches have been ex-
plored. The ﬁrst is a left-to-right (ltr) approach that
processes the document in chunks moving from
left-to-right. While such models have been success-
ful in autoregressive language modeling, they are
unsuitable for transfer learning approaches with
tasks that beneﬁt from bidirectional context.

Our work falls within the other general approach
that deﬁnes some form of sparse attention pattern
and avoids computing the full quadratic attention
matrix multiplication. The model with the most
similar attention pattern to ours is Sparse Trans-
former (Child et al., 2019), which uses a form of
dilated sliding window of blocks of size 8x8 pro-
vided by BlockSparse (Gray et al., 2017). Our
implementation (§3) also includes a custom CUDA
kernel, but it is more ﬂexible and maintainable than
BlockSparse which is implemented in C++, and
designed for a speciﬁc version of TensorFlow. We
also introduce additional task motivated global at-
tention patterns suitable for common NLP tasks
(§3) and show they are essential for good perfor-
mance in the transfer learning setting.

A few models tried tasks other than autoregres-
sive language modeling, which is a step forward
because arguably focusing on language modeling
as the primary evaluation has led to the develop-
ment of models with limited applicability. BP-
Transformer (Ye et al., 2019) evaluated on machine

2

(a) Full n2 attention

(b) Sliding window attention

(c) Dilated sliding window

(d) Global+sliding window

Figure 2: Comparing the full self-attention pattern and the conﬁguration of attention patterns in our Longformer.

translation (MT), but didn’t explore the pretrain-
ﬁnetune setting. Blockwise attention (Qiu et al.,
2019) pretrained their models and evaluated on
question answering (QA). However, the evaluation
is limited as it doesn’t include language modeling,
and the QA datasets are of relatively short docu-
ments,2 therefore the effectiveness of this model
on long document tasks remains unexplored.

Task-speciﬁc Models
for Long Documents
Many task-speciﬁc approaches have been devel-
oped to workaround the 512 limit of pretrained
transformer models like BERT. The simplest ap-
proach just truncates the document, commonly
used for classiﬁcation (Xie et al., 2019). An-
other approach chunks the document into chunks
of length 512 (could be overlapping), processes
each chunk separately, then combines the activa-
tions with a task speciﬁc model (Joshi et al., 2019).
A third approach popular for multihop and open
domain QA tasks uses a two-stage model where
the ﬁrst stage retrieves relevant documents that are
passed onto the second stage for answer extrac-
tion (Clark and Gardner, 2017; Chen et al., 2017).
All of these approaches suffer from information
loss due to truncation or cascading errors from
the two stage approach. In contrast, Longformer
can process long sequences without truncating or
chunking, allowing us to adopt a much simpler ap-
proach that concatenates the available context and
processes it in a single pass.

A few contemporaneous works3 have explored
similar ideas to Longformer using local + global
attention in Transformers, and pre-training it for
long document natural language tasks. In particu-
lar, ETC (Ainslie et al., 2020) uses a similar local
+ global attention instead of full self-attention to
scale Transformers to long documents. Different
from Longformer, ETC uses relative position em-

2SQuAD contexts typically ﬁt within the 512 limit, and
MRQA is constructed by dropping long-document examples.

3All were published on arXiv after Longformer.

beddings (which we only used for the Autoregres-
sive LM setting), introduces an additional training
objective (CPC loss) for pre-training, and conﬁg-
ures global attention in a slightly different way.
It shows strong results on several tasks including
reading comprehension and classiﬁcation. GMAT
(Gupta and Berant, 2020) uses a similar idea of
few global locations in the input serving as global
memory. BigBird (Zaheer et al., 2020) is an exten-
sion over ETC with evaluation on additional tasks,
including summarization. Importantly, through the-
oretical analysis, BigBird shows that sparse Trans-
formers are universal approximators of sequence
functions and preserve these properties of the full
self-attention.

3 Longformer
The original Transformer model has a self-attention
component with O(n2) time and memory complex-
ity where n is the input sequence length. To address
this challenge, we sparsify the full self-attention
matrix according to an “attention pattern” specify-
ing pairs of input locations attending to one another.
Unlike the full self-attention, our proposed atten-
tion pattern scales linearly with the input sequence,
making it efﬁcient for longer sequences. This sec-
tion discusses the design and implementation of
this attention pattern.

3.1 Attention Pattern
Sliding Window Given the importance of local
context (Kovaleva et al., 2019), our attention pat-
tern employs a ﬁxed-size window attention sur-
rounding each token. Using multiple stacked lay-
ers of such windowed attention results in a large
receptive ﬁeld, where top layers have access to all
input locations and have the capacity to build repre-
sentations that incorporate information across the
entire input, similar to CNNs (Wu et al., 2019).
Given a ﬁxed window size w, each token attends
to 1
2 w tokens on each side (Fig. 2b). The com-
putation complexity of this pattern is O(n × w),

3

which scales linearly with input sequence length n.
In a transformer with (cid:96) layers, the receptive ﬁeld
size at the top layer is (cid:96) × w (assuming w is ﬁxed
for all layers). Depending on the application, it
might be helpful to use different values of w for
each layer to balance between efﬁciency and model
representation capacity (§4.1).

Dilated Sliding Window To further increase the
receptive ﬁeld without increasing computation, the
sliding window can be “dilated”. This is analogous
to dilated CNNs (van den Oord et al., 2016) where
the window has gaps of size dilation d (Fig. 2c).
Assuming a ﬁxed d and w for all layers, the recep-
tive ﬁeld is (cid:96) × d × w, which can reach tens of
thousands of tokens even for small values of d.

In multi-headed attention, each attention head
computes a different attention score. We found set-
tings with different dilation conﬁgurations per head
improves performance by allowing some heads
without dilation to focus on local context, while
others with dilation focus on longer context.

Global Attention In state-of-the-art BERT-style
models for natural language tasks, the optimal in-
put representation differs from language modeling
and varies by task. For masked language modeling
(MLM), the model uses local context to predict the
masked word, while for classiﬁcation, the model ag-
gregates the representation of the whole sequence
into a special token ([CLS] in case of BERT). For
QA, the question and document are concatenated,
allowing the model to compare the question with
the document through self-attention.

In our case, the windowed and dilated attention
are not ﬂexible enough to learn task-speciﬁc repre-
sentations. Accordingly, we add “global attention”
on few pre-selected input locations. Importantly,
we make this attention operation symmetric: that
is, a token with a global attention attends to all
tokens across the sequence, and all tokens in the
sequence attend to it. Fig. 2d shows an example
of a sliding window attention with global attention
at a few tokens at custom locations. For example
for classiﬁcation, global attention is used for the
[CLS] token while in QA global attention is pro-
vided on all question tokens. Since the number of
such tokens is small relative to and independent of
n the complexity of the combined local and global
attention is still O(n). While specifying global
attention is task speciﬁc, it is a easy way to add in-
ductive bias to the model’s attention, and it is much

simpler than existing task speciﬁc approaches that
use complex architecture to combine information
across smaller input chunks.
Linear Projections for Global Attention Re-
call that given the linear projections Q, K, V , the
Transformer model (Vaswani et al., 2017) computes
attention scores as follows:

Attention(Q, K, V ) = softmax

V (1)

(cid:19)

(cid:18) QKT√

dk

Implementation

We use two sets of projections, Qs, Ks, Vs to com-
pute attention scores of sliding window attention,
and Qg, Kg, Vg to compute attention scores for the
global attention. The additional projections provide
ﬂexibility to model the different types of attention,
which we show is critical for best performance on
downstream tasks. Qg, Kg, Vg are all initialized
with values that match Qs, Ks, Vs.
3.2
In regular transformers, attention scores are com-
puted as in Eqn. 1. The expensive operation is
the matrix multiplication QKT because both Q
and K have n (sequence length) projections. For
Longformer, the dilated sliding window attention
computes only a ﬁxed number of the diagonals of
QKT . As shown in Fig. 1, this results in a linear
increase in memory usage compared to quadratic
increase for full self-attention. However, imple-
menting it requires a form of banded matrix mul-
tiplication that is not supported in existing deep
learning libraries like PyTorch/Tensorﬂow. Fig. 1
compares the performance of three different ways
of implementing it: loop is a memory efﬁcient Py-
Torch implementation that supports dilation but is
unusably slow and only used for testing; chunks
only supports the non-dilated case and is used for
the pretraining/ﬁnetuning setting; and cuda is our
fully functioning highly optimized custom CUDA
kernel implemented using TVM (Chen et al., 2018)
and used for the language modeling experiments
(see Appendix A for more details).

4 Autoregressive Language Modeling
Autoregressive or left-to-right language modeling
is loosely deﬁned as estimating the probability dis-
tribution of an existing token/character given its
previous tokens/characters in an input sequence.
This task is considered one of the fundamental tasks
in natural language and recent prior work on mod-
eling long sequences using transformers has relied

4

on this task as their primary evaluation (Dai et al.,
2019; Rae et al., 2020; Sukhbaatar et al., 2019).
Similarly, we develop and evaluate our model on
autoregressive language modeling.

4.1 Attention Pattern
For autoregressive language modeling we use
our dilated sliding window attention. Follow-
ing Sukhbaatar et al. (2019) we use differing win-
dow sizes across the layers. In particular, we use
small window sizes for the lower layers and in-
crease window sizes as we move to higher layers.
This allows the top layers to learn higher-level rep-
resentation of the entire sequence while having the
lower layers capture local information. In addition,
it provides balance between efﬁciency (smaller win-
dow sizes are less computationally expensive due
to fewer nonzero values) and performance (larger
window sizes have richer representation power and
often result in performance improvements).

We do not use dilated sliding windows for lower
layers to maximize their capacity to learn and uti-
lize the immediate local context. For the higher
layers, we use a small amount of increasing dila-
tion only on 2 heads. This gives the model the
ability to directly attend to distant tokens without
sacriﬁcing local context.

4.2 Experiment Setup
To compare to prior work we focus on character-
level LM (text8 and enwik8; Mahoney, 2009).

Training Ideally, we would like to train our
model on the largest window size and sequence
length we can ﬁt in a modern GPU memory. How-
ever, we found that the model needs a large number
of gradient updates to learn the local context ﬁrst,
before learning to utilize longer context. To accom-
modate this, we adopt a staged training procedure
where we increase the attention window size and
sequence length across multiple training phases. In
particular, in the ﬁrst phase we start with a short
sequence length and window size, then on each sub-
sequent phase, we double the window size and the
sequence length, and halve the learning rate. This
makes training fast, while keeping the slow part
(longest sequences and window sizes) to the end.
We train the model over 5 total phases with start-
ing sequence length of 2,048 and ending sequence
length of 23,040 on the last phase (see Appendix B
for detailed conﬁgurations of each phase, and for
all other hyperparameters).

Model
Dataset text8
T12 (Al-Rfou et al., 2018)
Adaptive (Sukhbaatar et al., 2019)
BP-Transformer (Ye et al., 2019)
Our Longformer
Dataset enwik8
T12 (Al-Rfou et al., 2018)
Transformer-XL (Dai et al., 2019)
Reformer (Kitaev et al., 2020)
Adaptive (Sukhbaatar et al., 2019)
BP-Transformer (Ye et al., 2019)
Our Longformer

#Param Dev

Test

44M
-
38M 1.05
39M
-
41M 1.04

44M
41M
-

-
-
-
39M 1.04
38M
-
41M 1.02

1.18
1.11
1.11
1.10

1.11
1.06
1.05
1.02
1.02
1.00

Table 2: Small model BPC on text8 & enwik8

Model
Transformer-XL (18 layers)
Sparse (Child et al., 2019)
Transformer-XL (24 layers)
Adaptive (Sukhbaatar et al., 2019)
Compressive (Rae et al., 2020)
Routing (Roy et al., 2020)
Our Longformer

#Param Test BPC
1.03
0.99
0.99
0.98
0.97
0.99
0.99

88M
≈100M
277M
209M
277M
≈223M
102M

Table 3: Performance of large models on enwik8

Evaluation We evaluate with sequences of
length 32,256. Following Dai et al. (2019), we
split the dataset into overlapping sequences of size
32,256 with a step of size 512, and report the per-
formance on the last 512 tokens on the sequence.

4.2.1 Results

Tab. 2 and 3 summarize evaluation results on
text8 and enwik8 datasets. We achieve a new
state-of-the-art on both text8 and enwik8 using
the small models with BPC of 1.10 and 1.00 on
text8 and enwik8 respectively, demonstrating
the effectiveness of our model.

For large models, given how expensive these
experiments are, and following recent work (Ki-
taev et al., 2020; Rae et al., 2020), we are only
evaluating on enwik8. Tab. 3 shows that Long-
former outperforms the comparable Transformer-
XL model, matches the performance of the compa-
rable Sparse Transformer (Child et al., 2019), and
matches or slightly underperforms recent models
that have more than twice the number of parameters.
It is worth noting that Adaptive Span (Sukhbaatar
et al., 2019) and Compressive Transformer (Rae
et al., 2020) are not good ﬁt for the pretraining-
ﬁnetuning paradigm as discussed in §2.

5

Model
Decreasing w (from 512 to 32)
Fixed w (= 230)
Increasing w (from 32 to 512)
No Dilation
Dilation on 2 heads

Dev BPC
1.24
1.23
1.21
1.21
1.20

Model
RoBERTa (seqlen: 512)
Longformer (seqlen: 4,096)

Longformer (train extra pos. embed. only)

+ copy position embeddings

+ 2K gradient updates
+ 65K gradient updates

base
1.846
10.299
1.957
1.753
1.705
1.850

large
1.496
8.738
1.597
1.414
1.358
1.504

Table 4: Top: changing window size across layers. Bot-
tom: with/without dilation (@ 150K steps on phase1)

Table 5: MLM BPC for RoBERTa and various pre-
trained Longformer conﬁgurations.

4.2.2 Ablation Study
To show the importance of the design choices of
our attention patterns, we tried different variants
and report their controlled experiment results. To
make the ablation study more manageable, we train
each conﬁguration for 150K steps4 with phase 1
conﬁguration on a small model on text8, then
report the BPC performance on the dev set.

The top of Tab. 4 demonstrates the impact of
different ways of conﬁguring the window sizes
per layer. We observe that increasing the window
size from the bottom to the top layer leads to the
best performance, arranging them in the reverse
way leads to worse performance, and using a ﬁxed
window size (the average of window sizes of the
other conﬁguration) leads to a performance that
it is in between. The bottom of Tab. 4 shows the
impact of adding dilation. Adding some dilation to
two heads leads to some improvement compared
with no dilation at all.

5 Pretraining and Finetuning

Current state-of-the-art systems for many NLP
tasks ﬁnetune a pretrained model with task super-
vision (e.g. BERT). One of our main motivations
is to develop such a model suitable for long docu-
ment tasks. To do so, we pretrained Longformer
on a document corpus and ﬁnetune it for six tasks,
including classiﬁcation, QA and coreference resolu-
tion. The resulting model can process sequences up
to 4,096 tokens long (8 times longer than BERT)5.
We pretrain Longformer with masked language
modeling (MLM), where the goal is to recover
randomly masked tokens in a sequence. Since
MLM pretraining is expensive, we continue pre-
training from the RoBERTa (Liu et al., 2019) re-
leased checkpoint, while only making the minimal

changes necessary to support Longformer’s atten-
tion mechanism. Note that our attention pattern can
be plugged into any pretrained transformer model
without the need to change the model architecture.

Attention Pattern We use sliding window atten-
tion with window size of 512, therefore using the
same amount of computation as RoBERTa.6

Position Embeddings RoBERTa uses learned
absolute position embeddings with the maximum
position being 512. To support longer documents,
we add extra position embeddings to support up to
position 4,096. To leverage RoBERTa’s pretrained
weights, instead of randomly initializing the new
position embeddings, we initialize them by copying
the 512 position embeddings from RoBERTa mul-
tiple times as analysis of BERT’s attention heads
shows a strong learned bias to attending to local
context, including the previous or next token (Clark
et al., 2019). Using the copy initialization preserves
this local structure everywhere except at the parti-
tion boundaries. Despite its simplicity, we found
this to be a very effective (see Tab. 5), allowing
Longformer pretraining to rapidly converge with a
small number of gradient updates.

Continued MLM Pretraining We pretrain
Longformer using fairseq (Ott et al., 2019) on a
corpus of long documents that we compiled (see
Appendix C for corpus details). We train two model
sizes, a base model and a large model. Both models
are trained for 65K gradient updates with sequences
length 4,096, batch size 64 (218 tokens), maximum
learning rate of 3e-5, linear warmup of 500 steps,
followed by a power 3 polynomial decay. The rest
of the hyperparameters are the same as RoBERTa.
Tab. 5 shows the BPC on the development set of
our training corpus. The ﬁrst row shows a 1.846

4One caveat is that the ordering of end performance will
not agree with that at step 150K. However, this approximation
saves the huge cost of running every experiment to completion.

5Sequences up to 16K are possible on current GPUs.

6Adding dilation on a few heads as in §4.1 hurt perfor-
mance, likely because it is not compatible with the pretrained
RoBERTa weights. Retraining such model from scratch might
be needed to improve performance.

6

Wordpieces WH TQA HQA
avg.
6,589 1,316
95th pctl.

ON IMDB
HY
1,535
506
300
705
705 1,975
3,627 17,126 1,889 1,147

Table 6: Average and 95th percentile of context length
of datasets in wordpieces. WH: WikiHop, TQA: Triv-
iaQA, HQA: HotpotQA, ON: OntoNotes, HY: Hyper-
partisan news

BPC using RoBERTa-base, which is comparable
to the 1.880 BPC reported on the RoBERTa paper
on their corpus. This indicates our training corpus
is from a distribution close to that used to train
RoBERTa. The following two rows show the per-
formance of Longformer before pretraining with
randomly initialized position embeddings and with
copied position embeddings. The signiﬁcant differ-
ence indicates the importance of the copy initial-
ization, and the relative small difference between
the RoBERTa BPC and the initialized BPC indi-
cates that our sliding window attention is working
well with the RoBERTa weights. The following
two rows show the impact of continuing pretrain-
ing. Traininig for 2K steps improves BPC from
1.957 to 1.753, which further decreases to 1.705 af-
ter 65K steps, demonstrating the model is learning
to better utilize the sliding window attention and
longer context. Similar patterns are observed with
RoBERTa-large and Longformer-large.

Frozen RoBERTa Weights We also pretrained
Longformer while freezing all RoBERTa weights,
and only training the new position embeddings.
The motivation for this conﬁguration is to perfectly
preserve the RoBERTa performance on short doc-
uments. This conﬁguration has a BPC of 1.850
(down from 1.957 at initialization), but higher than
1.705 where all the weights are trainable.

6 Tasks

We apply Longformer to multiple long document
tasks, including QA, coreference resolution and
classiﬁcation. Tab. 6 shows the evaluation datasets
have contexts signiﬁcantly longer than 512 word-
pieces. Our primary goal is to evaluate whether
our attention mechanism can act as a replace-
ment for the standard self-attention mechanism in
BERT style models, and to perform controlled tri-
als against a strong baseline. We are also interested
in evaluating whether we can replace complicated
task speciﬁc models necessitated by BERT’s lim-
ited context with simpler models that just concate-

nate all available context into a single sequence.

Our baseline is a RoBERTa based model that
breaks the context into the longest possible seg-
ment, passes each individually through RoBERTa,
and concatenates the activations for further process-
ing. For QA tasks, we also concatenate the question
to each segment so that RoBERTa can condition
it’s contextual representations of the context on
the question. The Longformer variant replaces the
RoBERTa self-attention mechanism with our win-
dowed attention used during pretraining, plus a task
motivated global attention. The global attention
uses additional linear projections (§3.1).

6.1 Question answering
We used three datasets: WikiHop (Welbl et al.,
2018), TriviaQA (Joshi et al., 2017, Wikipedia set-
ting), and HotpotQA, (Yang et al., 2018, distractor
setting).7

For WikiHop and TriviaQA we follow the sim-
ple QA model of BERT (Devlin et al., 2019), and
concatenate question and documents into one long
sequence, run it through Longformer, then have a
dataset-speciﬁc prediction layer. WikiHop uses a
classiﬁcation layer for the candidate while Trivi-
aQA uses the loss function of Clark and Gardner
(2017) to predict answer span. We include global
attention to question tokens and answer candidates
for WikiHop and to question tokens for TriviaQA.
HotpotQA is a multihop QA dataset that involves
extracting answer spans and evidence sentences
from 10 Wikipedia paragraphs, 2 of which are rele-
vant and the rest are distractors. We use a two-stage
model that ﬁrst selects the most relevant paragraphs
then passes them to a second stage for answer ex-
traction. Both stages concatenate question and con-
text into one sequence, run it through Longformer,
then use task-speciﬁc prediction layers. We train
the models in a multi-task way to predict relevant
paragraphs, evidence sentences, answer spans and
question types (yes/no/span) jointly. Note that this
model is simpler than recent SOTA models that in-
clude complex task-speciﬁc architectures (e.g., (Tu
et al., 2019; Chen et al., 2019; Tu et al., 2020;
Groeneveld et al., 2020)). See Appendix D for fur-
ther details about the models and hyperparameters.

6.2 Coreference Resolution
We use OntoNotes (Pradhan et al., 2012), and the
model from Joshi et al. (2019), a modiﬁcation of
7We use the full version of TriviaQA and HotpotQA, not

the simpliﬁed versions in MRQA (Fisch et al., 2019).

7

QA

Coref.

Classiﬁcation

Model
RoBERTa-base
Longformer-base

WikiHop
72.4
75.0

TriviaQA HotpotQA OntoNotes
78.4
78.6

74.3
75.2

63.5
64.4

IMDB Hyperpartisan
87.4
94.8

95.3
95.7

Table 7: Summary of ﬁnetuning results on QA, coreference resolution, and document classiﬁcation. Results are on
the development sets comparing our Longformer-base with RoBERTa-base. TriviaQA, Hyperpartisan metrics are
F1, WikiHop and IMDB use accuracy, HotpotQA is joint F1, OntoNotes is average F1.

the system from Lee et al. (2018) to replace ELMo
with BERT. The Longformer system is a straightfor-
ward adaption of the baseline model by replacing
RoBERTa with Longformer and extending the se-
quence length. We didn’t use global attention for
this task.

6.3 Document Classiﬁcation
We evaluate on IMDB (Maas et al., 2011) and Hy-
perpartisan news detection (Kiesel et al., 2019)
datasets.8 IMDB is a standard sentiment classiﬁca-
tion datasets consisting of movie reviews. While
most documents in this dataset are short, about
13.6% of them are larger than 512 wordpieces
(Tab. 6). Documents in Hyperpartisan are relatively
long, and it is small with only 645 documents mak-
ing it a good test for Longformer’s ability to adapt
to limited data. We use global attention on the
[CLS] token.

6.4 Results
Main Result Tab. 7 summarizes the results of all
our ﬁnetuning experiments. We observe that Long-
former consistently outperforms the RoBERTa
baseline. Its performance gain is especially ob-
vious for tasks that require long context such as
WikiHop and Hyperpartisan. For TriviaQA, the
improvement is more modest as the local context
is often sufﬁcient to answer the question. In the
case of HotpotQA, the supporting fact auxiliary
supervision allows models to easily ﬁnd relevant
contexts and then focus on local context, leading to
smaller gains. This is contrasted with WikiHop that
only includes distant supervision of intermediate
reasoning chains, where our approach excels by
reasoning over the entire context. On the IMDB
and OntoNotes datasets the performance gains are
smaller. For IMDB, the majority of the dataset
consists of short documents and thus it is expected
to see smaller improvements. For OntoNotes, we

8For Hyperpartisan we split the training data into 80/10/10

train/dev/test sets, and report mean F1 across ﬁve seeds.

Model
Current∗ SOTA
Longformer-large

WikiHop
78.3
81.9

TriviaQA HotpotQA
74.2
73.2

73.3
77.3

Table 8: Leaderboard results of Longformer-large at
time of submission (May 2020). All numbers are F1
scores.

found that the distance between any two mentions
is typically quite small so that a baseline that pro-
cesses smaller chunks separately is able to stitch
together mentions into coreference chains without
considering cross chunk interactions.

Longformer-large for QA We also evaluate the
performance of Longformer-large on long context
QA tasks. Tab. 8 shows that our Longformer-large
achieves new state-of-the-art results9 on WikiHop
and TriviaQA by large margins (3.6 and 4 points
respectively), and for HotpotQA, it underperforms
the current state-of-the-art (Fang et al., 2020) by
a point. Tab. 9 shows the detailed results of Hot-
potQA compared with published and unpublished
concurrent models. Longformer places second
on the published leaderboard, outperforming all
other published results except for HGN (Fang et al.,
2020). All published top performing models in
this task (Tu et al., 2019; Fang et al., 2020; Shao
et al., 2020) use GNNs (Kipf and Welling, 2017)
or graph network of entities, which seem to encode
an important inductive bias for the task and can po-
tentially improve our results further. Nevertheless,
Longformer performs strongly outperforming all
other methods including the recent non-GNN meth-
ods (Glaß et al., 2019; Shao et al., 2020; Groen-
eveld et al., 2020).

8

ans.
Model
79.8
TAP 2 (ensemble) (Glaß et al., 2019)
SAE (Tu et al., 2019)
79.6
Quark (dev) (Groeneveld et al., 2020) 81.2
81.2
C2F Reader (Shao et al., 2020)
Longformer-large
81.3
ETC-large† (Ainslie et al., 2020)
81.2
GSAN-large†
81.6
HGN-large (Fang et al., 2020)
82.2

supp.
joint
86.7 70.7
86.7 71.4
87.0 72.3
87.6 72.8
88.3 73.2
89.1 73.6
88.7 73.9
88.5 74.2

Table 9: HotpotQA results in distractor setting test set.
Quark’s test results are not available. All numbers are
F1 scores. † shows contemporaneous leaderboard sub-
missions.

Accuracy / ∆
Model
73.8
Longformer (seqlen: 4,096)
72.4 / -1.4
RoBERTa-base (seqlen: 512)
75.0 / +1.2
Longformer (seqlen: 4,096, 15 epochs)
71.7 / -2.1
Longformer (seqlen: 512, attention: n2)
73.1 / -0.7
Longformer (seqlen: 2,048)
73.2 / -0.6
Longformer (no MLM pretraining)
72.2 / -1.6
Longformer (no linear proj.)
Longformer (no linear proj. no global atten.)
65.5 / -8.3
Longformer (pretrain extra position embed. only) 73.5 / -0.3

Table 10: WikiHop development set ablations

6.5 Ablations on WikiHop

Tab. 10 presents an ablation study for WikiHop on
the development set. All results use Longformer-
base, ﬁne-tuned for ﬁve epochs with identical hy-
perparameters except where noted. Longformer
beneﬁts from longer sequences, global attention,
separate projection matrices for global attention,
MLM pretraining, and longer training. In addition,
when conﬁgured as in RoBERTa-base (seqlen: 512,
and n2 attention) Longformer performs slightly
worse then RoBERTa-base, conﬁrming that per-
formance gains are not due to additional pretrain-
ing. Performance drops slightly when using the
RoBERTa model pretrained when only unfreezing
the additional position embeddings, showing that
Longformer can learn to use long range context in
task speciﬁc ﬁne-tuning with large training datasets
such as WikiHop.

9At submission time, May 2020. Later, BigBird (Zaheer
et al., 2020) improved leaderboard results on these datasets.
There are confounding factors such as using 16X more com-
pute in BigBird’s pretraining compared with Longformer, po-
tentially affecting the performance.

7 Longformer-Encoder-Decoder (LED)

The original Transformer (Vaswani et al., 2017)
consisted of an encoder-decoder architecture, in-
tended for sequence-to-sequence tasks (Sutskever
et al., 2014), such as summarization and transla-
tion. While encoder-only Transformers are effec-
tive on a variety of NLP tasks, pre-trained encoder-
decoder Transformer models (e.g. BART (Lewis
et al., 2020) and T5 (Raffel et al., 2020)) have
achieved strong results on tasks like summariza-
tion. Yet, such models can’t efﬁciently scale to
seq2seq tasks with longer inputs.

To facilitate modeling long sequences for
seq2seq learning, we propose a Longformer variant
that has both the encoder and decoder Transformer
stacks but instead of the full self-attention in the
encoder, it uses the efﬁcient local+global attention
pattern of the Longformer. The decoder uses the
full self-attention to the entire encoded tokens and
to previously decoded locations. We call this model
Longformer-Encoder-Decoder (LED) which scales
linearly with the input. Since pre-training LED is
expensive, we initialize LED parameters from the
BART, and follow BART’s exact architecture in
terms of number of layers and hidden sizes. The
only difference is that to process longer inputs,
we extend position embedding to 16K tokens (up
from BART’s 1K tokens) and we initialize the new
position embedding matrix by repeatedly copying
BART’s 1K position embeddings 16 times as in
Section 5 for RoBERTa. Following BART, we re-
lease two model sizes, LED-base and LED-large,
which respectively have 6 and 12 layers in both
encoder and decoder stacks.

We evaluate LED on the summarization task us-
ing the arXiv summarization dataset (Cohan et al.,
2018) which focuses on long document summariza-
tion in the scientiﬁc domain. The 90th percentile
of document lengths is 14.5K tokens, making it
an appropriate testbed for evaluating LED. LED’s
encoder reads the document and its decoder gener-
ates the output summary. The encoder uses local
attention with window size 1,024 tokens and global
attention on the ﬁrst <s> token. The decoder uses
full attention to the entire encoder and previously
decoded locations. As standard in seq2seq models,
LED is trained using teacher forcing on gold train-
ing summaries and uses beam search at inference.
Tab. 11 demonstrates the results of LED-large
16K on the arXiv summarization task. This model
is merely initialized from BART, with no additional

9

Discourse-aware (2018)
Extr-Abst-TLM (2020)
Dancer (2020)
Pegasus (2020)
LED-large (seqlen: 4,096) (ours)
BigBird (seqlen: 4,096) (2020)
LED-large (seqlen: 16,384) (ours)

R-1
35.80
41.62
42.70
44.21
44.40
46.63
46.63

R-2
11.05
14.69
16.54
16.95
17.94
19.02
19.62

R-L
31.80
38.03
38.44
38.83
39.76
41.77
41.83

Table 11:
Summarization results of Longformer-
Encoder-Decoder (LED) on the arXiv dataset. Met-
rics from left to right are ROUGE-1, ROUGE-2 and
ROUGE-L.

and enwik8. When pretrained, Longformer con-
sistently outperforms RoBERTa on long document
tasks and sets new state-of-the-art results on Wik-
iHop and TriviaQA. We further present LED, an
encoder-decoder variant of Longformer for model-
ing sequence-to-sequence tasks, and achieve state-
of-the-art results on the arXiv long document sum-
marization task. For future work, we would like
to study other pretraining objectives, especially for
LED, increase the sequence length, and explore
other tasks that might beneﬁt from our model.

Acknowledgment

We would like to thank Noah Smith, Dan Weld,
Dirk Groeneveld, Kyle Lo, Daniel King and Doug
Downey for helpful discussions and feedback, and
the AI2 infrastructure team for technical support.

