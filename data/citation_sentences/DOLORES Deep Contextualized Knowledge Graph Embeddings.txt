Introduction  1 Knowledge graphs (Dong et al., 2014) enable structured access to world knowledge and form a key component of several applications like search engines, question answering systems and conversational assistants.
Most knowledge graph embedding methods can be classiﬁed into two major classes: one class which operates purely on triples like RESCAL (Nickel et al., 2011), TRANSE (Bordes et al., 2013), DISTMULT (Yang et al., 2015), TRANSD (Ji et al., 2015), COMPLEX (Trouillon et al., 2016), CONVE (Dettmers et al., 2018) and the second class which seeks to incorporate additional information (like multi-hops) (Wang et al., 2017).
Unlike most knowledge graph embeddings like TRANSD, TRANSE (Bordes et al., 2013; Ji et al., 2015) etc.
DOLORES is inspired by recent advances in learning word representations (word embeddings) from deep neural language models using Bi-Directional LSTMs (Peters et al., 2018).
In particular, we derive connections between the work of Peters et al. (2018) who learn deep contextualized word embeddings from sentences using a Bi-Directional LSTM based language model and random walks on knowledge graphs.
2 Related Work Extensive work exists on knowledge graph embeddings dating back to Nickel, Tresp, and Kriegel (2011) who ﬁrst proposed RESCAL based on a matrix factorization approach.
Bordes et al. (2013) advanced this line of work by proposing the ﬁrst translational model TRANSE which seeks to relate the head and tail entity embeddings by modeling the relation as a translational vector.
This culminated in a long series of new knowledge graph embeddings all based on the translational principle with various reﬁnements (Wang et al., 2014; Lin et al., 2015; Ji et al., 2015; Yang et al., 2015; Trouillon et al., 2016; Nickel and Kiela, 2017; Minervini et al., 2017; Xiao et al., 2017; Ma et al., 2017; Chen and Zaniolo, 2017; Chen et al., 2018).
Some recently proposed models like MANIFOLDE (Xiao et al., 2016) attempt to learn knowledge graph embeddings as a manifold while embeddings like HOLE (Nickel et al., 2011) derive inspiration from associative memories.
Furthermore, with the success of neural models, models based on convolutional neural networks have  been proposed like (Dettmers et al., 2018; Shi and Weninger, 2017) to learn knowledge graph embeddings.
Other models in this class of models include CONVKB (Nguyen et al., 2018b) and KBGAN (Cai and Wang, 2018).
Palumbo et al. (2018) use NODE2VEC to learn embeddings of entities and items in a knowledge graph.
Examples include Path Ranking Algorithm (PRA) (Lao et al., 2012), PTransE (Lin et al., 2015) and models based on recurrent neural networks (Neelakantan et al., 2015; Das et al., 2017).
Besides, Das et al. (2018) propose a reinforcement learning method that addresses practical task of answering questions where the relation is known, but only one entity.
Hartford et al. (2018) model interactions across two or more sets of objects using a parameter-sharing scheme.
While most of the above models except for the recurrent-neural net abased models above are shallow our model DOLORES differs from all of these works and especially that of Palumbo et al. (2018) in that we learn deep contextualized knowledge graph representations of entities and relations using a deep neural sequential model.
The work that is closest to our work is that of Das et al. (2017) who directly use an RNN-based architecture to model paths to predict missing links.
Moreover while Das et al. (2017) need to use paths generated from PRA that typically correlate with relations, our method has no such restriction and only uses paths generated by generic random walks greatly enhancing the scalability of our method.
In fact, we incorporate DOLORES embeddings to improve the performance of the model proposed by Das et al. (2017).
In contrast to previous knowledge graph embedding methods like (Bordes et al., 2013; Wang et al., 2014; Ji et al., 2015; Lin et al., 2015; Trouillon et al., 2016) which are based on shallow models and operates primarily on triples, our method DOLORES uses a deep neural model to learn “deep” and “contextualized” knowledge graph embeddings.
While, traditionally this has been modeled using count-based “ngram based” models (Manning and Sch¨utze, 1999; Jurafsky, 2000), recently deep neural models like LSTMs and RNN’s have been used to build such language models.
As noted by Peters et al. (2018), a forward language model implemented using an LSTM of “L” layers works as follows: At each position k, each LSTM layer outputs a context−−→ dependent representation denoted by hk,j corresponding to the jth layer of the LSTM.
We note that these context-dependent representations learned by the LSTM at each layer have been shown to be useful as “deep contextual” word representations in various predictive tasks in natural language processing (Peters et al., 2018).
We do this by generalizing this approach to graphs by noting connections ﬁrst noted by Perozzi, Al-Rfou, and Skiena (2014).
We adopt a component of NODE2VEC (Grover and Leskovec, 2016) to construct S. In particular, we perform a 2nd order random walk with two parameters p and q that determine the degree of breadth-ﬁrst sampling and depth-ﬁrst sampling.
Speciﬁcally as Grover and Leskovec (2016) described, p controls the likelihood of immediately revisiting a node in the walk whereas q controls whether the walk is biased towards nodes close to starting node or away from starting node.
Note that our learner (which is an L-layer Bi-Directional LSTM) computes a set of 2L+1 representations for each entity-relation pair which we denote by: −→ ht,i,  ←− ht,i | i = 1, 2,··· , L],  Rt = [xt,  TASK  PREVIOUS SOTA  DOLORES+ BASELINE  Link Prediction (head) Link Prediction (tail) Triple Classiﬁcation Missing Relation Type  (Nguyen et al., 2018b) (Nguyen et al., 2018b) (Nguyen et al., 2018b)  (Das et al., 2017)  35.5 44.3 88.20 71.74  37.5 48.7 88.40 74.42  INCREASE (ABSOLUTE/ RELATIVE) 2.0 / 3.1% 4.4 / 7.9% 0.20 / 1.7% 2.68 / 9.5%  Table 1: Summary of results of incorporating DOLORES embeddings on state-of-the-art models for various tasks.
4.2 Evaluation Tasks We consider three tasks, link prediction, triple classiﬁcation, and predicting missing relation types (Das et al., 2017):  • Link Prediction A common task for knowledge graph completion is link prediction, aiming to predict the missing entity when the other two parts of a triplet (h, r, t) are given.
In-line with prior work (Dettmers et al., 2018), we report results on link prediction in terms of Mean Reciprocal Rank (MRR), Mean Rank (MR) and Hits@10 on the FB15K-237 dataset in the ﬁltered setting on both sub-tasks: (a) head entity prediction and (b) tail entity prediction.
Triple classiﬁcation is a binary classiﬁcation task widely explored by previous work (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015).
We follow Das et al. (2017) and use the same dataset released by Neelakantan, Roth, and McCallum (2015) which is a subset of FREEBASE enriched with information from CLUEWEB.
Neelakantan et al. (2015) infer the relation type by examining the phrase between two entities.
We use the same evaluation criterion as used by Das et al. (2017) and report our results in Table 4.
Note that the current state-of-the-art model from Das et al. (2017) yields a score of 71.74.
MODEL MAP PRA (Lao et al., 2011) 64.43 PRA + Bigram (Neelakantan et.al 2015) 64.93 RNN-Path (Das et.al 2017) 68.43 RNN-Path-entity (Das et.al 2017) SOTA 71.74 RNN-Path-entity (+DOLORES) 74.42  Table 4: Results of missing relation type prediction.
RNNPath-entity (Das et al., 2017) is the state of the art which yields an improvement of 9.5% (71.74 vs 74.42) on mean average precision (MAP) when incorporated with DOLORES.
As was noted by research in computer vision, deep representations learned on one dataset are effective and very useful in transfer-learning tasks (Huang et al., 2017).
