For example, RESCAL (Nickel et al., 2011), one of the ﬁrst KGE models, showed strong performance when trained with state-of-the-art techniques; it was competitive to or outperformed more recent architectures.
They have been successfully applied to knowledge graph completion (Nickel et al., 2015) as well as in downstream tasks and applications such as recommender systems (Wang et al., 2017) or visual relationship detection (Baier et al., 2017).
A vast number of different KGE models for multi-relational link prediction have been proposed in the recent literature; e.g., RESCAL (Nickel et al., 2011), TransE (Bordes et al., 2013), DistMult, ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), TuckER (Balazevic et al., 2019), RotatE (Sun et al., 2019a), SACN (Shang et al., 2019), and many more.
{daniel,broscheit}@informatik.uni-mannheim.de †rgemulla@uni-mannheim.de  1  Published as a conference paper at ICLR 2020  Model  Training  Regularizer Optimizer Reciprocal  Loss RESCAL MSE TransE MR NegSamp Normalization DistMult MR  Publication Nickel et al. (2011) Bordes et al.
pairwise margin ranking or binary cross entropy), new forms of regularization (such as unweighted and weighted L2), or the use of reciprocal relations (Kazemi & Poole, 2018; Lacroix et al., 2018)— and ablation studies were not always performed.
Indeed, it has been observed that newer training strategies can considerably improve model performance (Kadlec et al., 2017; Lacroix et al., 2018; Salehi et al., 2018).
Our study complements and expands on the results of Kotnis & Nastase (2018) (focus on negative sampling) and Mohamed et al. (2019) (focus on loss functions) as well as similar studies in other areas, including language modeling (Melis et al., 2017), generative adversarial networks (Lucic et al., 2018), or sequence tagging (Reimers & Gurevych, 2017).
For example, RESCAL (Nickel et al., 2011), which constitutes one of the ﬁrst KGE models but is rarely considered in newer work, showed very strong performance in our study: it was competitive to or outperformed more recent architectures such as ConvE (Dettmers et al., 2018) and TuckER (Balazevic et al., 2019).
Our study focuses solely on pure KGE models, which do not exploit auxiliary information such as textual data or logical rules (Wang et al., 2017).
Reasonably recent survey articles about KGE models include Nickel et al. (2015) and Wang et al.
The goal of multi-relational link prediction is to “complete the KG”, i.e., to predict true but unobserved triples based on the information in K. Common approaches include rule-based methods (Galarraga et al., 2013; Meilicke et al., 2019), KGE methods (Nickel et al., 2011; Bordes et al., 2013; Trouillon et al., 2016; Dettmers et al., 2018), and hybrid methods (Guo et al., 2018).
The most popular models in this category are perhaps RESCAL (Nickel et al., 2011), TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), and ConvE (Dettmers et al., 2018).
Other recent examples for decomposable models include TuckER (Balazevic et al., 2019), RotatE (Sun et al., 2019a), and SACN (Shang et al., 2019).
First, training with negative sampling (NegSamp) (Bordes et al., 2013) obtains for each positive triple t = (i, k, j) from the training data a set of (pseudo-)negative triples obtained by randomly perturbing the subject, relation, or object position in t (and optionally verifying that the so-obtained triples do not exist in the KG).
An  f ((cid:80)  3  Published as a conference paper at ICLR 2020  alternative approach (Lacroix et al., 2018), which we term 1vsAll, is to omit sampling and take all triples that can be obtained by perturbing the subject and object positions as negative examples for t (even if these tuples exist in the KG).
Finally, Dettmers et al. (2018) proposed a training type that we term KvsAll2: this approach (i) constructs batches from non-empty rows (i, k,∗) or (∗, k, j) instead of from individual triples, and (ii) labels all such triples as either positive (occurs in training data) or negative (otherwise).
Trouillon et al. (2016) proposed to use binary cross entropy (BCE) loss: it applies a sigmoid to the score of each (positive or negative) triple and uses the cross entropy between the resulting probability and that triple’s label as loss.
Finally, Kadlec et al. (2017) used cross entropy (CE) between the model distribution (softmax distribution over scores) and the data distribution (labels of corresponding triples, normalized to sum to 1).
Mohamed et al. (2019) found that the choice of loss function can have a signiﬁcant impact on model performance, and that the best choice is data and model dependent.
Kazemi & Poole (2018) and Lacroix et al. (2018) introduced the technique of reciprocal relations into KGE training.
Both scoring functions share entity embeddings, but they do not share relation embeddings: each relation thus has two embeddings.3 The use of reciprocal relations may decrease the computational cost (as in the case of ConvE), and it may also lead to better model performance Lacroix et al. (2018) (e.g., for relations in which one direction is easier to predict).
Kazemi & Poole (2018) proposed to take the average of the two triple scores and explored the resulting models.
The most popular form of regularization in the literature is L2 regularization on the embedding vectors, either unweighted or weighted by the frequency of the corresponding entity or relation (Yang et al., 2015).
Lacroix et al. (2018) proposed to use L3 regularization.
ConvE used dropout (Srivastava et al., 2014) in its hidden layers (and only in those).
2Note that the KvsAll strategy is called 1-N scoring in Dettmers et al. (2018).
We used the FB15K-237 (Toutanova & Chen, 2015) (extracted from Freebase) and WNRR (Dettmers et al., 2018) (extracted from WordNet) datasets in our study.
We selected RESCAL (Nickel et al., 2011), TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016) and ConvE (Dettmers et al., 2018) for our study.
Quasi-random search methods aim to distribute hyperparameter settings evenly and try avoid “clumping” effects (Bergstra & Bengio, 2012).
For example, ComplEx was ﬁrst run on FB15K-237 by Dettmers et al. (2018), where it achieved a ﬁltered MRR of 24.7%.
In our  4https://github.com/uma-pi1/kge-iclr20  5  Published as a conference paper at ICLR 2020  t s r i F  s r u O  RESCAL (Wang et al., 2019) TransE (Nguyen et al., 2018) DistMult (Dettmers et al., 2018) ComplEx (Dettmers et al., 2018) ConvE (Dettmers et al., 2018) RESCAL TransE DistMult ComplEx ConvE  t TuckER (Balazevic et al., 2019)  RotatE (Sun et al., 2019a) SACN (Shang et al., 2019)  n e c e R  g r a L  e DistMult (Salehi et al., 2018)  ComplEx-N3 (Lacroix et al., 2018)  FB15K-237  MRR Hits@10 27.0 29.4 24.1 24.7 32.5 35.7 31.3 34.3 34.8 33.9 35.8 33.8 35.0 35.7 37.0  42.7 46.5 41.9 42.8 50.1 54.1 49.7 53.1 53.6 52.1 54.4 53.3 54.0 54.8 56.0  WNRR  MRR Hits@10 42.0 22.6 43.0 44.0 43.0 46.7 22.8 45.2 47.5 44.2 47.0 47.6 47.0 45.5 49.0  44.7 50.1 49.0 51.0 52.0 51.7 52.0 53.1 54.7 50.4 52.6 57.1 54.4 54.4 58.0  Table 2: Model performance in prior studies and our study (as percentages, on test data).
