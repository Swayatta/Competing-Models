We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientiﬁc data.
As  shown GPT  through ELMo (Radford et al.,  (Peters et al., 2018), and 2018) BERT (Devlin et al., 2019), unsupervised pretraining of language models on large corpora signiﬁcantly improves performance on many NLP tasks.
2 Methods  Background The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al., 2017).
Vocabulary BERT uses WordPiece (Wu et al., 2016) for unsupervised tokenization of the input text.
ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g.
The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 ﬁelds of study.
1.14M papers  of (Ammar et al., 2018).
We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientiﬁc text.
Dependency Parsing (DEP)  PICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011).
EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts.
SciERC (Luan et al., 2018) annotates entities and relations from computer science ab 1https://github.com/google/sentencepiece 2https://github.com/allenai/SciSpaCy  3.3 Pretrained BERT Variants  BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB.
All pretrained BERT models are converted to be compatible with PyTorch using the pytorchtransformers library.6 All our models (Sections 3.4 and 3.5) are implemented in PyTorch using AllenNLP (Gardner et al., 2017).
Casing We follow Devlin et al. (2019) in using the cased models for NER and the uncased models  3https://academic.microsoft.com/ 4https://github.com/google-research/bert 5BERT’s largest model was trained on 16 Cloud TPUs for 4 days.
Expected 40-70 days (Dettmers, 2019) on an 8-GPU machine.
3.4 Finetuning BERT  We mostly follow the same architecture, optimization, and hyperparameter choices used in Devlin et al. (2019).
For DEP, we use the model from Dozat and Manning (2017) with dependency tag and arc embeddings of size 100 and biafﬁne matrix attention over BERT vectors instead of stacked BiLSTMs.
In all settings, we apply a dropout of 0.1 and optimize cross entropy loss using Adam (Kingma and Ba, 2015).
We ﬁnetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangular schedule (Howard and Ruder, 2018) which is equivalent to the linear warmup followed by linear decay (Devlin et al., 2019).
3.5 Frozen BERT Embeddings  We also explore the usage of BERT as pretrained contextualized word embeddings, like ELMo (Peters et al., 2018), by training simple task-speciﬁc models atop frozen BERT embeddings.
For DEP, we use the full model from  Dozat and Manning (2017) with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks.
We did not ﬁnd changing the depth or size of the BiLSTMs to signiﬁcantly impact results (Reimers and Gurevych, 2017).
In addition, SCIBERT achieves new SOTA results on BC5CDR (Lee et al., 2019), and EBMand ChemProt NLP (Nye et al., 2018).
The SOTA model for JNLPBA is a BiLSTM-CRF ensemble trained on multiple NER datasets not just JNLPBA (Yoon et al., 2018).
The SOTA model for NCBI-disease is BIOBERT (Lee et al., 2019), which is BERTBase ﬁnetuned on 18B tokens from biomedical papers.
The SOTA result for GENIA is in Nguyen and Verspoor (2019) which uses the model from Dozat and Manning (2017) with partof-speech (POS) features, which we do not use.
In Table 2, we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in (Lee et al., 2019).
Field  Task  Dataset  SOTA  BERT-Base  SCIBERT  Frozen  Finetune  Frozen  Finetune  Bio  CS  NER  PICO  DEP  REL  NER REL CLS  BC5CDR (Li et al., 2016) JNLPBA (Collier and Kim, 2004) NCBI-disease (Dogan et al., 2014) EBM-NLP (Nye et al., 2018) GENIA (Kim et al., 2003) - LAS GENIA (Kim et al., 2003) - UAS ChemProt (Kringelum et al., 2016)  SciERC (Luan et al., 2018) SciERC (Luan et al., 2018) ACL-ARC (Jurgens et al., 2018)  Multi  CLS  Paper Field SciCite (Cohan et al., 2019)  Average  88.857 78.58 89.36 66.30 91.92 92.84 76.68  64.20  n/a 67.9  n/a 84.0  85.08 74.05 84.06 61.44 90.22 91.84 68.21  63.58 72.74 62.04  63.64 84.31  73.58  86.72 76.09 86.88 71.53 90.33 91.89 79.14  65.24 78.71 63.91  65.37 84.85  77.16  88.73 75.77 86.39 68.30 90.36 92.00 75.03  65.77 75.25 60.74  64.38 85.42  76.01  90.01 77.28 88.57 72.28 90.43 91.99 83.64  67.57 79.97 70.98  65.71 85.49  79.27  Table 1: Test performances of all BERT variants on all tasks and datasets.
In addition, SCIBERT achieves new SOTA results on ACLARC (Cohan et al., 2019), and the NER part of SciERC (Luan et al., 2018).
For relations in SciERC, our results are not comparable with those in Luan et al. (2018) because we are performing relation classiﬁcation given gold entities, while they perform joint entity and relation extraction.
In addition, SCIBERT outperforms the SOTA on Sci Cite (Cohan et al., 2019).
6 Related Work  Recent work on domain adaptation of BERT includes BIOBERT (Lee et al., 2019) and CLINICALBERT (Alsentzer et al., 2019; Huang et al., BIOBERT is trained on PubMed ab2019).
stracts and PMC full text articles, and CLINICALBERT is trained on clinical text from the MIMIC-III database (Johnson et al., 2016).
In contrast, SCIBERT is trained on the full text of 1.14M biomedical and computer science papers from the Semantic Scholar corpus (Ammar et al., 2018).
SCIBERT signiﬁcantly outperformed BERT-Base and achieves new SOTA results on several of these tasks, even compared to some reported BIOBERT (Lee et al., 2019) results on biomedical tasks.
