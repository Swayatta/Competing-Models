Keywords: Entity summarization · Feature extraction · Preference simulation · Attention Mechanism · Knowledge graphs  1  Introduction  Semantic data enables users or machines to comprehend and manipulate the conveyed information quickly [10].
2  Wei D, Liu Y, et al. as triples [4].
as triples [4].
With the growth of knowledge graphs, entity descriptions are becoming extremely lengthy [23].
Since Google ﬁrst released the knowledge graph, “get the best summary” for entities has been one of the main contributions in Google Search4 [25].
Speciﬁcally, Google Search returns a top-k subset of triples which can best describe the entity from a query on the right-hand side of the result pages [15].
In addition, entity summarization has been integrated into various applications such as document browsing, Question Answering (QA), etc [15].
Most previous entity summarization methods are adopted from random surfer [4], clustering [9, 10] and Latent Dirichlet Allocation (LDA) [19] models, depending too much on the hand-crafted templates for feature extraction as well as human expertise for feature selection.
Meanwhile, entities are capable to represent diverse information (or multi-aspect information) in knowledge graphs [21], resulting in diﬀerent user preference (sometimes multi-user preference [27]).
However, due to the countless quantities and unpredictable types of entities in real large-scale knowledge graphs, extracting discriminative features or selecting suitable models based on human expertise could be arduous [15].
To ﬁnd the most central triples, RELIN [4] and SUMMARUM [24] compute the relatedness and informativeness based on the features extracted from hand-crafted templates.
Meanwhile, FACES [9] and ES-LDA [19] introduce a clustering algorithm and LDA model for capturing multi-aspect information, respectively.
Recently, deep learning methods relieve the dependency on human expertise in Natural Language Processing (NLP) [17] community.
To generate the summaries without human expertise, an entity summarization method with a single-layer attention (ESA) [29] is proposed to calculate the attention score for each triple.
In ES-LDAext [20], Pouriyeh et al. stated the key point of learning word embeddings was the deﬁnition for “words”.
Automatic Feature Extraction In Named Entity Recognition (NER) task, the bidirectional LSTM (BiLSTM) has been widely used for automatic feature extraction [14].
For instance, in order to automatically extract features from a small and supervised training corpus, an LSTM-CRF model was proposed by Lample et al. [14], utilizing a BiLSTM for feature extraction and conditional random ﬁelds [13] for entity recognition.
The BiLSTM extracted representative and contextual features of a word, aligning with other words in the same sentence [8].
Since the single-layer attention mechanism in ESA [29] cannot capture multi-aspect information, we then design a multi-aspect attention mechanism with multiple (stacked) attention layers to overcome the drawback of ESA.
One seminal work using stacked attention layers is neural machine translation (NMT) [17], where the stacked attention layers (Transformer) [26] are utilized to capture the multi-aspect information from a sentence.
In each attention layer, a general attention function [17] is utilized to calculate the relevance between each triple and the information captured from the attention layer, termed attention scores.
Here, instead of combining all attention layers to generate overall attention scores of Transformer [26], we directly output the attention scores from each attention layer for multi-user preference simulation in user-phase attention.
User-Phase Attention When users browse triples, they will allocate high preference values (more attention) to triples which are more related with the information they are interested in [9].
In NER, a BiLSTM can maintain the independence and capture the intrinsic relationships among words [8].
7 http://ws.nju.edu.cn/summarization/esbm/  8  Wei D, Liu Y, et al. Baselines Our baselines consist of some existing state-of-the-art entity summarization methods, including RELIN [4], DIVERSUM [21], CD [30], FACES [9], LinkSUM [23], MPSUM [28] and ESA [29].
Baselines Our baselines consist of some existing state-of-the-art entity summarization methods, including RELIN [4], DIVERSUM [21], CD [30], FACES [9], LinkSUM [23], MPSUM [28] and ESA [29].
Sydow et al. [22] stated that entity summarization task could be treated as an extractive task of information retrieval (IR).
Meanwhile, given the limited number of entities in ESBM, we conduct 5-fold cross-validation to reduce the risk of overﬁtting without losing the number of learning instances [11].
Since ESA has signiﬁcantly better than all other state-of-the-art methods in our baselines, we then compare the statistical signiﬁcance among ESA and AutoSUMs (i.e., the original AutoSUM and the modiﬁed AutoSUM1∼5, respectively) utilizing Student’s paired t-test (p-value ≤ 0.05) [12].
