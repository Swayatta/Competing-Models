Segmentation can, for example, be used to convert unstructured medical dictations into clinical reports (Sadoughi et al., 2018), which in turn can help with medical coding (since a diagnosis mentioned in a "Medical History" might be different from a diagnosis mentioned in an "Intake" section (Ganesan and Subotin, 2014)).
Segmentation can also be used downstream for retrieval (Hearst and Plaunt, 2002; Edinger et al., 2017; Allan et al., 1998), where it can be particularly useful when applied to informal text or speech that lacks explicit segment markup.
Topically segmented documents are also useful for pre-reading (the process of skimming or surveying a text prior to careful reading), thus serving as an aid for reading comprehension (Swaffar et al., 1991; Ajideh, 2003).
Prior approaches to text segmentation can largely be split into two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al. ; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016).
; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016).
The beneﬁt of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difﬁcult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000).
Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models (Koshorek et al., 2018; Arnold et al.).
Hearst (1997) introduced the TextTiling algorithm, which uses term co-occurrences to ﬁnd coherent segments in a document.
Eisenstein and Barzilay (2008) introduced BayesSeg, a Bayesian method that can incorporate other features such as cue phrases.
Riedl and Biemann (2012) later introduced TopicTiling, which uses coherence shifts in topic vectors to ﬁnd segment bounds.
Nguyen et al. (2012) proposed SITS, a model for topic segmentation in dialogues that incorporates a per-speaker likelihood to change topics.
This was the approach taken in Koshorek et al. (2018), where the authors used Wikipedia as a source of training data to learn text segmentation as a supervised task.
Pomares-Quimbaya et al. (2019) provide a current overview of work on clinical segmentation.
Ganesan and Subotin (2014) trained a logistic regression model on a clinical segmentation task, though they did not consider the task of segment labeling.
Tepper et al. (2012) considered both tasks of segmentation and segment labeling, and proposed a two-step pipelined method that ﬁrst segments and then classiﬁes the segments.
Concurrent work considers the task of document outline generation (Zhang et al., 2019).
The work offers an alternative view of the joint segmentation and labeling problem, and is evaluated using exact match for segmentation and ROUGE (Lin, 2004) for heading generation if the segment is predicted correctly.
In contrast, we evaluate our models using a commonly-used probabilistic segmentation measure, Pk, which assigns partial credit to incorrect segmentations (Beeferman et al., 1999).
The problem of jointly learning to segment and classify is well-studied in NLP, though largely at a lower level, with Inside-OutsideBeginning (IOB) tagging (Ramshaw and Marcus, 1999).
for named entity recognition (NER, McCallum and Li, 2003).
The models that perform best at joint segmentation/classiﬁcation tasks like NER or phrase chunking were IOB tagging models, typically LSTMs with a CRF decoder (Lample et al., 2016) until BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018).
Tepper et al. (2012) proposed the use of IOB tagging to segment and label clinical documents, but argued for a pipelined approach.
S-LSTM is agnostic to the choice of sentence encoder, though in this work we use a concat pooled bi-directional  LSTM (Howard and Ruder, 2018).
This is similar to the approach taken by TextSeg in Koshorek et al. (2018), though we do not estimate a threshold, τ, and instead learn to to predict two classes: (B)eginning and (I)nside.
opposed to model predictions, was ﬁrst developed in Williams and Zipser (1989).
The ﬁrst task aligns closely with the clinical domain, in which headers are typically drawn from a ﬁxed label set (Tepper et al., 2012).
The second aligns more closely with learning to segment and label from naturally labeled data, such as contracts or Wikipedia articles, which can potentially then be transferred (Koshorek et al., 2018).
The Wiki-50 dataset was introduced as a test set in Koshorek et al. (2018), which also introduced the full Wiki-727k dataset.
The dataset contains 50 randomly sampled Wikipedia articles, segmented and with their headers, and was used to evaluate computationally expensive methods such as BAYESSEG (Eisenstein and Barzilay, 2008).
The Cities and Elements  datasets were introduced in Chen et al. (2009).
We use the Clinical Textbook dataset from Eisenstein and Barzilay (2008), which has segment boundaries but no headings.
Pk is a probabilistic measure (Beeferman et al., 1999) that works by running a sliding window of width k over the predicted and ground truth segments, and counting the number of times there is disagreement about the ends of the probe being in the same or different sections (see Figure 3).
4.4 Baselines We report C99 (Choi, 2000), TopicTiling (Riedl and Biemann, 2012), and TextSeg (Koshorek et al., 2018) as baselines on WikiSection segmentation.
For the additional datasets, we report GraphSeg (Glavaš et al.,  2016), BayesSeg (Eisenstein and Barzilay, 2008) and pretrained TextSeg and SECTOR models.
In addition, we implemented an LSTM-LSTMCRF IOB tagging model following Lample et al. (2016).
4.5 Model Setup For each task and dataset, we use the same set of hyperparameters: Adam optimizer (Kingma and Ba, 2015) with learning rate 0.001 and weight decay 0.9.
Dropout (Srivastava et al., 2014) is applied after each layer except the ﬁnal classiﬁcation layers; we use a single dropout probability of 0.1 for every instance.
Model weights are initialized using Xavier normal initialization (Glorot and Bengio, 2010).
We use ﬁxed 300-dimensional FastText embeddings (Bojanowski et al., 2017) for both English and German, and project them down to 200 dimensions using a trainable linear layer.
Baselines are TopicTiling (Riedl and Biemann, 2012), TextSeg (Koshorek et al., 2018), and C99 (Choi, 2000), and the best neural SECTOR models from Arnold et al..  WikiSection-headings multi-label classiﬁcation model conﬁguration C99 TopicTiling TextSeg SEC>H+emb S-LSTM S-LSTM, -expl S-LSTM, -expl, -pool  ↓ Pk 37.4 43.4 24.3 30.7 19.8 20.8 21.2  en_disease 179 topics ↑ Prec@1  n/a n/a n/a 50.5 53.5 52.1 52.3  ↑ MAP ↓ Pk 42.7 45.4 35.7 32.9 18.6 19.1 19.8  n/a n/a n/a 57.3 60.3 59 59.5  de_disease 115 topics ↑ Prec@1  n/a n/a n/a 26.6 36.2 34.7 34.4  ↑ MAP ↓ Pk 36.8 30.5 19.3 17.9 9.0 9.2 10.4  n/a n/a n/a 36.7 46.1 44.8 45  en_city 603 topics ↑ Prec@1  n/a n/a n/a 72.3 73 72.7 69.7  ↑ MAP ↓ Pk 38.3 41.3 27.5 19.3 8.2 8.5 10.2  n/a n/a n/a 71.1 71.3 70.8 67.2  de_city 318 topics ↑ Prec@1  ↑ MAP  n/a n/a n/a 68.4 74.1 73.8 64.1  n/a n/a n/a 70.2 75.1 74.4 66.7  Table 2: WikiSection headings task results, which predicts a multi-label bag-of-words drawn from section headers.
This follows from a long line of work in NLP that shows  that for tasks such as dependency parsing (Ballesteros et al., 2016), constituency parsing (Goodman, 1996), and machine translation (Och, 2003), all show improvements by optimizing on a loss that aligns with evaluation.
