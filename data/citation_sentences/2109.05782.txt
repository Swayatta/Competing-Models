Introduction  1 Task-oriented dialogue systems have been widely deployed to a variety of sectors (Yan et al., 2017; Chen et al., 2017; Zhang et al., 2020c; HosseiniAsl et al., 2020), ranging from shopping (Yan et al., 2017) to medical services (Arora et al., 2020a; Wei et al., 2018), to provide interactive experience.
To tackle few-shot intent detection, some recent attempts employ induction network (Geng et al., 2019), generation-based methods (Xia et al.,  ∗Equal contribution.
2020a,b), metric learning (Nguyen et al., 2020), or self-training (Dopierre et al., 2020).
Most recently, large-scale pre-trained language models such as BERT (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) have shown great promise in many natural language understanding tasks (Wang et al., 2019), and there has been a surge of interest in ﬁne-tuning the pre-trained language models for intent detection (Zhang et al., 2020a,b; Peng et al., 2020; Wu et al., 2020; Casanueva et al., 2020; Larson et al., 2019).
Therefore, recent efforts have been dedicated to adapting pre-trained language models to a  speciﬁc task such as intent detection by conducting continued pre-training (Gururangan et al., 2020; Gu et al., 2021) on a large unlabeled dialogue corpus with a specially designed optimization objective.
• CONVBERT (Mehri et al., 2020) ﬁnetunes BERT on an unlabeled dialogue corpus consisting of nearly 700 million conversations.
• TOD-BERT (Wu et al., 2020) further pretrains BERT on a task-oriented dialogue corpus of 100, 000 unlabeled samples with masked language modelling (MLM) and response contrastive objectives.
• USE-ConveRT (Henderson et al., 2020; Casanueva et al., 2020) investigates a dual encoder model trained with response selection tasks on 727 million input-response pairs.
• DNNC (Zhang et al., 2020a) pre-trains a language model with around 1 million annotated samples for natural language inference (NLI) and use the pre-trained model for intent detection.
• WikiHowRoBERTa (Zhang et al., 2020b) constructs some pre-training tasks based on the wikiHow database with 110, 000 articles.
While these methods have achieved impressive performance, they heavily rely on the existence of a large-scale corpus (Mehri et al., 2020) that is close in semantics to the target domain or consists of similar tasks for continued pre-training, which needs huge effort for data collection and comes at a high computational cost.
For example, the dataset OOS (Larson et al., 2019) provides labeled utterances across 10 different domains.
This joint-training scheme can learn better semantic representations and signiﬁcantly outperforms existing two-stage pre-training methods (Gururangan et al., 2020).
2 Methodology  We present a continued pre-training framework for intent classiﬁcation based on the pre-trained language model BERT (Devlin et al., 2019).
Such data samples can be readily obtained from public intent detection datasets such as OOS (Larson et al., 2019) and HWU64 (Liu et al., 2021).
To train our IntentBERT, we continue to pre-train BERT on either of the two datasets, OOS (Larson et al., 2019)1 and HWU64 (Liu et al., 2021), both of which contain multiple domains, providing rich resources to learn from2.
For evaluation, we employ three datasets: BANKING77 (Casanueva et al., 2020) is a ﬁne-grained intent detection dataset focusing on “Banking”; MCID (Arora et al., 2020a) is a dataset for “Covid19” chat bots; HINT3 (Arora et al., 2020b) contains 3 domains, “Mattress Products Retail”, “Fitness Supplements Retail” and “Online Gaming”.
BERT-Freeze simply freeze the off-the-shelf BERT; TOD-BERT (Wu et al., 2020) further pre-trains BERT on a huge  1The domains “Banking” and “Credit Cards” are excluded  because they are semantically close to the evaluation data.
amount of task-oriented conversations with MLM and response selection tasks; CONVBERT (Mehri et al., 2020) further pre-trains BERT on a large open-domain multi-turn dialogue corpus; USEConveRT (Henderson et al., 2020; Casanueva et al., 2020) is a fast embedding-based classiﬁer pre-trained on an open-domain dialogue corpus by dialogue response selection tasks; DNNC (Zhang et al., 2020a) further pre-trains a BERT-based model on NLI tasks and then applies a similarity-based classiﬁer for classiﬁcation; WikiHowRoBERTa (Zhang et al., 2020b) further pretrains RoBERTa (Liu et al., 2019) on fake intent detection data synthesized from wikiHow3.
We use BERTbase  5 (the base conﬁguration with d = 768) as the encoder, Adam (Kingma and Ba, 2015) as the optimizer, and PyTorch library for implementation.
Our joint pre-training scheme can also be applied to other language models such as GPT-2 (Radford et al., 2019) and ELMo (Peters et al., 2018), which is left as future work.
First, we investigate a two-stage pre-training scheme (Gururangan et al., 2020) where we use BERT or IntentBERT as initialization and perform MLM in the target domain (the top two rows in Table 3).
