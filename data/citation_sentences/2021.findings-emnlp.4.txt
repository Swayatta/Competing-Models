Introduction  1 Large transformer language models (Vaswani et al., 2017; Devlin et al., 2019) have shown impressive progress on tasks across different languages, including joint multilingual learning.
Many works have focused on cross-lingual transfer from highto low-resource languages in a zero- or few-shot setting (Hu et al., 2020).
However recent work has also highlighted that small amounts of data may be available for some low-resource languages, and even very few examples for ﬁne-tuning on a target language can be effective (Lauscher et al., 2020).
One transformer model that has shown particularly strong performance on multilingual tasks is XLM-Roberta (Conneau et al., 2020), a variant of the Roberta model (Liu et al., 2019) that adapts the multilingual training regime of XLM (Lample and Conneau, 2019) to a CommonCrawl corpus containing 100 languages.
Additionally, inspired by work in multilingual neural machine translation (NMT) (Tan et al., 2019), we investigate a method for grouping similar languages using an automated clustering method.
We provide a focused evaluation of this method on 15 languages from the WikiAnn corpus (Pan et al., 2017) following the train-test splits from Rahimi et al. (2019) and show that NER models trained on language clusters largely outperform (a) individual monolingual models trained for each language, (b) multilingual models trained on languages that are grouped by linguistic family, and (c) a single multilingual model trained on all available languages.
2 Related Work  Mueller et al. (2020) ﬁne-tune multilingual NER models monolingually on individual target languages, showing this technique to be effective in boosting F1 scores in all considered languages in their study.
In a similar vein, Lauscher et al. (2020) test the effectiveness of few-shot adaptation of multilingual models to new languages, ﬁnding that even including as few as 10 samples from the target language increases performance over zero-shot transfer.
Similar to our work, Chung et al. (2020) explore grouping languages by similarity, but focus on optimally constructing multilingual sub-word vocabularies, and show that these inputs perform bet FindingsoftheAssociationforComputationalLinguistics:EMNLP2021,pages40–45November7–11,2021.©2021AssociationforComputationalLinguistics40ter on tasks such as XNLI and WikiAnn NER.
In a more focused work, Arkhipov et al. (2019) investigate NER performance on four related Slavic languages, and demonstrate the advantages of pretraining multilingual BERT on the unsupervised language modeling task.
Finally, while not focusing on NER, Tan et al. (2019) show performance gains in multilingual NMT using clustering based on language tag embeddings.
To automatically group languages, we follow Tan et al. (2019) in choosing bottom-up agglomerative clustering, which assigns each data point its own cluster and iteratively merges clusters such that the sum of squared distances between points within all clusters is minimized.
We initialize all NER models from the pretrained XLM-R checkpoint available from the Huggingface Transformers library (Wolf et al., 2020) and train all models for 3 epochs, with a batch size of 20, and maximum input sequence length of 300 sub-tokens.
We evaluate with span-based F1 score as in the CoNLL-2003 evaluation script (Sang and Meulder, 2003), and report this metric for the three classes available in the dataset - location, organization, and person.
We also note drastic performance improvement for Swahili and Yoruba when trained in a single multilingual model compared to monolingual training, consistent with previous ﬁndings for lowresource languages in multilingual settings (Rahimi et al., 2019; Hu et al., 2020; Mueller et al., 2020; Conneau et al., 2020).
However, we observe best performance for these two languages when grouped using our proposed clustering method, which is somewhat surprising given the counterintuitive grouping with mostly European languages, though this grouping is also observed in previous work (Chung et al., 2020).
We ﬁnally note that, despite not being extensively tuned on this dataset, we achieve results within 3.5 F1 points of previously reported state of the art results on this test set (Yamada et al., 2020).
