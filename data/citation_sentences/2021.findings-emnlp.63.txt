Specialized language and task adapters have recently been proposed to facilitate cross-lingual transfer of multilingual pretrained models (Pfeiffer et al., 2020b).
Introduction  1 Massively multilingual pretrained models (Devlin et al., 2019; Huang et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) combined with cross-lingual transfer now deﬁne the state of the art on a variety of NLP tasks (Hu et al., 2020).
of pretrained models (Rebufﬁ et al., 2017; Houlsby et al., 2019).
The adapters can be applied to the cross-lingual transfer setting by training separate language and task adapters (Pfeiffer et al., 2020b; Üstün et al., 2020).
In fact, certain language varieties are not included in the standard language identiﬁcation tools, which makes it challenging to reliably obtain even unlabeled data (Salameh et al., 2018; Caswell et al., 2020; Demszky et al., 2021).
To give just one example, the Nordic languages and dialects form a dialect continuum where the total number of language varieties is difﬁcult to estimate, and language varieties constantly emerge in culturally and linguistically diverse areas (Svendsen and Røyneland, 2008; Røyneland and Jensen, 2020).
Although highly related, these language  FindingsoftheAssociationforComputationalLinguistics:EMNLP2021,pages730–737November7–11,2021.©2021AssociationforComputationalLinguistics730TaskLang 1Layer LLayer L+1TaskLayer LLayer L+1Weighted ensembleLanguage/task adapters EMEALang 1Lang 2Lang 3α1α2α3varieties have many systematic differences, which need to be addressed by NLP systems that equitably serve all speakers (Kumar et al., 2021).
2 Adapters for Cross-lingual Transfer  To facilitate our discussion, we brieﬂy summarize the MAD-X framework (Pfeiffer et al., 2020b) for zero-shot cross-lingual transfer and identify its shortcomings.
The resulting model, which we denote as Lj ◦ M, is trained on unlabeled data in Lj using an unsupervised objective such as masked language modeling (MLM; Devlin et al., 2019).
The online database AdapterHub1 aims to improve the efﬁciency and reuse of trained language and task adapters (Pfeiffer et al., 2020a) but currently supports only about 50 languages, and hence most languages are not covered.
Second, while the pretrained model M may be relatively robust against distribution shifts (Hendrycks et al., 2020), the specialized language adapters might make the model brittle to language variations because they are trained for speciﬁc languages.
w=1  that is,  The intuition behind our method is that a good adapter weight α for a test input x should make the model more conﬁdent in its prediction for x, it should lead to lower model entropy over the input (Shannon, 1948; Wang et al., 2021).
We use the WikiAnn dataset (Pan et al., 2017) for NER and Universial Treebank 2.0 for POS tagging (Nivre et al., 2018).
Model We use the mBERT (Devlin et al., 2019) model, which shows good performance for lowresource languages on the structured prediction tasks (Pfeiffer et al., 2020b; Hu et al., 2020).
Each group has a language with a pretrained adapter available on the AdapterHub (Pfeiffer et al., 2020a), and we test on the languages without adapters.
Baselines We compare with several baselines: 1) En: the English adapter; 2) Related: the best performing related language adapter; 3) Continual learning (CL): we use the English language adapter and update its parameters using the entropy loss for each test input; 4) Fusion: learn another set of key, value and query parameters in each layer that uses the layer output as a query to mix together the output of each adapter (Pfeiffer et al., 2021).
Finally, our method is inspired by the test time adaptation framework proposed for image classiﬁcation (Sun et al., 2020; Wang et al., 2021; Kedia and Chinthakindi, 2021).
5 Related Work  Our work is related to parameter efﬁcient ﬁnetuning of pretrained models (Bapna et al., 2019; Pfeiffer et al., 2020b; Li and Liang, 2021; Guo et al., 2021).
Speciﬁcally, (Üstün et al., 2020; Karimi Mahabadi et al., 2021) make adapters more generalizable by learning a parameter generator, while our work aims to utilize existing pretrained adapters without further training.
Pfeiffer et al. (2021) propose to learn extra parameters using la 734
