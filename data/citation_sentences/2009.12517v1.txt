A typical problem in KGs is the lack of many valid triples (West et al., 2014); therefore, research approaches have been proposed to predict whether a new triple missed in KGs is likely valid (Bordes et al., 2011; 2013; Socher et al., 2013).
The well-known embedding model TransE (Bordes et al., 2013) uses translations within a latent space to capture relationships between the head and tail entities, so that the embedding vh of the head entity plus the embedding vr of the relation is close to the embedding vt of the tail entity, i.e., vh + vr ≈ vt, where vh, vr, and vt ∈ Rn.
This view has formed the foundation for several early successful model such as TransH (Wang et al., 2014), TransR (Lin et al., 2015), TransD (Ji et al., 2015), STransE (Nguyen et al., 2016), DistMult (Yang et al., 2015), and up-to-date approaches, which has been reviewed in Nguyen (2017).
For example, ConvE (Dettmers et al., 2018) and ConvKB (Nguyen et al., 2018) are based on convolutional neural networks to score the triples for knowledge graph completion.
Moving beyond real-valued vector space, ComplEx (Trouillon et al., 2016) is an extension of DistMult (Yang et al., 2015) within the complex vector space to produce the score.
In addition, RotatE (Sun et al., 2019) considers each relation as a rotation-based translation from the head entity to the tail entity in the complex vector space as: vh ◦ vr ≈ vt, where vh, vr, vt ∈ Cn and ◦ denotes the element-wise product.
More recently, QuatE (Zhang et al., 2019) utilizes the Quaternion space H with Hamilton product In particular, a quaternion q ∈ H is a hyper-complex number to embed entities and relations.
Although QuatE is one of recent state-of-the-art models for the knowledge graph completion task which has shown to outperform up-to-date strong baselines (Zhang et al., 2019), directly using the quaternion embeddings vh, vr, vt to obtain the triple score might lead to the problem of struggling to strengthen the relation-aware correlations between the head and tail entities.
2 RELATED WORK  et al., 2015) employs a multiple-linear dot product to score the triples as: f (h, r, t) =(cid:80)n  Existing embedding models (Bordes et al., 2013; Wang et al., 2014) have been proposed to learn the vector representations of entities and relations for the knowledge graph completion task, where the goal is to score valid triples higher than invalid triples.
Early translation-based approaches exploit a translational characteristic so that the embedding of tail entity t should be close to the embedding of head entity h plus the embedding of relation t. For example, TransE (Bordes et al., 2013) deﬁnes a score function: f (h, r, t) = −(cid:107)vh + vr - vt(cid:107)p, where vh, vr, and vt ∈ Rn are vector embeddings of h, r and t respectively; and (cid:107)v(cid:107)p denotes the p-norm of vector v. As a result, TransE is suitable for 1-to-1 relationships, but not well-adapted for Many-to-1, 1-to-Many, and Many-to-Many relationships.
To this end, some translation-based methods have been proposed to deal with this issue such as TransH (Wang et al., 2014), TransR (Lin et al., 2015), TransD (Ji et al., 2015), and STransE (Nguyen et al., 2016).
One of the recent trends is to apply deep neural networks to measure the triples (Dettmers et al., 2018; Nguyen et al., 2018).
For example, ConvE (Dettmers et al., 2018) uses a convolution layer on a 2D input matrix of reshaping the embeddings of both the head entity and relation to produce feature maps that are then vectorized and computed with the embedding of the tail entity to return the score.
We can see an overview of other approaches, as summarized in (Nguyen, 2017).
Several works have moved beyond the real-valued vector space to the complex vector space, such as ComplEx (Trouillon et al., 2016) and RotatE (Sun et al., 2019).
Zhu et al. (2018) and Gaudet & Maida (2018) embed the greyscale and each of RGB channels of the image to the real and three separate imaginary axes of the Quaternion space and achieve better accuracies compared real-valued convolutional neural networks with same structures for image classiﬁcation tasks.
The Quaternion space has also been successfully applied to speech recognition (Parcollet et al., 2018; 2019), and natural language processing (Tay et al., 2019).
Regarding knowledge graph embeddings, Zhang et al. (2019) has recently proposed QuatE, which aims to learn entity and relation embeddings within the Quaternion space with the Hamilton product.
3 QUATRE: RELATION-AWARE QUATERNIONS FOR KNOWLEDGE GRAPH  EMBEDDINGS  3.1 QUATERNION BACKGROUND  For completeness, we brieﬂy provide a background in quaternion, which has also similarly described in recent works (Zhu et al., 2018; Parcollet et al., 2019; Zhang et al., 2019; Tay et al., 2019).
We employ the Adagrad optimizer (Duchi et al., 2011) to train our proposed QuatRE by minimizing the following loss function (Trouillon et al., 2016) with the regularization on model parameters θ as:  log(cid:0)1 + exp(cid:0)−t(h,r,t) · f (h, r, t)(cid:1)(cid:1) + λ(cid:107)θ(cid:107)2  2  (cid:88)  L =  (20)  (h,r,t)∈{G∪G(cid:48)}  in which, t(h,r,t) =  (cid:26) 1 for (h, r, t) ∈ G  −1 for (h, r, t) ∈ G(cid:48)  where we use l2-norm with the regularization rate λ; and G and G(cid:48) are collections of valid and invalid triples, respectively.
Here, G(cid:48) is generated by corrupting valid triples in G. We use a common strategy (Wang et al., 2014; Lin et al., 2015) when sampling invalid triples in G(cid:48).
For the fairness, similar to previous works, we apply the standard Glorot initialization (Glorot & Bengio, 2010) for parameter initialization in our QuatRE instead of utilizing a specialized initialization scheme used in QuatE (Zhang et al., 2019).
4 EXPERIMENTAL SETUP  In the knowledge graph completion task (Bordes et al., 2013), the goal is to predict a missing entity given a relation with another entity, for example, inferring a head entity h given (r, t) or inferring a tail entity t given (h, r).
4.1 DATASETS  We evaluate our proposed QuatRE on four benchmark datasets: WN18, FB15k (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova & Chen, 2015).
WN18 and FB15k are derived from the lexical KG WordNet (Miller, 1995) and the real-world KG Freebase (Bollacker et al., 2008) respectively.
As mentioned in (Toutanova & Chen, 2015), WN18 and FB15k contains many reversible relations, which makes the prediction task become trivial and irrealistic.
As shown in (Dettmers et al., 2018), recent state-of-the-art results on WN18 are still obtained by using a simple  1-1i-ijk-k-jhtrQuatE1-1i-ijk-k-jvr,1vr,2htrQuatREreversal.
4.2 EVALUATION PROTOCOL  Following Bordes et al. (2013), for each valid test triple (h, r, t), we replace either h or t by each of other entities to create a set of corrupted triples.
We use the “Filtered” setting protocol (Bordes et al., 2013), i.e., not including any corrupted triples that appear in the KG.
4.3 TRAINING PROTOCOL  We implement our QuatRE based on Pytorch (Paszke et al., 2019) and test on a single GPU.
Method  TransE (2013) DistMult (2015) ComplEx (2016) ConvE (2018) SimplE (2018) NKGE (2018) TorusE (2018) RotatE (2019) QuatE (2019) QuatRE RotatEAdv (2019) QuatEN 3Rec (2019) R-GCN+ (2018)  WN18  FB15k  –  MR MRR H@10 H@3 H@1 MR MRR H@10 H@3 H@1 – 29.7 655 – 374 – 336 – 184 162 116 309 – –  0.463 0.798 0.692 0.657 0.727 0.730 0.733 0.699 0.782 0.808 0.797 0.833 0.696  0.495 0.797 0.941 0.943 0.942 0.947 0.947 0.947 0.950 0.939 0.949 0.950 0.819  74.9 89.3 84.0 83.1 83.8 87.1 83.2 87.2 90.0 89.6 88.4 90.0 84.2  88.8  –  94.5 94.6 94.4 94.9 95.0 95.3 95.4 95.3 95.2 95.4 92.9  94.3 94.6 94.7 95.6 94.7 95.7 95.4 96.1 95.9 96.3 95.9 96.2 96.4  57.8  –  75.9 72.3 77.3 79.0 77.1 78.8 83.5 85.1 83.0 85.9 76.0  59.9 55.8 66.0 65.0 67.4 58.5 71.1 75.1 74.6 80.0 60.1  11.3  –  93.6 93.5 93.9 94.2 94.3 93.8 94.5 92.3 94.4 94.4 69.7  – 42 – 51 – 56 – 32 17 23 40 – –  We note that GC-OTE and RotatEAdv apply a self-adversarial negative sampling, which is different from the common sampling strategy.
QuatEN 3Rec uses the N3 regularization and reciprocal learning (Lacroix et al., 2018), which requires a large embedding dimension.
The results of TransE are taken from (Nguyen et al., 2018).
The results of DistMult and ComplEx are taken from (Dettmers et al., 2018).
The results of ConvKB are taken using the Pytorch implementation released by Nguyen et al. (2018).
Method  TransE (2013) DistMult (2015) ComplEx (2016) ConvE (2018) ConvKB (2018) NKGE (2018) RotatE (2019) InteractE (2020) AutoSF (2020) QuatE (2019) QuatRE GC-OTE (2020) ReInceptionE (2020) RotatEAdv (2019) QuatEN 3Rec (2019) R-GCN+ (2018)  0.226 0.430 0.440 0.460 0.220 0.450 0.470 0.463 0.490 0.488 0.493 0.491 0.483 0.476 0.482  50.1 49.0 51.0 48.0 50.8 52.6 56.5 52.8 56.7 58.2 59.2 58.3 58.2 57.1 57.2  –  –  –  2314 1986  –  1894 3340  – –  –  – –  50.8 51.9 51.1  –  49.2 49.9  –  –  42.1 42.2 43.0 45.1 43.8 43.9 44.2  –  42.8 43.6  –  WN18RR  FB15k-237  –  –  MR MRR H@10 H@3 H@1 MR MRR H@10 H@3 H@1 3384 5110 5261 5277 2741 4170 3277 5202  26.3 27.5 35.0  15.5 15.8 23.9  44.0 46.0 43.0  39.0 41.0 39.0  36.5 32.8  46.5 48.8  –  357 254 339 246 196 237 185 172 – 87 88 – 173 177 – –  0.294 0.241 0.247 0.316 0.302 0.330 0.297 0.354 0.360 0.348 0.367 0.361 0.349 0.338 0.366 0.249  46.5 41.9 42.8 49.1 48.3 51.0 48.0 53.5 55.2 55.0 56.3 55.0 52.8 53.3 55.6 41.7  –  –  – –  38.2 40.4 39.6  –  37.5 40.1 26.4  –  24.1 20.5 26.3 26.7 24.8 26.9 26.7  –  24.1 27.1 15.1  R-GCN+ integrate information about relation paths.
To qualitatively demonstrate the correlations between the entities, we use t-SNE (Maaten & Hinton, 2008) to visualize the learned quaternion embeddings of the entities on WN18RR for QuatE and QuatRE.
Following Bordes et al. (2013), for each relation r, we calculate the averaged number ηh of head entities per tail entity and the averaged number ηt of tail entities per head entity.
In International Conference on Learning Representations, 2019.
Association for Computational Linguistics, 2020.
In International Conference on Learning Representations, 2020.
In Proceedings of the International Conference on Learning Representations, 2015.
