Real-world KGs such as Yago (Suchanek et al., 2007) or Wordnet (Miller, 1995) are usually incomplete, so a common approach to predicting missing links in KGs is via embedding into vector spaces.
Recent research has proposed embedding hierarchical graphs into these spaces instead of conventional Euclidean space (Nickel and Kiela, 2017; Sala et al., 2018).
We evaluate the performance of our approach, ATTH, on the KG link prediction task using the standard WN18RR (Dettmers et al., 2018; Bordes et al., 2013), FB15k-237 (Toutanova and Chen, 2015) and YAGO3-10 (Mahdisoltani et al., 2013) benchmarks.
These include translation approaches (Bordes et al., 2013; Ji et al., 2015; Wang et al., 2014; Lin et al., 2015) or tensor factorization methods such as RESCAL (Nickel et al., 2011) or DistMult (Yang et al., 2015).
Complex embeddings Recently, there has been interest in learning embeddings in complex space, as in the ComplEx (Trouillon et al., 2016) and RotatE (Sun et al., 2019) models.
The recent QuatE model (Zhang et al., 2019) learns KG embeddings using quaternions.
For instance, R-GCN (Schlichtkrull et al., 2018) extends graph neural networks to the multirelational setting by adding a relation-speciﬁc aggregation step.
ConvE and ConvKB (Dettmers et al., 2018; Nguyen et al., 2018) leverage the expressiveness of convolutional neural networks to learn entity embeddings and relation embeddings.
More recently, the KBGAT (Nathani et al., 2019) and A2N (Bansal et al., 2019) models use graph attention networks for knowledge graph embeddings.
Instead, M¨obius addition ⊕c (Ganea et al., 2018) provides an analogue to Euclidean addition for hyperbolic space.
In fact, twodimensional hyperbolic space can represent any tree with arbitrarily small error (Sala et al., 2018).
Rotations Rotations have been successfully used to encode compositions in complex space with the RotatE model (Sun et al., 2019); we lift these to hyperbolic space.
We then compute a weighted average using the recently proposed tangent space average (Chami et al., 2019; Liu et al., 2019):  Att(xH , yH ; a) := expc  0(αxxE + αyyE).
The resulting scoring function is:  t )2 + bh + bt, (10)  s(h, r, t) = −dcr (Q(h, r), eH where (bv)v∈V are entity biases which act as margins in the scoring function (Tifrea et al., 2019; Balaˇzevi´c et al., 2019).
5.1 Experimental setup Datasets We evaluate our approach on the link prediction task using three standard competition benchmarks, namely WN18RR (Bordes et al., 2013; Dettmers et al., 2018), FB15k-237 (Bordes et al., 2013; Toutanova and Chen, 2015) and YAGO3-10 (Mahdisoltani et al., 2013).
For each KG, we follow the standard data augmentation protocol by adding inverse relations (Lacroix et al., 2018) to the datasets.
Additionally, we estimate the global graph curvature ξG (Gu et al., 2019) (see Appendix A.2 for more details),  Model RotatE MuRE ComplEx-N3  U Rd Cd Bd,1 MuRP REFE ROTE ATTE REFH ROTH ATTH  Bd,c  Rd  WN18RR  FB15k-237  YAGO3-10  MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 .387 .458 .420 .465 .455 .463 .456 .447 .472 .466  .491 .525 .460 .544 .521 .529 .526 .518 .553 .551  .417 .471 .420 .484 .470 .477 .471 .464 .490 .484  .330 .421 .390 .420 .419 .426 .419 .408 .428 .419  .316 .340 .322 .353 .330 .337 .339 .342 .346 .354  .290 .313 .294 .323 .302 .307 .311 .312 .314 .324  .208 .226 .211 .235 .216 .220 .223 .224 .223 .236   .317 .367 .247 .403 .417 .410 .415 .435 .437   .187 .259 .150 .289 .295 .290 .302 .307 .310  .458 .489 .463 .501 .474 .482 .488 .489 .497 .501   .478 .484 .392 .527 .548 .537 .530 559 .566   .283 .336 .230 .370 .381 .374 .381 .393 .397  Table 2: Link prediction results for low-dimensional embeddings (d = 32) in the ﬁltered setting.
Evaluation metrics At test time, we use the scoring function in Equation 10 to rank the correct tail or head entity against all possible entities, and use in use inverse relations for head prediction (Lacroix et al., 2018).
We follow the standard evaluation protocol in the ﬁltered setting (Bordes et al., 2013): all true triples in the KG are ﬁltered out during evaluation, since predicting a low rank for these triples should not be penalized.
Since optimization in hyperbolic space is practically challenging, we instead deﬁne all parameters in the tangent space at the origin, optimize embeddings using standard Euclidean techniques, and use the exponential map to recover the hyperbolic parameters (Chami et al., 2019).
Baselines We compare our method to SotA models, including MurP (Balazevic et al., 2019), MurE (which is the Euclidean analogue or MurP), RotatE (Sun et al., 2019), ComplEx-N3 (Lacroix et al., 2018) and TuckER (Balazevic et al., 2019).
In contrast, ﬁxed curvature degrades performance in high dimensions (Figure 5a), conﬁrming the importance of trainable curvatures and its impact on precision and capacity (previously studied by (Sala et al., 2018)).
DistMult, ConvE and ComplEx results are taken from (Dettmers et al., 2018).
