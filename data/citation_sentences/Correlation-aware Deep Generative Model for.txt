1  Introduction  Anomaly detection aims at identifying abnormal patterns that deviate signiﬁcantly from the normal behavior, which is ubiquitous in a multitude of application domains, such as cyber-security [15], medical care [19], and surveillance video proﬁling [14].
Formally, anomaly detection problem can be viewed as density estimation from the data distribution [23]: anomalies tend to reside in the  2  Fan et al. low probability density areas.
Reconstruction based methods, such as PCA [5] based approaches [18,10] and autoencoder based approaches [21,22,23,20], assume that outliers cannot be eﬀectively reconstructed from the compressed low-dimensional projections.
Clustering based methods [17,6] aim at density estimation of data points and usually adopt a two-step strategy [3] that performs dimensionality reduction ﬁrstly and then clustering.
To encode the correlation among the samples, a graph attention layer [16] is employed to adaptively aggregate the representation from neighbor nodes, by performing a shared attentional mechanism on the nodes:  wi,j = attn(Xi, Xj) = σ(aT · [WcXi||WcXj])  (3) where wi,j indicates the importance weight of node vi to node vj, attn(•) denotes the neural network parametrized by weights a ∈ RDc 2 ×F that shared by all nodes and Dc is the number of hidden neurons in attn(•), || denotes the concatenate operation.
Inspired by DAGMM [23], a sub-network consists of several fully connected layers is utilized, which takes the reconstruction error preserved low-dimentional embedding as input, to estimate the mixture membership for each sample.
Database # Dimensions # Instances Anomaly ratio  KDD99  Arrhythmia  Satellite  120 274 36  494,021  452 6,435  0.2 0.15 0.32  where the ﬁrst term is reconstruction error used for feature reconstruction, the second is sample energy, which aims to maximize the likelihood to observed samples, the third is covariance penalization, used for solving singularity problem as in GMM [23] by penalizing small values on the diagonal entries of covariance matrix, and the last is embedding penalization, which serves as a regularizer to impose the magnitude of normal samples as small as possible in the latent space, to deviate the normal samples from the abnormal ones.
4.2 Baseline Methods  – One Class Support Vector Machines (OC-SVM) [4] is a classic kernel method for anomaly detection, which learns a decision boundary between the inliers and outliers.
8  Fan et al. – Isolation Forests (IF) [8] conducts anomaly detection by building trees using randomly selected split values across sample features, and deﬁning the anomaly score as the average path length from a speciﬁc sample to the root.
– Isolation Forests (IF) [8] conducts anomaly detection by building trees using randomly selected split values across sample features, and deﬁning the anomaly score as the average path length from a speciﬁc sample to the root.
– Deep Structured Energy Based Models (DSEBM) [21] is a deep energy-based model, which aims to accumulate the energy across the layers.
DSEBM-r and DSEBM-e are utilized in [21] by taking the energy and reconstruction error as the anomaly score respectively.
– Deep Autoencoding Gaussian Mixture Model (DAGMM) [23] is an autoencoder based method for anomaly detection, which consists of a compression network for dimension reduction, and an estimate network to perform density estimation under the Gaussian Mixture Model.
– AnoGAN [13] is an anomaly detection algorithm based on GAN, which trains a DCGAN [12] to recover the representation of each data sample in the latent space during prediction.
– ALAD [20] is based on bi-directional GANs for anomaly detection by deriving adversarially learned features and uses reconstruction errors based on the learned features to determine if a data sample is anomalous.
Method  KDD99  Arrhythmia  Satellite  Precision Recall F1 Precision Recall F1 Precision Recall F1  OC-SVM [4]  IF [8]  DSEBM-r [21] DSEBM-e [21] DAGMM [23] AnoGAN [13]  52.42 60.81 67.84 67.79 80.77 71.19 ALAD [20] 79.41 CADGMM 96.01 97.53 96.71 56.41 57.89 57.14 81.99  40.82 45.81 54.69 53.03 15.13 15.10 45.65 46.01 50.78 49.83 43.75 42.42 53.13 51.52  85.23 79.54 93.73 92.94 64.72 73.28 64.66 73.99 94.42 93.69 82.97 88.65 95.77 95.01  74.57 92.16 85.21 86.19 92.97 87.86 94.27  53.97 51.47 15.15 46.67 49.09 41.18  50  59.99 61.07 94.89 75.40 68.61 68.22 68.56 68.18 81.6 81.19 72.03 71.59 80.32 79.85 82.75 82.37  5.1 Anomaly Detection  As in previous literatures [21,23,20], in this paper, Precision, Recall and F1 score are employed as the evaluation metrics.
Speciﬁcally, we take the low-dimensional embeddings of samples learned by DAGMM and CADGMM, as the inputs to the t-SNE tool [9].
