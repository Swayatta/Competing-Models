We ﬁnally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.1  1  Introduction  Transformers (Vaswani et al., 2017) have achieved state-of-the-art results in a wide range of natural language tasks including generative language modeling (Dai et al., 2019; Radford et al., 2019) and discriminative language understanding (Devlin et al., 2019).
However, they primarily focus on autoregressive language modeling (LM), while the application of long document transformers to documentlevel NLP tasks in the transfer learning setting (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) has remained largely unexplored.
Then, to evaluate Longformer’s ability to replace the full self-attention operation of existing pretrained models, we pretrain it with the masked language modeling (MLM) objective, continuing from the RoBERTa (Liu et al., 2019) released checkpoint.
We ﬁnally introduce a variant of Longformer which instead of an encoder-only Transformer architecture, it follows an encoder-decoder architecture similar to the original Transformer model (Vaswani et al., 2017), and it is intended for sequence-to-sequence (seq2seq) learning (Sutskever et al., 2014).
We call this model Longformer-Encoder-Decoder (LED) that uses  Transformer-XL (2019) Adaptive Span (2019) Compressive (2020) Reformer (2020) Sparse (2019) Routing (2020) BP-Transformer (2019) Blockwise (2019) Our Longformer  ltr ltr ltr  sparse sparse sparse sparse sparse sparse  Table 1: Summary of prior work on adapting Transformers for long documents.
We demonstrate the effectiveness of LED on the arXiv summarization dataset (Cohan et al., 2018).
The model with the most similar attention pattern to ours is Sparse Transformer (Child et al., 2019), which uses a form of dilated sliding window of blocks of size 8x8 provided by BlockSparse (Gray et al., 2017).
BPTransformer (Ye et al., 2019) evaluated on machine  2  (a) Full n2 attention  (b) Sliding window attention  (c) Dilated sliding window  (d) Global+sliding window  Figure 2: Comparing the full self-attention pattern and the conﬁguration of attention patterns in our Longformer.
Blockwise attention (Qiu et al., 2019) pretrained their models and evaluated on question answering (QA).
The simplest approach just truncates the document, commonly used for classiﬁcation (Xie et al., 2019).
Another approach chunks the document into chunks of length 512 (could be overlapping), processes each chunk separately, then combines the activations with a task speciﬁc model (Joshi et al., 2019).
A third approach popular for multihop and open domain QA tasks uses a two-stage model where the ﬁrst stage retrieves relevant documents that are passed onto the second stage for answer extraction (Clark and Gardner, 2017; Chen et al., 2017).
In particular, ETC (Ainslie et al., 2020) uses a similar local + global attention instead of full self-attention to scale Transformers to long documents.
GMAT (Gupta and Berant, 2020) uses a similar idea of few global locations in the input serving as global memory.
BigBird (Zaheer et al., 2020) is an extension over ETC with evaluation on additional tasks, including summarization.
3.1 Attention Pattern Sliding Window Given the importance of local context (Kovaleva et al., 2019), our attention pattern employs a ﬁxed-size window attention surrounding each token.
Using multiple stacked layers of such windowed attention results in a large receptive ﬁeld, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input, similar to CNNs (Wu et al., 2019).
This is analogous to dilated CNNs (van den Oord et al., 2016) where the window has gaps of size dilation d (Fig.
Linear Projections for Global Attention Recall that given the linear projections Q, K, V , the Transformer model (Vaswani et al., 2017) computes attention scores as follows:  Attention(Q, K, V ) = softmax  V (1)  (cid:19)  (cid:18) QKT√  dk  Implementation  We use two sets of projections, Qs, Ks, Vs to compute attention scores of sliding window attention, and Qg, Kg, Vg to compute attention scores for the global attention.
1 compares the performance of three different ways of implementing it: loop is a memory efﬁcient PyTorch implementation that supports dilation but is unusably slow and only used for testing; chunks only supports the non-dilated case and is used for the pretraining/ﬁnetuning setting; and cuda is our fully functioning highly optimized custom CUDA kernel implemented using TVM (Chen et al., 2018) and used for the language modeling experiments (see Appendix A for more details).
This task is considered one of the fundamental tasks in natural language and recent prior work on modeling long sequences using transformers has relied  4  on this task as their primary evaluation (Dai et al., 2019; Rae et al., 2020; Sukhbaatar et al., 2019).
Following Sukhbaatar et al. (2019) we use differing window sizes across the layers.
4.2 Experiment Setup To compare to prior work we focus on characterlevel LM (text8 and enwik8; Mahoney, 2009).
Model Dataset text8 T12 (Al-Rfou et al., 2018) Adaptive (Sukhbaatar et al., 2019) BP-Transformer (Ye et al., 2019) Our Longformer Dataset enwik8 T12 (Al-Rfou et al., 2018) Transformer-XL (Dai et al., 2019) Reformer (Kitaev et al., 2020) Adaptive (Sukhbaatar et al., 2019) BP-Transformer (Ye et al., 2019) Our Longformer  #Param Dev  Test  44M 38M 1.05 39M 41M 1.04  44M 41M  39M 1.04 38M 41M 1.02  1.18 1.11 1.11 1.10  1.11 1.06 1.05 1.02 1.02 1.00  Table 2: Small model BPC on text8 & enwik8  Model Transformer-XL (18 layers) Sparse (Child et al., 2019) Transformer-XL (24 layers) Adaptive (Sukhbaatar et al., 2019) Compressive (Rae et al., 2020) Routing (Roy et al., 2020) Our Longformer  #Param Test BPC 1.03 0.99 0.99 0.98 0.97 0.99 0.99  88M ≈100M 277M 209M 277M ≈223M 102M  Table 3: Performance of large models on enwik8  Evaluation We evaluate with sequences of length 32,256.
Following Dai et al. (2019), we split the dataset into overlapping sequences of size 32,256 with a step of size 512, and report the performance on the last 512 tokens on the sequence.
For large models, given how expensive these experiments are, and following recent work (Kitaev et al., 2020; Rae et al., 2020), we are only evaluating on enwik8.
3 shows that Longformer outperforms the comparable TransformerXL model, matches the performance of the comparable Sparse Transformer (Child et al., 2019), and matches or slightly underperforms recent models that have more than twice the number of parameters.
It is worth noting that Adaptive Span (Sukhbaatar et al., 2019) and Compressive Transformer (Rae et al., 2020) are not good ﬁt for the pretrainingﬁnetuning paradigm as discussed in §2.
Since MLM pretraining is expensive, we continue pretraining from the RoBERTa (Liu et al., 2019) released checkpoint, while only making the minimal  changes necessary to support Longformer’s attention mechanism.
To leverage RoBERTa’s pretrained weights, instead of randomly initializing the new position embeddings, we initialize them by copying the 512 position embeddings from RoBERTa multiple times as analysis of BERT’s attention heads shows a strong learned bias to attending to local context, including the previous or next token (Clark et al., 2019).
Continued MLM Pretraining We pretrain Longformer using fairseq (Ott et al., 2019) on a corpus of long documents that we compiled (see Appendix C for corpus details).
6.1 Question answering We used three datasets: WikiHop (Welbl et al., 2018), TriviaQA (Joshi et al., 2017, Wikipedia setting), and HotpotQA, (Yang et al., 2018, distractor setting).7  For WikiHop and TriviaQA we follow the simple QA model of BERT (Devlin et al., 2019), and concatenate question and documents into one long sequence, run it through Longformer, then have a dataset-speciﬁc prediction layer.
WikiHop uses a classiﬁcation layer for the candidate while TriviaQA uses the loss function of Clark and Gardner (2017) to predict answer span.
Note that this model is simpler than recent SOTA models that include complex task-speciﬁc architectures (e.g., (Tu et al., 2019; Chen et al., 2019; Tu et al., 2020; Groeneveld et al., 2020)).
6.2 Coreference Resolution We use OntoNotes (Pradhan et al., 2012), and the model from Joshi et al. (2019), a modiﬁcation of 7We use the full version of TriviaQA and HotpotQA, not  the simpliﬁed versions in MRQA (Fisch et al., 2019).
the system from Lee et al. (2018) to replace ELMo with BERT.
6.3 Document Classiﬁcation We evaluate on IMDB (Maas et al., 2011) and Hyperpartisan news detection (Kiesel et al., 2019) datasets.8 IMDB is a standard sentiment classiﬁcation datasets consisting of movie reviews.
8 shows that our Longformer-large achieves new state-of-the-art results9 on WikiHop and TriviaQA by large margins (3.6 and 4 points respectively), and for HotpotQA, it underperforms the current state-of-the-art (Fang et al., 2020) by a point.
Longformer places second on the published leaderboard, outperforming all other published results except for HGN (Fang et al., 2020).
All published top performing models in this task (Tu et al., 2019; Fang et al., 2020; Shao et al., 2020) use GNNs (Kipf and Welling, 2017) or graph network of entities, which seem to encode an important inductive bias for the task and can potentially improve our results further.
Nevertheless, Longformer performs strongly outperforming all other methods including the recent non-GNN methods (Glaß et al., 2019; Shao et al., 2020; Groeneveld et al., 2020).
Model 79.8 TAP 2 (ensemble) (Glaß et al., 2019) SAE (Tu et al., 2019) 79.6 Quark (dev) (Groeneveld et al., 2020) 81.2 81.2 C2F Reader (Shao et al., 2020) Longformer-large 81.3 ETC-large† (Ainslie et al., 2020) 81.2 GSAN-large† 81.6 HGN-large (Fang et al., 2020) 82.2  supp.
Later, BigBird (Zaheer et al., 2020) improved leaderboard results on these datasets.
7 Longformer-Encoder-Decoder (LED)  The original Transformer (Vaswani et al., 2017) consisted of an encoder-decoder architecture, intended for sequence-to-sequence tasks (Sutskever et al., 2014), such as summarization and translation.
BART (Lewis et al., 2020) and T5 (Raffel et al., 2020)) have achieved strong results on tasks like summarization.
We evaluate LED on the summarization task using the arXiv summarization dataset (Cohan et al., 2018) which focuses on long document summarization in the scientiﬁc domain.
This model is merely initialized from BART, with no additional  9  Discourse-aware (2018) Extr-Abst-TLM (2020) Dancer (2020) Pegasus (2020) LED-large (seqlen: 4,096) (ours) BigBird (seqlen: 4,096) (2020) LED-large (seqlen: 16,384) (ours)  R-1 35.80 41.62 42.70 44.21 44.40 46.63 46.63  R-2 11.05 14.69 16.54 16.95 17.94 19.02 19.62  R-L 31.80 38.03 38.44 38.83 39.76 41.77 41.83  Table 11: Summarization results of LongformerEncoder-Decoder (LED) on the arXiv dataset.
