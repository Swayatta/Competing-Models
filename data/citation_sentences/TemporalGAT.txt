This case applies to knowledge bases [35], 3D models [18], social media [22], and biological networks [7] which are usually represented by graphs.
413–423, 2020. https://doi.org/10.1007/978-3-030-47426-3_32  414  A. Fathy and K. Li  to apply machine learning in graph analysis and data mining tasks easily and eﬃciently such as node classiﬁcation [11,22], link prediction [7], clustering [4], and visualization [30].
However, a wide range of real-world applications are intrinsically dynamic and the underlying graph structure evolves over time and are usually represented as a sequence of graph snapshots over time [14].
However, these methods generally fail to learn eﬀective representations when graph nodes exhibit substantially distinct evolutionary behaviors over time [24].
Trivedi et al. [27] handle temporal reasoning problem in multi-relational knowledge graphs through employing a recurrent neural network.
Recently, the authors in [24] propose dynamic graph embedding approach that leverage self-attention networks to learn node representations.
Veliˇckovi´c et al. [29] extend the self-attention mechanism and apply it on static graphs by enabling each node to attend over its neighbors.
In this paper, we speciﬁcally focus on applying graph attention networks (GATs) [29] because of its eﬀectiveness in addressing the shortcomings of prior methods based on graph  TemporalGAT: Attention-Based Dynamic Graph Representation Learning  415  convolutions such as [8,11].
When using a large dilation factors, the output at the highest level can represent a wider range of inputs, thus eﬀectively expanding the receptive ﬁeld [32] of convolution networks.
The proposed GAT layer is a variant of GAT [29], with dilated convolutions applied on each graph snapshot:  (cid:3)  (cid:2)  (cid:4)  hu = σ  αvuWdxv  v∈Nu  where hu is the learned hidden representations of node u, σ is a non-linear activation function, Nu represents the immediate neighbors of u, Wd is the shared transformation weight of dilated convolutions, xv is the input representation vector of node v, and αvu is the coeﬃcient learned by the attention mechanism deﬁned as:  (cid:5)  αvu =  (cid:7)  (cid:5)  (cid:6)(cid:6) Avu · aT [Wdxv(cid:5)Wdxu]  exp w∈Nu exp (σ (Awu · aT [Wdxw(cid:5)Wdxu]))  σ  (2)  (3)  TemporalGAT: Attention-Based Dynamic Graph Representation Learning  417  where Avu is the edge weight of the adjacency matrix between u and v, aT is a weight vector parameter of the attention function implemented as feed-forward layer and (cid:5) is the concatenation operator.
Following, we adopt binary cross-entropy loss function to predict the existence of an edge between a pair of nodes using the learned node representations similar to [24].
We follow the experiment design by [24] and classify each node pair into linked and non-linked nodes, and use sampling approach to achieve positive and negative node pairs where we randomly sample 25% of each snapshot nodes for training and use the remaining 75% for testing.
4.4 Baseline Algorithms  We evaluate our method against the several baseline algorithms including static graph representation approaches such as: GAT [29], Node2Vec [7], GraphSAGE [8], graph autoencoders [9], GCN-AE and GAT-AE as autoencoders for link prediction [38].
Dynamic graph representation learning including Know-Evolve [27], DynamicTriad [36], DynGEM [10] and DySAT [24].
To evaluate the link prediction performance of each baseline model, we train a logistic regression classiﬁer similar to [36].
Algorithm  Enron  UCI  Yelp  Macro  Node2Vec  Macro 83.1 ± 1.2 80.0 ± 0.4 80.5 ± 0.6  Macro  Micro  Micro Micro 83.7 ± 0.7 67.9 ± 0.2 65.34 ± 0.2 82.5 ± 0.6 81.9 ± 0.5 79.2 ± 0.4 82.9 ± 0.2 61.0 ± 0.1 58.56 ± 0.2 66.2 ± 0.1 65.1 ± 0.2 73.3 ± 0.6 74.0 ± 0.4 66.7 ± 0.2 65.8 ± 0.2 81.7 ± 1.5 80.5 ± 0.3 65.9 ± 0.1 65.4 ± 0.1 76.0 ± 1.4 80.0 ± 0.2 63.5 ± 0.3 62.7 ± 0.3 79.0 ± 0.9 77.6 ± 0.6 56.9 ± 0.2 59.7 ± 0.2 62.3 ± 1.5 71.2 ± 0.5 69.7 ± 1.3 77.5 ± 0.3 66.0 ± 0.2 66.0 ± 0.2 86.6 ± 0.2 81.0 ± 0.2 85.8 ± 0.1 70.2 ± 0.1 69.9 ± 0.1 71.9 ± 0.3 70.3 ± 0.2  G-SAGE G-SAGE + GAT 72.5 ± 0.4 81.6 ± 1.5 GCN-AE 75.7 ± 1.1 80.3 ± 0.8 61.6 ± 1.1 67.8 ± 0.6 85.7 ± 0.3 86.4 ± 0.4 86.8 ± 0.3 82.7 ± 0.2 85.2 ± 0.2  79.8 ± 0.2 83.5 ± 0.5 81.9 ± 0.3 80.3 ± 0.5 80.9 ± 0.2 79.8 ± 0.5  TemporalGAT  DynamicTriad  Know-Evolve  GAT-AE  DynGEM  DySAT  pair of nodes as suggested by [7].
We report the averaged micro and macro AUC scores over all time steps for the methods in Table 2 (given in paper [24]).
Tang et al. [25] designed two loss functions to capture the local and global graph structure.
DANE [16] is based on this idea to update the eigenvectors of graph Laplacian matrix over time series.
In [33], the authors learn representations through observing the graph changes and incrementally re-sample a few walks in the successive time step.
Recent works learn incremental node representations across time steps [10], where the authors apply an autoencoder approach that minimizes the reconstruction loss with a distance metric between connected nodes in the embedding space.
More recently, [24] proposed an approach that leverage the most relevant historical contexts through self-attention layers to preserve graph structure and temporal evolution patterns.
