Owing to recent advances in machine learning, authorship attribution methods can now identify authors with impressive accuracy (Abbasi and Chen, 2008) even in challenging settings such as cross-domain (Overdorf and Greenstadt, 2016) and at a large-scale (Narayanan et al., 2012; Ruder et al., 2016).
Such powerful authorship attribution methods pose a threat to privacyconscious users such as journalists and activists who may wish to publish anonymously (Times, 2018; Anonymous, 2018).
Since it  is challenging to accomplish this manually, researchers have developed automated authorship obfuscation methods that can evade attribution while preserving semantics (PAN, 2018).
However, a key limitation of prior work is that authorship obfuscation methods do not consider the adversarial threat model where the adversary is “obfuscation aware” (Karadzhov et al., 2017; Potthast et al., 2018; Mahmood et al., 2019).
Early work by Brennan et al. (2012) instructed users to manually obfuscate text such as by imitating the writing style of someone else.
Anonymouth (McDonald et al., 2012, 2013) was proposed to automatically identify the words and phrases that were most revealing of an author’s identity so that these could be manually obfuscated by users.
Follow up research leveraged automated machine translation to suggest alternative sentences that can be further tweaked by users (Almishari et al., 2014; Keswani et al., 2016).
Moving towards full automation, the digital text forensics community (Potthast and Hagen, 2018) has developed rule-based authorship obfuscators (Mansoorizadeh et al., 2016; Karadzhov et al., 2017; Castro-Castro et al., 2017).
For example, Karadzhov et al. (2017) presented a rule-based obfuscation approach to adapt the style of a text towards the “average style” of the text corpus.
Castro et al. (2017) presented another rule-based obfuscation approach to “simplify” the style of a text.
For example, Mahmood et al. (2019) proposed a genetic algorithm approach to “search” for words that when changed, using a sentimentpreserving word embedding, would have the maximum adverse effect on authorship attribution.
Bevendorff et al. (2019) proposed a heuristicbased search algorithm to ﬁnd words that when changed using operators such as synonyms or hypernyms, increased the stylistic distance to the author’s text corpus.
Shetty et al. (2018) used Generative Adversarial Networks (GANs) to “transfer” the style of an input text to a target style.
Emmery et al. (2018) used auto-encoders with a gradient reversal layer to “de-style” an input text (aka style invariance).
2.2 Obfuscation Detection  Prior work has successfully used stylometric analysis to detect manual authorship obfuscation (Juola, 2012; Afroz et al., 2012).
In a related area, Shahid et al. (2017) used stylometric analysis to detect whether or not a document was “spun” by text spinners.
Using contextual word likelihoods, as estimated using a pre-trained language model (Radford et al., 2019), Gehrmann et al. (2019) were able to raise the accuracy of humans at detecting synthetic text from 54% to 72%.
Zellers et al. (2019) showed that a classiﬁer based on a language model can accurately detect synthetic text generated by the same language model.
Bakhtin et al. (2019) also showed that the detection accuracy degrades when the synthetic text is generated using a language model trained on a different corpus.
The quality and smoothness of automated text transformations using the state-of-the-art obfuscators differ from that of human written text (Mahmood et al., 2019).
Speciﬁcally, we choose well-known context-aware neural language models GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2018).
GPT-2 has been shown to perform better than BERT (Gehrmann et al., 2019) at synthetic text detection, with word rank giving higher performance than word probability.
GPT-2 released by Open AI in 2019 uses at its core, a variation of the “transformer” architecture, an attention based model (Vaswani et al., 2017) and is trained on text from 45 million outbound links on Reddit (40 GB worth of text).
The authors (Radford et al., 2019) trained four versions of GPT-2 differing in architecture size.
For example, for audio classiﬁ 3https://github.com/HendrikStrobelt/detecting-fake-text  cation (Hershey et al., 2017) store plots of the logmel spectogram of the audios as images and then apply image classiﬁcation methods.
VGG (Simonyan and Zisserman, 2014), was one of the top performers of the different classiﬁers they tested.
4 Experimental Setup 4.1 Authorship Obfuscation Approaches As state-of-the-art automated authorship obfuscators we identiﬁed the top two systems (Potthast et al., 2018) from PAN, a shared CLEF task.8 We also chose Mutant-X, a search based system presented in (Mahmood et al., 2019), which shows better performance than the PAN obfuscation systems.
Document Simpliﬁcation (Castro-Castro et al., 2017).
Style Neutralization (Karadzhov et al., 2017).
MUTANT-X (Mahmood et al., 2019) This system uses a genetic algorithm (GAs) in combination with an authorship attribution system to identify words that when changed would have the highest positive effect towards obfuscation.
Text  8https://pan.webis.de/  transformations are done using a sentiment preserving variation of Word2Vec (Yu et al., 2017).
4.2 Data We use the two data collections which were used by (Mahmood et al., 2019).
This text corpus from (Brennan et al., 2012) contains 699 documents written by 45 unique authors.
This text corpus which is from (Schler et al., 2006) contains more than 600,000 blogger.com blog posts written by 19,320 unique authors.
The authors in (Mahmood et al., 2019) tested their GA obfuscators using 49 EBG and 200 BLOG documents written by 10 authors.
Using the writeprintsRFC authorship attributor (Mahmood et al., 2019) for this purpose, we process all obfuscated documents (127 and 571 for EBG and BLOG respectively).
4.3.2 Other Methods  1) GLTR (Gehrmann et al., 2019).
2) Character trigrams + KNN (Juola, 2012).
3) Writeprints + SVM (Afroz et al., 2012).
It is also similar to the approach in (Shahid et al., 2017), which uses stylometric features to perform spun document detection.
We see that obfuscation detection is easier in EBG than in BLOG with median EBG F1 scores being signiﬁcantly better than median BLOG F1 scores (notches do not overlap (Krzywinski and Altman, 2014)).
