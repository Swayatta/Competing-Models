In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer (Vaswani et al., 2017).
Our model is trained using a new pretraining task based on the masked language model of BERT (Devlin et al., 2019).
Conventional entity representations assign each entity a ﬁxed embedding vector that stores information regarding the entity in a knowledge base (KB) (Bordes et al., 2013; Trouillon et al., 2016; Yamada et al., 2016, 2017).
By contrast, contextualized word representations (CWRs) based on the transformer (Vaswani et al., 2017), such as BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2020), provide effective general-purpose word representations trained with unsupervised pretraining tasks based on language modeling.
Many recent studies have solved entity-related tasks using the contextualized representations of entities computed based on CWRs (Zhang et al., 2019; Peters et al., 2019; Joshi et al., 2020).
Although the transformer can capture the complex relationships between words by relating them to each other multiple times using the self-attention mechanism (Clark et al., 2019; Reif et al., 2019), it is difﬁcult to perform such reasoning between entities because many entities are split into multiple tokens in the model.
LUKE is based on a transformer (Vaswani et al., 2017) trained using a large amount of entity-annotated corpus  Proceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages6442–6454,November16–20,2020.c(cid:13)2020AssociationforComputationalLinguistics6442Figure 1: Architecture of LUKE using the input sentence “Beyonc´e lives in Los Angeles.” LUKE outputs contextualized representation for each word and entity in the text.
LUKE is trained using a new pretraining task, a straightforward extension of BERT’s masked language model (MLM) (Devlin et al., 2019).
Our model outperforms all baseline models, including RoBERTa, in all experiments, and obtains state-of-the-art results on ﬁve tasks: entity typing on the Open Entity dataset (Choi et al., 2018), relation classiﬁcation on the TACRED dataset (Zhang et al., 2017), NER on the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003), clozestyle QA on the ReCoRD dataset (Zhang et al., 2018a), and extractive QA on the SQuAD 1.1 dataset (Rajpurkar et al., 2016).
They include knowledge embeddings trained on knowledge graphs (Bordes et al., 2013; Yang et al., 2015; Trouillon et al., 2016), and embeddings trained using textual contexts or descriptions of entities retrieved from a KB (Yamada et al., 2016, 2017; Cao et al., 2017; Ganea and Hofmann, 2017).
Similar to our pretraining task, NTEE (Yamada et al., 2017) and RELIC (Ling et al., 2020) use an approach that trains entity embeddings by predicting entities given their textual contexts obtained from a KB.
Contextualized Word Representations Many recent studies have addressed entity-related tasks based on the contextualized representations of entities in text computed using the word representations of CWRs (Zhang et al., 2019; Baldini Soares et al., 2019; Peters et al., 2019; Joshi et al., 2020; Wang et al., 2019b, 2020).
Representative examples of CWRs are ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), which are based on deep bidirectional long short-term memory (LSTM) and the transformer (Vaswani et al., 2017), respectively.
Most recent CWRs, such as RoBERTa (Liu et al., 2020), XLNet (Yang et al., 2019), SpanBERT (Joshi et al., 2020), ALBERT (Lan et al., 2020), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020), are based on transformer trained using a task equivalent to or similar to the MLM.
ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019) use a similar idea to enhance CWRs using static entity embeddings sep arately learned from a KB.
WKLM (Xiong et al., 2020) trains the model to detect whether an entity name in text is replaced by another entity name of the same type.
KEPLER (Wang et al., 2019b) conducts pretraining based on the MLM and a knowledge-embedding objective (Bordes et al., 2013).
K-Adapter (Wang et al., 2020) was proposed concurrently with our work, and extends CWRs using neural adapters that inject factual and linguistic knowledge.
The model adopts a multi-layer bidirectional transformer (Vaswani et al., 2017).
Following past work (Devlin et al., 2019; Liu et al., 2020), we insert special tokens [CLS] and [SEP] into the word sequence as the ﬁrst and last words, respectively.
3.2 Entity-aware Self-attention The self-attention mechanism is the foundation of the transformer (Vaswani et al., 2017), and relates tokens each other based on the attention score between each pair of tokens.
Formally, the original entity corresponding to a masked entity is predicted by applying the softmax function over all entities in our vocabulary:  m = layer norm(cid:0)gelu(Whhe + bh)(cid:1)  ˆy = softmax(BTm + bo)  where he is the representation corresponding to the masked entity, T ∈ RH×D and Wh ∈ RD×D are weight matrices, bo ∈ RVe and bh ∈ RD are bias vectors, gelu(·) is the gelu activation function (Hendrycks and Gimpel, 2016), and layer norm(·) is the layer normalization function (Lei Ba et al., 2016).
3.4 Modeling Details Our model conﬁguration follows RoBERTaLARGE (Liu et al., 2020), pretrained CWRs based on a bidirectional transformer and a variant of BERT (Devlin et al., 2019).
Following past work (Devlin et al., 2019; Liu et al., 2020), we mask 15% of all words and entities at random.
F1 Name 77.4 60.6 68.0 UFET (Zhang et al., 2019) 76.4 71.0 73.6 BERT (Zhang et al., 2019) 78.4 72.9 75.6 ERNIE (Zhang et al., 2019) KEPLER (Wang et al., 2019b) 77.2 74.2 75.7 KnowBERT (Peters et al., 2019) 78.6 73.7 76.1 K-Adapter (Wang et al., 2020) 79.3 75.8 77.5 77.6 75.0 76.2 RoBERTa (Wang et al., 2020) 79.9 76.6 78.2 LUKE  Table 1: Results of entity typing on the Open Entity dataset.
Following Zhang et al. (2019), we use the Open Entity dataset (Choi et al., 2018), and consider only nine general entity types.
Following Wang et al. (2020), we report loose micro-precision, recall, and F1, and employ the micro-F1 as the primary metric.
Baselines UFET (Choi et al., 2018) is a conventional model that computes context representations using the bidirectional LSTM.
6446Name BERT (Zhang et al., 2019) C-GCN (Zhang et al., 2018b) ERNIE (Zhang et al., 2019) SpanBERT (Joshi et al., 2020) MTB (Baldini Soares et al., 2019) KnowBERT (Peters et al., 2019) KEPLER (Wang et al., 2019b) K-Adapter (Wang et al., 2020) RoBERTa (Wang et al., 2020) LUKE  Prec.
F1 67.2 64.8 66.0 69.9 63.3 66.4 70.0 66.1 68.0 70.8 70.9 70.8 71.5 71.6 71.4 71.5 70.4 73.0 71.7 68.9 75.4 72.0 70.2 72.4 71.3 70.4 75.1 72.7    Name F1 LSTM-CRF (Lample et al., 2016) 91.0 92.2 ELMo (Peters et al., 2018) 92.8 BERT (Devlin et al., 2019) 93.1 Akbik et al. (2018) 93.5 Baevski et al.
We conduct experiments using TACRED dataset (Zhang et al., 2017), a large-scale relation classiﬁcation dataset containing 106,264 sentences with 42 relation types.
Following Wang et al. (2020), we report the micro-precision, recall, and F1, and use the micro-F1 as the primary metric.
Baselines C-GCN (Zhang et al., 2018b) uses graph convolutional networks over dependency tree structures to solve the task.
MTB (Baldini Soares et al., 2019) learns relation representations based on BERT through the matching-the-blanks task using a large amount of entity-annotated text.
4.3 Named Entity Recognition We conduct experiments on the NER task using the standard CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003).
Following past work, we  Model Following Sohrab and Miwa (2018), we solve the task by enumerating all possible spans (or n-grams) in each sentence as entity name candidates, and classifying them into the target entity types or non-entity type, which indicates that the span is not an entity.
Following Devlin et al. (2019), we include the maximal document context in the target document.
Baselines LSTM-CRF (Lample et al., 2016) is a model based on the bidirectional LSTM with conditional random ﬁelds (CRF).
Akbik et al. (2018) address the task using the bidirectional LSTM with CRF enhanced with character-level contextualized representations.
Similarly, Baevski et al. (2019) use the bidirectional LSTM with CRF enhanced with CWRs based on a bidirectional transformer.
Furthermore, it achieves a new state of the art on this competitive dataset by outperforming the previous state of the art reported in Baevski et al. (2019) by 0.8 F1 points.
4.4 Cloze-style Question Answering We evaluate our model on the ReCoRD dataset (Zhang et al., 2018a), a cloze-style QA dataset consisting of over 120K examples.
Following Liu et al. (2020), given a question q1, q2, ..., qj, and a passage p1, p2, ..., pl, the input word sequence is constructed as: [CLS]q1, q2, ..., qj[SEP] [SEP]p1, p2, ..., pl[SEP].
Baselines DocQA+ELMo (Clark and Gardner, 2018) is a model based on ELMo, bidirectional attention ﬂow (Seo et al., 2017), and self-attention mechanism.
XLNet+Veriﬁer (Li et al., 2019) is a model based on XLNet with rule-based answer veriﬁcation, and is the winner of a recent competition  Name  DocQA+ELMo (Zhang et al., 2018a) BERT (Wang et al., 2019a) XLNet+Veriﬁer (Li et al., 2019) RoBERTa (Liu et al., 2020) RoBERTa (ensemble) (Liu et al., 2020) LUKE  Dev F1  Test EM  Test Dev F1 EM 44.1 45.4 45.4 46.7 71.3 72.0 80.6 82.1 81.5 82.7 89.0 89.5        90.0 90.6 90.8 91.4 90.6 91.2  Table 4: Results of cloze-style question answering on the ReCoRD dataset.
based on this dataset (Ostermann et al., 2019).
4.5 Extractive Question Answering Finally, we conduct experiments using the wellknown Stanford Question Answering Dataset (SQuAD) 1.1 consisting of 100K question/answer pairs (Rajpurkar et al., 2016).
Because the results for RoBERTa and ALBERT are reported only on the development set, we conduct a comparison  6448Name  BERT (Devlin et al., 2019) SpanBERT (Joshi et al., 2020) XLNet (Yang et al., 2019) ALBERT (Lan et al., 2020) RoBERTa (Liu et al., 2020) LUKE    Dev F1  Test EM  Test Dev F1 EM 84.2 91.1 85.1 91.8 88.8 94.6 89.0 94.5 89.9 95.1 89.3 94.8 88.9 94.6 89.8 95.0 90.2 95.4    Table 5: Results of extractive question answering on the SQuAD 1.1 dataset.
To conduct a fair compassion with RoBERTa, we use the same model architecture and hyper-parameters as those of RoBERTa (Liu et al., 2020).
Because past studies (Liu et al., 2020; Lan et al., 2020) suggest that simply increasing the number of training steps of CWRs tends to improve performance on downstream tasks, the superior experimental results of LUKE compared with those of RoBERTa may be obtained because of its greater number of pretraining steps.
