KGs ﬁnd uses in a wide variety of applications such as semantic search (Berant et al., 2013; Berant and Liang, 2014), dialogue generation (He et al., 2017; Keizer et al., 2017), and question answering (Zhang et al., 2016; Diefenbach et al., 2018), to name a few.
However, KGs typically ∗ Equal Contribution  suffer from missing relations (Socher et al., 2013a; West et al., 2014).
They are broadly classiﬁed as translational models (Bordes et al., 2013; Yang et al., 2015; Trouillon et al., 2016) and convolutional neural network (CNN) (Nguyen et al., 2018; Dettmers et al., 2018) based models.
Our idea is: 1) to capture multi-hop relations (Lin et al., 2015) surrounding a given node, 2) to encapsulate the diversity of roles played by an entity in various relations, and 3) to consolidate the existing knowledge present in semantically similar relation clusters (Valverde-Rebaza  Proceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages4710–4723Florence,Italy,July28-August2,2019.c(cid:13)2019AssociationforComputationalLinguistics4710and de Andrade Lopes, 2012).
To resolve this issue, we use relation composition as proposed by (Lin et al., 2015) to introduce an auxiliary edge between nhop neighbors, which then readily allows the ﬂow of knowledge between entities.
Our architecture is an encoder-decoder model where our generalized graph attention model and ConvKB (Nguyen et al., 2018) play the roles of an encoder and decoder, respectively.
Moreover, this method can be extended for learning effective embeddings for Textual Entailment Graphs (Kotlerman et al., 2015), where global learning has proven effective in the past as shown by (Berant et al., 2015) and (Berant et al., 2010).
RESCAL (Nickel et al., 2011), NTN (Socher et al., 2013b), and the Holographic embedding  Figure 1: Subgraph of a knowledge graph contains actual relations between entities (solid lines) and inferred relations that are initially hidden (dashed lines).
model (HOLE) (Nickel et al., 2016) are examples of compositional based models.
In comparison,  translational models  like TransE (Bordes et al., 2013), DISTMULT (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) propose arguably simpler models.
DISTMULT (Yang et al., 2015) learns embeddings using a bilinear diagonal model which is a special case of the bilinear objective used in NTN and TransE.
ComplEx (Trouillon et al., 2016) generalizes DISTMULT (Yang et al., 2015) by using complex embeddings and Hermitian dot products instead.
Recently,  two CNN based models have been proposed for relation prediction, namely ConvE (Dettmers and ConvKB (Nguyen et al., 2018).
acted_inacted_inborn_in?capital_ofdirectedcolleaguebrother_ofA graph based neural network model called R-GCN (Schlichtkrull et al., 2018) is an extension of applying graph convolutional networks (GCNs) (Kipf and Welling, 2017) to relational data.
3.2 Graph Attention Networks (GATs) Graph convolutional networks (GCNs) (Kipf and Welling, 2017) gather information from the entity’s neighborhood and all neighbors contribute equally in the information passing.
To ad(Veliˇckovi´c dress the shortcomings of GCNs, et al., 2018) introduced graph attention networks (GATs).
GAT employs multi-head attention to stabilize the learning process as credited to (Vaswani et al., 2017).
3.4 Training Objective Our model borrows the idea of a translational scoring function from (Bordes et al., 2013), which learns embeddings such that for a given valid triple ij = (ei, rk, ej), the condition (cid:126)hi+(cid:126)gk ≈ (cid:126)hj holds, tk i.e., ej is the nearest neighbor of ei connected via relation rk.
We train our model using hinge-loss which is  given by the following expression  L(Ω) =  max{dt(cid:48)  ij  − dtij + γ, 0} (13)  (cid:88)  (cid:88)  tij∈S  ij∈S(cid:48) t(cid:48)  where γ > 0 is a margin hyper-parameter, S is the set of valid triples, and S(cid:48) denotes the set of invalid  4714Graph Attention  Layer 1 Triple NChristian Baleacted_inThe MachinistTriple 1Barack Obamapresident_ofUnited StatesAttentionHead 1 AttentionHead 2 Graph AttentionLayer 2 TrainingLoss triples, given formally as (cid:123)(cid:122) (cid:125) i ∈ E \ ei} i(cid:48)j | e(cid:48) S(cid:48) = {tk  (cid:124)  replace head entity  (cid:124)  ∪{tk  ij(cid:48) | e(cid:48)  (cid:123)(cid:122) (cid:125) j ∈ E \ ej}  replace tail entity  3.5 Decoder Our model uses ConvKB (Nguyen et al., 2018) as a decoder.
The model is trained using soft-margin loss as L =  (cid:88)  (cid:107)W(cid:107)2  log(1+exp(ltk  ij)))+  .f (tk  2  ij  λ 2  ij  =  where ltk  for tk 1 −1 for tk  ij ∈ S ij ∈ S(cid:48) 4 Experiments and Results 4.1 Datasets To evaluate our proposed method, we use ﬁve benchmark datasets: WN18RR (Dettmers et al., 2018), FB15k-237 (Toutanova et al., 2015), NELL-995 (Xiong et al., 2017), Uniﬁed Medical Language Systems (UMLS) (Kok and Domingos, 2007) and Alyawarra Kinship (Lin et al., 2018).
Previous works (Toutanova et al., 2015; Dettmers et al., 2018) suggest that the task of relation prediction in WN18 and FB15K suffers from the problem of inverse relations, whereby one can achieve state-of-the-art results using a simple reversal rule based model, as shown by (Dettmers et al., 2018).
We used the data splits provided by (Nguyen et al., 2018).
Entity and relation embeddings produced by TransE (Bordes et al., 2013; Nguyen et al., 2018) are used to initialize our embeddings.
We follow a two-step training procedure, i.e., we ﬁrst train our generalized GAT to encode information about the graph entities and relations and then train a decoder model like ConvKB (Nguyen et al., 2018) to perform the relation prediction task.
Similar to previous work ((Bordes et al., 2013), (Nguyen et al., 2018), (Dettmers et al., 2018)), we evaluate all the models in a ﬁltered setting, i.e, during ranking we remove corrupt triples which are already present in one of the training, validation, or test sets.
4715Dataset WN18RR FB15k-237 NELL-995 Kinship UMLS  # Entities 40,943 14,541 75,492  104 135  # Relations  11 237 200 25 46  Training 86,835 272,115 149,678  8544 5216  # Edges  Validation  3034 17,535  543 1068 652  Test 3134 20,466 3992 1074 661  Total 93,003 310,116 154,213 10,686 6529  Mean in-degree  Median in-degree  2.12 18.71 1.98 82.15 38.63  1 8 0  82.5 20  Table 1: Dataset statistics  WN18RR  Hits@N  FB15K-237  Hits@N  MR MRR @1 @3 @10 MR MRR @1 @3 @10  DistMult (Yang et al., 2015) ComplEx (Trouillon et al., 2016) ConvE (Dettmers et al., 2018) TransE (Bordes et al., 2013) ConvKB (Nguyen et al., 2018) R-GCN (Schlichtkrull et al., 2018) 6700 0.123 Our work  50.4 7000 0.444 41.2 47 7882 0.449 40.9 46.9 53 4464 0.456 41.9 53.1 47 2300 0.243 4.27 44.1 53.2 1295 0.265 5.82 44.5 55.8 13.7 20.7 1940 0.440 36.1 48.3 58.1  8  512 0.281 19.9 30.1 44.6 546 0.278 19.4 29.7 45 245 0.312 22.5 34.1 49.7 323 0.279 19.8 37.6 44.1 216 0.289 19.8 32.4 47.1 30 600 0.164 210 0.518 62.6  18.1 54  10 46  Table 2: Experimental results on WN18RR and FB15K-237 test sets.
NELL-995  Hits@N  Kinship  Hits@N  MR MRR @1 @3 @10  MR MRR @1 @3 @10  4213 0.485 40.1 52.4 61 DistMult (Yang et al., 2015) 4600 0.482 39.9 52.8 60.6 ComplEx (Trouillon et al., 2016) 3560 0.491 40.3 53.1 61.3 ConvE (Dettmers et al., 2018) 2100 0.401 34.4 47.2 50.1 TransE (Bordes et al., 2013) 600 47 ConvKB (Nguyen et al., 2018) 54.5 0.43 0.12 R-GCN (Schlichtkrull et al., 2018) 7600 12.6 18.8 0.530 44.7 56.4 69.5 Our work 965  37.0 8.2  36.7 73.3 73.8 0.9  5.26 2.48 2.03 6.8 3.3 25.92 0.109 1.94 0.904  58.1 86.7 0.516 89.9 97.11 0.823 91.7 98.14 0.833 64.3 84.1 0.309 95.3 0.614 43.62 75.5 23.9 8.8 94.1 98  3 85.9  Table 3: Experimental results on NELL-995 and Kinship test sets.
The proposed model can be extended to learn embeddings for various tasks using KGs such as dialogue generation (He et al., 2017; Keizer et al., 2017), and question answering (Zhang et al., 2016; Diefenbach et al., 2018).
